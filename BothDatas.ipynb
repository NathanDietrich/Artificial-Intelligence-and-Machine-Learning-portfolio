{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYTBUU76hV/PqHOcb7GER4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NathanDietrich/Artificial-Intelligence-and-Machine-Learning-portfolio/blob/main/BothDatas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code gathers sentiment and historical stock data with technical indicators from a user selected stock and time period. This program also normalizes the data and saves it to an output file, and saves the scaler in a .pkl file for later use\n"
      ],
      "metadata": {
        "id": "N6SvtxjYVPu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance textblob ta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL4FvCIBo-Dl",
        "outputId": "7deb49c8-8358-40fc-ec39-be8f6bfa182c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.52)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: ta in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.3.1)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import datetime\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "from textblob import TextBlob\n",
        "from ta.volatility import AverageTrueRange, BollingerBands, DonchianChannel, KeltnerChannel\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from google.colab import userdata\n",
        "\n",
        "def fetch_stock_data_polygon(ticker, start_date, end_date, api_key):\n",
        "    \"\"\"\n",
        "    Fetches historical stock data from Polygon.io.\n",
        "    \"\"\"\n",
        "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date}/{end_date}?apiKey={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error fetching stock data: {response.text}\")\n",
        "        return None\n",
        "\n",
        "    data = response.json()\n",
        "    if \"results\" not in data:\n",
        "        print(\"No results found.\")\n",
        "        return None\n",
        "\n",
        "    # Convert response to DataFrame\n",
        "    df = pd.DataFrame(data[\"results\"])\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"t\"], unit=\"ms\").dt.date\n",
        "    df.rename(columns={\"o\": \"Open\", \"h\": \"High\", \"l\": \"Low\", \"c\": \"Close\", \"v\": \"Volume\"}, inplace=True)\n",
        "    df = df[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "\n",
        "    return df\n",
        "\n",
        "def fetch_and_calculate_technical_indicators(ticker, start_date, end_date, api_key):\n",
        "    \"\"\"\n",
        "    Fetch stock data using Polygon.io and compute technical indicators.\n",
        "    \"\"\"\n",
        "    df = fetch_stock_data_polygon(ticker, start_date, end_date, api_key)\n",
        "    if df is None:\n",
        "        print(\"Failed to fetch stock data.\")\n",
        "        return None\n",
        "\n",
        "    # Average True Range (ATR)\n",
        "    df['ATR'] = AverageTrueRange(high=df['High'], low=df['Low'], close=df['Close']).average_true_range()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    bb = BollingerBands(close=df['Close'])\n",
        "    df['BB_High'] = bb.bollinger_hband()\n",
        "    df['BB_low'] = bb.bollinger_lband()\n",
        "\n",
        "    # Donchian Channel\n",
        "    dc = DonchianChannel(high=df['High'], low=df['Low'], close=df['Close'])\n",
        "    df['DC_High'] = dc.donchian_channel_hband()\n",
        "    df['DC_low'] = dc.donchian_channel_lband()\n",
        "\n",
        "    # Keltner Channel\n",
        "    kc = KeltnerChannel(high=df['High'], low=df['Low'], close=df['Close'])\n",
        "    df['KC_High'] = kc.keltner_channel_hband()\n",
        "    df['KC_Low'] = kc.keltner_channel_lband()\n",
        "\n",
        "    # Williams %R\n",
        "    wr = WilliamsRIndicator(high=df['High'], low=df['Low'], close=df['Close'])\n",
        "    df['Williams_%R'] = wr.williams_r()\n",
        "\n",
        "    # Drop any NaN caused by rolling calculations\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def get_historical_news_chunked(ticker, start_date, end_date, api_key, limit=1000):\n",
        "    \"\"\"\n",
        "    Fetches historical news data from Polygon.io in 1-month chunks.\n",
        "    \"\"\"\n",
        "    url = \"https://api.polygon.io/v2/reference/news\"\n",
        "    all_results = []\n",
        "\n",
        "    current_start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    final_end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "    while current_start_date < final_end_date:\n",
        "        chunk_end_date = current_start_date + datetime.timedelta(days=30)\n",
        "        if chunk_end_date > final_end_date:\n",
        "            chunk_end_date = final_end_date\n",
        "\n",
        "        chunk_start_str = current_start_date.strftime(\"%Y-%m-%d\")\n",
        "        chunk_end_str = chunk_end_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        print(f\"Fetching news from {chunk_start_str} to {chunk_end_str}...\")\n",
        "\n",
        "        params = {\n",
        "            \"ticker\": ticker,\n",
        "            \"published_utc.gte\": chunk_start_str,\n",
        "            \"published_utc.lte\": chunk_end_str,\n",
        "            \"apiKey\": api_key,\n",
        "            \"limit\": limit\n",
        "        }\n",
        "\n",
        "        while True:\n",
        "            response = requests.get(url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                results = data.get(\"results\", [])\n",
        "                all_results.extend(results)\n",
        "\n",
        "                next_cursor = data.get(\"next_cursor\")\n",
        "                if not next_cursor:\n",
        "                    break\n",
        "\n",
        "                params[\"cursor\"] = next_cursor\n",
        "            else:\n",
        "                print(f\"Error: {response.status_code}, {response.text}\")\n",
        "                break\n",
        "\n",
        "        current_start_date = chunk_end_date\n",
        "        time.sleep(14)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def analyze_sentiment(news_data):\n",
        "    \"\"\"\n",
        "    Uses TextBlob to compute sentiment polarity and subjectivity for each news article.\n",
        "    \"\"\"\n",
        "    analyzed_data = []\n",
        "    for article in news_data:\n",
        "        title = article.get(\"title\", \"\")\n",
        "        description = article.get(\"description\", \"\")\n",
        "        full_text = f\"{title} {description}\"\n",
        "        sentiment = TextBlob(full_text).sentiment\n",
        "\n",
        "        analyzed_data.append({\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "            \"published_date\": article.get(\"published_utc\", \"\"),\n",
        "            \"sentiment_polarity\": sentiment.polarity,\n",
        "            \"sentiment_subjectivity\": sentiment.subjectivity\n",
        "        })\n",
        "    return analyzed_data\n",
        "\n",
        "import os\n",
        "\n",
        "def main():\n",
        "    # === Get user inputs ===\n",
        "    ticker = input(\"Enter the stock ticker (e.g., TSLA): \").strip().upper()\n",
        "    start_date = input(\"Enter start date (YYYY-MM-DD): \").strip()\n",
        "    end_date = input(\"Enter end date (YYYY-MM-DD): \").strip()\n",
        "\n",
        "    # === Load Polygon API key ===\n",
        "    api_key = userdata.get('Polygon_Key')\n",
        "    if not api_key:\n",
        "        print(\"Polygon API key not found.\")\n",
        "        return\n",
        "\n",
        "    # === Create Dynamic Folder for Saving Files ===\n",
        "    folder_name = f\"{ticker}_{start_date}_to_{end_date}\"\n",
        "    os.makedirs(folder_name, exist_ok=True)  # Create folder if it doesn’t exist\n",
        "    print(f\"\\n📁 Saving all data in folder: {folder_name}\")\n",
        "\n",
        "    # === Fetch stock data with technical indicators ===\n",
        "    print(f\"\\nFetching stock data for {ticker} from {start_date} to {end_date}...\")\n",
        "    stock_df = fetch_and_calculate_technical_indicators(ticker, start_date, end_date, api_key)\n",
        "\n",
        "    # === Fetch news data and analyze sentiment ===\n",
        "    print(f\"\\nFetching news for {ticker} from {start_date} to {end_date}...\")\n",
        "    news_data = get_historical_news_chunked(ticker, start_date, end_date, api_key, limit=1000)\n",
        "\n",
        "    if not news_data:\n",
        "        print(\"No news data found. Proceeding without sentiment data.\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nPerforming sentiment analysis...\")\n",
        "    analyzed_news = analyze_sentiment(news_data)\n",
        "    sentiment_df = pd.DataFrame(analyzed_news)\n",
        "\n",
        "    sentiment_df['published_date'] = pd.to_datetime(sentiment_df['published_date'], errors='coerce')\n",
        "    sentiment_df['Date'] = sentiment_df['published_date'].dt.date\n",
        "\n",
        "    daily_sentiment = sentiment_df.groupby('Date').agg({\n",
        "        'sentiment_polarity': 'mean',\n",
        "        'sentiment_subjectivity': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # === Merge sentiment with stock data BEFORE saving raw data ===\n",
        "    print(\"\\nMerging sentiment data with stock data...\")\n",
        "    stock_df = pd.merge(stock_df, daily_sentiment, on='Date', how='left')\n",
        "\n",
        "    # Forward-fill missing sentiment values (but NOT stock indicators)\n",
        "    stock_df[['sentiment_polarity', 'sentiment_subjectivity']] = (\n",
        "        stock_df[['sentiment_polarity', 'sentiment_subjectivity']].replace(0, np.nan).ffill()\n",
        "    )\n",
        "\n",
        "    # === Save raw stock data WITH sentiment ===\n",
        "    raw_csv = os.path.join(folder_name, \"raw.csv\")\n",
        "    stock_df.to_csv(raw_csv, index=False)\n",
        "    print(f\"✅ Raw stock data WITH sentiment saved at: {raw_csv}\")\n",
        "\n",
        "    # === Preprocessing for LSTM/RNN/CNN ===\n",
        "    print(\"\\nScaling data for deep learning models...\")\n",
        "    numeric_cols = stock_df.select_dtypes(include=[np.number]).columns.difference(['sentiment_polarity', 'sentiment_subjectivity'])\n",
        "    scaler = MinMaxScaler()\n",
        "    stock_df[numeric_cols] = scaler.fit_transform(stock_df[numeric_cols])\n",
        "\n",
        "    # Save the scaler inside the same folder\n",
        "    scaler_path = os.path.join(folder_name, \"scaler.pkl\")\n",
        "    joblib.dump(scaler, scaler_path)\n",
        "    print(f\"✅ Scaler saved at: {scaler_path}\")\n",
        "\n",
        "    # === Save preprocessed data ===\n",
        "    preprocessed_csv = os.path.join(folder_name, \"preprocessed.csv\")\n",
        "    stock_df.to_csv(preprocessed_csv, index=False)\n",
        "    print(f\"✅ Preprocessed data saved at: {preprocessed_csv}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "O3Kpcww0REKZ",
        "outputId": "8dbab611-92af-4492-8344-a99593649455",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the stock ticker (e.g., TSLA): cat\n",
            "Enter start date (YYYY-MM-DD): 2024-08-08\n",
            "Enter end date (YYYY-MM-DD): 2025-02-19\n",
            "\n",
            "📁 Saving all data in folder: CAT_2024-08-08_to_2025-02-19\n",
            "\n",
            "Fetching stock data for CAT from 2024-08-08 to 2025-02-19...\n",
            "\n",
            "Fetching news for CAT from 2024-08-08 to 2025-02-19...\n",
            "Fetching news from 2024-08-08 to 2024-09-07...\n",
            "Fetching news from 2024-09-07 to 2024-10-07...\n",
            "Fetching news from 2024-10-07 to 2024-11-06...\n",
            "Fetching news from 2024-11-06 to 2024-12-06...\n",
            "Fetching news from 2024-12-06 to 2025-01-05...\n",
            "Fetching news from 2025-01-05 to 2025-02-04...\n",
            "Fetching news from 2025-02-04 to 2025-02-19...\n",
            "\n",
            "Performing sentiment analysis...\n",
            "\n",
            "Merging sentiment data with stock data...\n",
            "✅ Raw stock data WITH sentiment saved at: CAT_2024-08-08_to_2025-02-19/raw.csv\n",
            "\n",
            "Scaling data for deep learning models...\n",
            "✅ Scaler saved at: CAT_2024-08-08_to_2025-02-19/scaler.pkl\n",
            "✅ Preprocessed data saved at: CAT_2024-08-08_to_2025-02-19/preprocessed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dont use code below, yfinnance was having issues"
      ],
      "metadata": {
        "id": "ZVzAPEDRe1bT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "wV6NshkQofot",
        "outputId": "455d001c-2df4-4343-bf16-9b01afbe9727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the stock ticker (e.g., TSLA): cat\n",
            "Enter start date (YYYY-MM-DD): 2023-01-01\n",
            "Enter end date (YYYY-MM-DD): 2025-02-19\n",
            "\n",
            "Fetching stock data for CAT from 2023-01-01 to 2025-02-19...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "YFRateLimitError",
          "evalue": "Too Many Requests. Rate limited. Try after a while.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mYFRateLimitError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fdd37f88a146>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-fdd37f88a146>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# === Step 3: Fetch and process stock data / technical indicators ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nFetching stock data for {ticker} from {start_date} to {end_date}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0mstock_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_and_calculate_technical_indicators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;31m# === Step 4: Fetch news data, perform sentiment analysis ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fdd37f88a146>\u001b[0m in \u001b[0;36mfetch_and_calculate_technical_indicators\u001b[0;34m(ticker, start_date, end_date)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \"\"\"\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Fetch data from Yahoo Finance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTicker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Keep only necessary columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mIndentationContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Exiting {func.__name__}()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/base.py\u001b[0m in \u001b[0;36mhistory\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_indent_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_load_price_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# ------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/base.py\u001b[0m in \u001b[0;36m_lazy_load_price_history\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_lazy_load_price_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_price_history\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_price_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPriceHistory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ticker_tz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_price_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/base.py\u001b[0m in \u001b[0;36m_get_ticker_tz\u001b[0;34m(self, proxy, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtz\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mtz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_ticker_tz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_valid_timezone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mIndentationContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Exiting {func.__name__}()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/base.py\u001b[0m in \u001b[0;36m_fetch_ticker_tz\u001b[0;34m(self, proxy, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mYFRateLimitError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/data.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# copy over the lru_cache extra methods to this wrapper to be able to access them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/data.py\u001b[0m in \u001b[0;36mcache_get\u001b[0;34m(self, url, user_agent_headers, params, proxy, timeout)\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mlru_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_maxsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcache_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent_headers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_proxy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mIndentationContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Exiting {func.__name__}()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/data.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, user_agent_headers, params, proxy, timeout)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_indent_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent_headers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_agent_headers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_indent_decorator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mIndentationContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Exiting {func.__name__}()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/yfinance/data.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, url, request_method, user_agent_headers, body, params, proxy, timeout)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0;31m# Raise exception if rate limited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m429\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mYFRateLimitError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mYFRateLimitError\u001b[0m: Too Many Requests. Rate limited. Try after a while."
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import datetime\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import joblib\n",
        "from textblob import TextBlob\n",
        "from ta.volatility import AverageTrueRange, BollingerBands, DonchianChannel, KeltnerChannel\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_historical_news_chunked(ticker, start_date, end_date, api_key, limit=1000):\n",
        "    \"\"\"\n",
        "    Fetches historical news data from Polygon in 1-month chunks.\n",
        "    \"\"\"\n",
        "    url = \"https://api.polygon.io/v2/reference/news\"\n",
        "    all_results = []\n",
        "\n",
        "    current_start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    final_end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "    while current_start_date < final_end_date:\n",
        "        # Calculate the chunk end date (1 month from the start date)\n",
        "        chunk_end_date = current_start_date + datetime.timedelta(days=30)\n",
        "        if chunk_end_date > final_end_date:\n",
        "            chunk_end_date = final_end_date\n",
        "\n",
        "        chunk_start_str = current_start_date.strftime(\"%Y-%m-%d\")\n",
        "        chunk_end_str = chunk_end_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        print(f\"Fetching news from {chunk_start_str} to {chunk_end_str}...\")\n",
        "\n",
        "        params = {\n",
        "            \"ticker\": ticker,\n",
        "            \"published_utc.gte\": chunk_start_str,\n",
        "            \"published_utc.lte\": chunk_end_str,\n",
        "            \"apiKey\": api_key,\n",
        "            \"limit\": limit\n",
        "        }\n",
        "\n",
        "        while True:\n",
        "            response = requests.get(url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                results = data.get(\"results\", [])\n",
        "                all_results.extend(results)\n",
        "\n",
        "                # Check for pagination\n",
        "                next_cursor = data.get(\"next_cursor\")\n",
        "                if not next_cursor:\n",
        "                    break\n",
        "\n",
        "                # Update the cursor for the next request\n",
        "                params[\"cursor\"] = next_cursor\n",
        "            else:\n",
        "                print(f\"Error: {response.status_code}, {response.text}\")\n",
        "                break\n",
        "\n",
        "        # Move to the next chunk\n",
        "        current_start_date = chunk_end_date\n",
        "\n",
        "        # Respect API rate limit (5 requests/min): sleep longer just to be safe\n",
        "        print(\"Waiting for 1 minute to respect API rate limits...\")\n",
        "        time.sleep(14)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def analyze_sentiment(news_data):\n",
        "    \"\"\"\n",
        "    Uses TextBlob to compute sentiment polarity and subjectivity for each news article.\n",
        "    \"\"\"\n",
        "    analyzed_data = []\n",
        "    for article in news_data:\n",
        "        title = article.get(\"title\", \"\")\n",
        "        description = article.get(\"description\", \"\")\n",
        "        # Combine title and description for sentiment analysis\n",
        "        full_text = f\"{title} {description}\"\n",
        "        sentiment = TextBlob(full_text).sentiment\n",
        "\n",
        "        analyzed_data.append({\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "            \"published_date\": article.get(\"published_utc\", \"\"),\n",
        "            \"sentiment_polarity\": sentiment.polarity,\n",
        "            \"sentiment_subjectivity\": sentiment.subjectivity\n",
        "        })\n",
        "    return analyzed_data\n",
        "\n",
        "def fetch_and_calculate_technical_indicators(ticker, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Fetch historical stock data using yfinance and calculate technical indicators.\n",
        "    Returns a DataFrame with the technical indicators included.\n",
        "    \"\"\"\n",
        "    # Fetch data from Yahoo Finance\n",
        "    df = yf.Ticker(ticker).history(start=start_date, end=end_date)\n",
        "\n",
        "    # Keep only necessary columns\n",
        "    df = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "    # Average True Range (ATR)\n",
        "    df['ATR'] = AverageTrueRange(high=df['High'], low=df['Low'], close=df['Close']).average_true_range()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    bb = BollingerBands(close=df['Close'])\n",
        "    df['BB_High'] = bb.bollinger_hband()\n",
        "    df['BB_low'] = bb.bollinger_lband()\n",
        "\n",
        "    # Donchian Channel\n",
        "    dc = DonchianChannel(high=df['High'], low=df['Low'], close=df['Close'])\n",
        "    df['DC_High'] = dc.donchian_channel_hband()\n",
        "    df['DC_low'] = dc.donchian_channel_lband()\n",
        "\n",
        "    # Keltner Channel\n",
        "    kc = KeltnerChannel(high=df['High'], low=df['Low'], close=df['Close'])\n",
        "    df['KC_High'] = kc.keltner_channel_hband()\n",
        "    df['KC_Low'] = kc.keltner_channel_lband()\n",
        "\n",
        "    # Chaikin Volatility\n",
        "    high_low_range = df['High'] - df['Low']\n",
        "    df['Chaikin_volatility'] = (\n",
        "        high_low_range.rolling(window=10).mean() /\n",
        "        high_low_range.rolling(window=10).std()\n",
        "    )\n",
        "\n",
        "    # Historical Volatility\n",
        "    log_returns = np.log(df['Close'] / df['Close'].shift(1))\n",
        "    df['Historical_volatility'] = log_returns.rolling(window=30).std() * np.sqrt(252)\n",
        "\n",
        "    # Standard Deviation\n",
        "    df['Standard_Deviation'] = df['Close'].rolling(window=14).std()\n",
        "\n",
        "    # Williams %R\n",
        "    wr = WilliamsRIndicator(high=df['High'], low=df['Low'], close=df['Close'])\n",
        "    df['Williams_%R'] = wr.williams_r()\n",
        "\n",
        "    # Commodity Channel Index (CCI)\n",
        "    df['CCI'] = (\n",
        "        (df['Close'] - df['Close'].rolling(20).mean())\n",
        "        / (0.015 * df['Close'].rolling(20).std())\n",
        "    )\n",
        "\n",
        "    # RSI-Based Volatility (simple example)\n",
        "    rsi_diff = df['Close'].rolling(window=14).apply(lambda x: max(x) - min(x))\n",
        "    df['RSI_Based_Volatility'] = rsi_diff / df['Close']\n",
        "\n",
        "    # Ulcer Index\n",
        "    df['Ulcer_Index'] = (\n",
        "        (df['Close'] - df['Close'].rolling(window=14).max()) ** 2\n",
        "    ).rolling(window=14).mean()\n",
        "\n",
        "    # True Strength Index (TSI) example\n",
        "    df['TSI'] = (\n",
        "        log_returns.ewm(span=25).mean() /\n",
        "        log_returns.abs().ewm(span=13).mean()\n",
        "    ) * 100\n",
        "\n",
        "    # Fractal Chaos Oscillator\n",
        "    df['Fractal_chaos_Oscillator'] = df['Close'].rolling(window=14).apply(lambda x: np.ptp(x))\n",
        "\n",
        "    # Drop any rows with NaN caused by rolling calculations\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Convert index to a column named \"Date\"\n",
        "    df.reset_index(inplace=True)\n",
        "    df.rename(columns={'Date': 'Date'}, inplace=True)\n",
        "    # Keep date in date format\n",
        "    df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    # === Step 1: Get user inputs ===\n",
        "    ticker = input(\"Enter the stock ticker (e.g., TSLA): \").strip().upper()\n",
        "    start_date = input(\"Enter start date (YYYY-MM-DD): \").strip()\n",
        "    end_date = input(\"Enter end date (YYYY-MM-DD): \").strip()\n",
        "\n",
        "    # === Step 2: Load Polygon API key from Colab secrets ===\n",
        "    api_key = userdata.get('Polygon_Key')\n",
        "    if not api_key:\n",
        "        print(\"Polygon API key not found in userdata. Please set it in Colab secrets.\")\n",
        "        return\n",
        "\n",
        "    # === Step 3: Fetch and process stock data / technical indicators ===\n",
        "    print(f\"\\nFetching stock data for {ticker} from {start_date} to {end_date}...\")\n",
        "    stock_df = fetch_and_calculate_technical_indicators(ticker, start_date, end_date)\n",
        "\n",
        "    # === Step 4: Fetch news data, perform sentiment analysis ===\n",
        "    print(f\"\\nFetching news for {ticker} from {start_date} to {end_date}...\")\n",
        "    news_data = get_historical_news_chunked(ticker, start_date, end_date, api_key, limit=1000)\n",
        "\n",
        "    if not news_data:\n",
        "        print(\"No news data found. Proceeding without sentiment data.\")\n",
        "        final_csv = f\"combined_{ticker}_{start_date}_to_{end_date}.csv\"\n",
        "        stock_df.to_csv(final_csv, index=False)\n",
        "        print(f\"Output saved to {final_csv}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nPerforming sentiment analysis on fetched news...\")\n",
        "    analyzed_news = analyze_sentiment(news_data)\n",
        "    sentiment_df = pd.DataFrame(analyzed_news)\n",
        "\n",
        "    # Convert published_date to datetime, then to just date\n",
        "    sentiment_df['published_date'] = pd.to_datetime(sentiment_df['published_date'], errors='coerce')\n",
        "    sentiment_df['Date'] = sentiment_df['published_date'].dt.date\n",
        "\n",
        "    # Group sentiment by Date to get daily average (or any other aggregation you prefer)\n",
        "    daily_sentiment = sentiment_df.groupby('Date').agg({\n",
        "        'sentiment_polarity': 'mean',\n",
        "        'sentiment_subjectivity': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # === Step 5: Merge sentiment with stock data ===\n",
        "\n",
        "    print(\"\\nMerging sentiment data with stock data...\")\n",
        "    combined_df = pd.merge(stock_df, daily_sentiment, on='Date', how='left')\n",
        "\n",
        "    # Replace 0s in sentiment data with NaN, then forward-fill missing values\n",
        "    combined_df[['sentiment_polarity', 'sentiment_subjectivity']] = (\n",
        "        combined_df[['sentiment_polarity', 'sentiment_subjectivity']].replace(0, np.nan).ffill()\n",
        "    )\n",
        "\n",
        "    # Fill other missing values (technical indicators, etc.) with 0\n",
        "    combined_df.dropna(inplace=True)\n",
        "\n",
        "\n",
        "    # === Step 6: Additional Preprocessing for LSTM/RNN/CNN ===\n",
        "    print(\"\\nPerforming final preprocessing on the combined data...\")\n",
        "\n",
        "    # (A) Fill any remaining NaN values (e.g., sentiment on days without news)\n",
        "    combined_df.fillna(0, inplace=True)\n",
        "\n",
        "    # (B) Scale all numeric columns except 'Date' and sentiment scores\n",
        "    numeric_cols = combined_df.select_dtypes(include=[np.number]).columns.difference(['sentiment_polarity', 'sentiment_subjectivity'])\n",
        "    scaler = MinMaxScaler()\n",
        "    combined_df[numeric_cols] = scaler.fit_transform(combined_df[numeric_cols])\n",
        "\n",
        "    # Save the scaler\n",
        "    joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "    # Reshape for RNN/LSTM/CNN (batch_size, timesteps, features)\n",
        "    X_values = combined_df.drop(columns=['Date']).values\n",
        "    X_reshaped = X_values.reshape((X_values.shape[0], 1, X_values.shape[1]))\n",
        "\n",
        "    # === Step 7: Save final combined data to CSV ===\n",
        "    final_csv = f\"combined_{ticker}_{start_date}_to_{end_date}.csv\"\n",
        "    combined_df.to_csv(final_csv, index=False)\n",
        "\n",
        "    print(f\"\\nAll done! Preprocessed data (scaled technical indicators + daily sentiment) saved to {final_csv} and scaler saved to scaler.pkl\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below finishes the preprocessing"
      ],
      "metadata": {
        "id": "v63ZIDrfv3M8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "# Load preprocessed dataset\n",
        "data = pd.read_csv(\"INPUT.csv\")  # Update with actual file name\n",
        "scaler = joblib.load(\"scaler.pkl\")  # Load the saved MinMaxScaler\n",
        "\n",
        "# Drop Date column (if still present)\n",
        "if 'Date' in data.columns:\n",
        "    data.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "# Convert to numpy array\n",
        "X_values = data.values\n",
        "\n",
        "# Reshape for RNN/LSTM/CNN\n",
        "X_reshaped = X_values.reshape((X_values.shape[0], 1, X_values.shape[1]))  # (samples, timesteps, features)\n",
        "\n",
        "y = data['Close'].shift(-1).dropna().values  # Predict next day's close\n",
        "X_reshaped = X_reshaped[:-1]  # Align X and Y\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reshaped, y, test_size=0.2, shuffle=False)\n"
      ],
      "metadata": {
        "id": "MwMQBrCMuqmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Data verification:\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "# Checking the first few values\n",
        "print(\"\\nSample X_train values (first 3 rows):\")\n",
        "print(X_train[:3])  # Print first 3 rows of X_train\n",
        "\n",
        "print(\"\\nSample y_train values (first 3 rows):\")\n",
        "print(y_train[:3])  # Print first 3 values of y_train\n"
      ],
      "metadata": {
        "id": "OlqQVs0_vtOc",
        "outputId": "e6b57c4e-62ec-443f-bf0b-49ec9a8516b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data verification:\n",
            "X_train shape: (44, 1, 23)\n",
            "X_test shape: (12, 1, 23)\n",
            "y_train shape: (44,)\n",
            "y_test shape: (12,)\n",
            "\n",
            "Sample X_train values (first 3 rows):\n",
            "[[[0.09502601 0.03726284 0.07688235 0.07359947 0.18582068 0.13559091\n",
            "   0.01969507 0.         0.         0.         0.         0.\n",
            "   0.44213652 1.         0.66105857 0.8414359  0.70430956 1.\n",
            "   0.06836845 0.77378353 0.64804188 0.         0.        ]]\n",
            "\n",
            " [[0.07639075 0.13000896 0.09515902 0.15895877 0.46332903 0.19152035\n",
            "   0.07300457 0.01119432 0.02224776 0.         0.03465629 0.03231944\n",
            "   0.70789002 0.85795658 0.49213153 0.92258509 0.7403699  0.87644118\n",
            "   0.05656642 0.82268936 0.59464244 0.39927128 0.54453283]]\n",
            "\n",
            " [[0.2069061  0.13274883 0.09893517 0.06666224 0.52350561 0.24263891\n",
            "   0.0987134  0.03908745 0.02532701 0.         0.06821886 0.061013\n",
            "   0.67896943 0.89913234 0.2363322  0.729276   0.65550109 0.45570115\n",
            "   0.05452834 0.6896135  0.26673146 0.11496212 0.49924242]]]\n",
            "\n",
            "Sample y_train values (first 3 rows):\n",
            "[0.15895877 0.06666224 0.06428391]\n"
          ]
        }
      ]
    }
  ]
}