{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoSse0fO7EXYha26WWeMG3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NathanDietrich/Artificial-Intelligence-and-Machine-Learning-portfolio/blob/main/BothDatas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code gathers sentiment and historical stock data with technical indicators from a user selected stock and time period. This program also preprocesses the data besides dropping the date, and saves it to an output file, and saves the scaler in a .pkl file for later use\n"
      ],
      "metadata": {
        "id": "N6SvtxjYVPu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance textblob ta"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aL4FvCIBo-Dl",
        "outputId": "7879617c-377c-4b8c-cf8d-dbc94539f267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.52)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (5.3.1)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (1.17.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=77d703dd4912a83813e54e6acf42ef9f9907a5255d4a32e7c12a6eba25cc4398\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV6NshkQofot",
        "outputId": "1179ac5b-d8e7-4af0-8ade-e3c3f80a373a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the stock ticker (e.g., TSLA): TSLA\n",
            "Enter start date (YYYY-MM-DD): 2024-10-10\n",
            "Enter end date (YYYY-MM-DD): 2025-02-17\n",
            "\n",
            "Fetching stock data for TSLA from 2024-10-10 to 2025-02-17...\n",
            "\n",
            "Fetching news for TSLA from 2024-10-10 to 2025-02-17...\n",
            "Fetching news from 2024-10-10 to 2024-11-09...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-11-09 to 2024-12-09...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-12-09 to 2025-01-08...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2025-01-08 to 2025-02-07...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2025-02-07 to 2025-02-17...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "\n",
            "Performing sentiment analysis on fetched news...\n",
            "\n",
            "Merging sentiment data with stock data...\n",
            "\n",
            "Performing final preprocessing on the combined data...\n",
            "\n",
            "All done! Preprocessed data (scaled technical indicators + daily sentiment) saved to combined_TSLA_2024-10-10_to_2025-02-17.csv and scaler saved to scaler.pkl\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import datetime\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import joblib\n",
        "from textblob import TextBlob\n",
        "from ta.volatility import AverageTrueRange, BollingerBands, DonchianChannel, KeltnerChannel\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_historical_news_chunked(ticker, start_date, end_date, api_key, limit=1000):\n",
        "    \"\"\"\n",
        "    Fetches historical news data from Polygon in 1-month chunks.\n",
        "    \"\"\"\n",
        "    url = \"https://api.polygon.io/v2/reference/news\"\n",
        "    all_results = []\n",
        "\n",
        "    current_start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    final_end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "    while current_start_date < final_end_date:\n",
        "        # Calculate the chunk end date (1 month from the start date)\n",
        "        chunk_end_date = current_start_date + datetime.timedelta(days=30)\n",
        "        if chunk_end_date > final_end_date:\n",
        "            chunk_end_date = final_end_date\n",
        "\n",
        "        chunk_start_str = current_start_date.strftime(\"%Y-%m-%d\")\n",
        "        chunk_end_str = chunk_end_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        print(f\"Fetching news from {chunk_start_str} to {chunk_end_str}...\")\n",
        "\n",
        "        params = {\n",
        "            \"ticker\": ticker,\n",
        "            \"published_utc.gte\": chunk_start_str,\n",
        "            \"published_utc.lte\": chunk_end_str,\n",
        "            \"apiKey\": api_key,\n",
        "            \"limit\": limit\n",
        "        }\n",
        "\n",
        "        while True:\n",
        "            response = requests.get(url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                results = data.get(\"results\", [])\n",
        "                all_results.extend(results)\n",
        "\n",
        "                # Check for pagination\n",
        "                next_cursor = data.get(\"next_cursor\")\n",
        "                if not next_cursor:\n",
        "                    break\n",
        "\n",
        "                # Update the cursor for the next request\n",
        "                params[\"cursor\"] = next_cursor\n",
        "            else:\n",
        "                print(f\"Error: {response.status_code}, {response.text}\")\n",
        "                break\n",
        "\n",
        "        # Move to the next chunk\n",
        "        current_start_date = chunk_end_date\n",
        "\n",
        "        # Respect API rate limit (5 requests/min): sleep longer just to be safe\n",
        "        print(\"Waiting for 1 minute to respect API rate limits...\")\n",
        "        time.sleep(14)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def analyze_sentiment(news_data):\n",
        "    \"\"\"\n",
        "    Uses TextBlob to compute sentiment polarity and subjectivity for each news article.\n",
        "    \"\"\"\n",
        "    analyzed_data = []\n",
        "    for article in news_data:\n",
        "        title = article.get(\"title\", \"\")\n",
        "        description = article.get(\"description\", \"\")\n",
        "        # Combine title and description for sentiment analysis\n",
        "        full_text = f\"{title} {description}\"\n",
        "        sentiment = TextBlob(full_text).sentiment\n",
        "\n",
        "        analyzed_data.append({\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "            \"published_date\": article.get(\"published_utc\", \"\"),\n",
        "            \"sentiment_polarity\": sentiment.polarity,\n",
        "            \"sentiment_subjectivity\": sentiment.subjectivity\n",
        "        })\n",
        "    return analyzed_data\n",
        "\n",
        "def fetch_and_calculate_technical_indicators(ticker, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Fetch historical stock data using yfinance and calculate technical indicators.\n",
        "    Returns a DataFrame with the technical indicators included.\n",
        "    \"\"\"\n",
        "    # Fetch data from Yahoo Finance\n",
        "    df = yf.Ticker(ticker).history(start=start_date, end=end_date)\n",
        "\n",
        "    # Keep only necessary columns\n",
        "    df = df[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "    # Average True Range (ATR)\n",
        "    df['ATR'] = AverageTrueRange(high=df['High'], low=df['Low'], close=df['Close']).average_true_range()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    bb = BollingerBands(close=df['Close'])\n",
        "    df['BB_High'] = bb.bollinger_hband()\n",
        "    df['BB_low'] = bb.bollinger_lband()\n",
        "\n",
        "    # Donchian Channel\n",
        "    dc = DonchianChannel(high=df['High'], low=df['Low'], close=df['Close'])\n",
        "    df['DC_High'] = dc.donchian_channel_hband()\n",
        "    df['DC_low'] = dc.donchian_channel_lband()\n",
        "\n",
        "    # Keltner Channel\n",
        "    kc = KeltnerChannel(high=df['High'], low=df['Low'], close=df['Close'])\n",
        "    df['KC_High'] = kc.keltner_channel_hband()\n",
        "    df['KC_Low'] = kc.keltner_channel_lband()\n",
        "\n",
        "    # Chaikin Volatility\n",
        "    high_low_range = df['High'] - df['Low']\n",
        "    df['Chaikin_volatility'] = (\n",
        "        high_low_range.rolling(window=10).mean() /\n",
        "        high_low_range.rolling(window=10).std()\n",
        "    )\n",
        "\n",
        "    # Historical Volatility\n",
        "    log_returns = np.log(df['Close'] / df['Close'].shift(1))\n",
        "    df['Historical_volatility'] = log_returns.rolling(window=30).std() * np.sqrt(252)\n",
        "\n",
        "    # Standard Deviation\n",
        "    df['Standard_Deviation'] = df['Close'].rolling(window=14).std()\n",
        "\n",
        "    # Williams %R\n",
        "    wr = WilliamsRIndicator(high=df['High'], low=df['Low'], close=df['Close'])\n",
        "    df['Williams_%R'] = wr.williams_r()\n",
        "\n",
        "    # Commodity Channel Index (CCI)\n",
        "    df['CCI'] = (\n",
        "        (df['Close'] - df['Close'].rolling(20).mean())\n",
        "        / (0.015 * df['Close'].rolling(20).std())\n",
        "    )\n",
        "\n",
        "    # RSI-Based Volatility (simple example)\n",
        "    rsi_diff = df['Close'].rolling(window=14).apply(lambda x: max(x) - min(x))\n",
        "    df['RSI_Based_Volatility'] = rsi_diff / df['Close']\n",
        "\n",
        "    # Ulcer Index\n",
        "    df['Ulcer_Index'] = (\n",
        "        (df['Close'] - df['Close'].rolling(window=14).max()) ** 2\n",
        "    ).rolling(window=14).mean()\n",
        "\n",
        "    # True Strength Index (TSI) example\n",
        "    df['TSI'] = (\n",
        "        log_returns.ewm(span=25).mean() /\n",
        "        log_returns.abs().ewm(span=13).mean()\n",
        "    ) * 100\n",
        "\n",
        "    # Fractal Chaos Oscillator\n",
        "    df['Fractal_chaos_Oscillator'] = df['Close'].rolling(window=14).apply(lambda x: np.ptp(x))\n",
        "\n",
        "    # Drop any rows with NaN caused by rolling calculations\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Convert index to a column named \"Date\"\n",
        "    df.reset_index(inplace=True)\n",
        "    df.rename(columns={'Date': 'Date'}, inplace=True)\n",
        "    # Keep date in date format\n",
        "    df['Date'] = pd.to_datetime(df['Date']).dt.date\n",
        "\n",
        "    return df\n",
        "\n",
        "def main():\n",
        "    # === Step 1: Get user inputs ===\n",
        "    ticker = input(\"Enter the stock ticker (e.g., TSLA): \").strip().upper()\n",
        "    start_date = input(\"Enter start date (YYYY-MM-DD): \").strip()\n",
        "    end_date = input(\"Enter end date (YYYY-MM-DD): \").strip()\n",
        "\n",
        "    # === Step 2: Load Polygon API key from Colab secrets ===\n",
        "    api_key = userdata.get('Polygon_Key')\n",
        "    if not api_key:\n",
        "        print(\"Polygon API key not found in userdata. Please set it in Colab secrets.\")\n",
        "        return\n",
        "\n",
        "    # === Step 3: Fetch and process stock data / technical indicators ===\n",
        "    print(f\"\\nFetching stock data for {ticker} from {start_date} to {end_date}...\")\n",
        "    stock_df = fetch_and_calculate_technical_indicators(ticker, start_date, end_date)\n",
        "\n",
        "    # === Step 4: Fetch news data, perform sentiment analysis ===\n",
        "    print(f\"\\nFetching news for {ticker} from {start_date} to {end_date}...\")\n",
        "    news_data = get_historical_news_chunked(ticker, start_date, end_date, api_key, limit=1000)\n",
        "\n",
        "    if not news_data:\n",
        "        print(\"No news data found. Proceeding without sentiment data.\")\n",
        "        final_csv = f\"combined_{ticker}_{start_date}_to_{end_date}.csv\"\n",
        "        stock_df.to_csv(final_csv, index=False)\n",
        "        print(f\"Output saved to {final_csv}\")\n",
        "        return\n",
        "\n",
        "    print(\"\\nPerforming sentiment analysis on fetched news...\")\n",
        "    analyzed_news = analyze_sentiment(news_data)\n",
        "    sentiment_df = pd.DataFrame(analyzed_news)\n",
        "\n",
        "    # Convert published_date to datetime, then to just date\n",
        "    sentiment_df['published_date'] = pd.to_datetime(sentiment_df['published_date'], errors='coerce')\n",
        "    sentiment_df['Date'] = sentiment_df['published_date'].dt.date\n",
        "\n",
        "    # Group sentiment by Date to get daily average (or any other aggregation you prefer)\n",
        "    daily_sentiment = sentiment_df.groupby('Date').agg({\n",
        "        'sentiment_polarity': 'mean',\n",
        "        'sentiment_subjectivity': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # === Step 5: Merge sentiment with stock data ===\n",
        "    print(\"\\nMerging sentiment data with stock data...\")\n",
        "    combined_df = pd.merge(\n",
        "        stock_df,\n",
        "        daily_sentiment,\n",
        "        on='Date',\n",
        "        how='left'  # keep all stock rows, match sentiment where available\n",
        "    )\n",
        "\n",
        "    # === Step 6: Additional Preprocessing for LSTM/RNN/CNN ===\n",
        "    print(\"\\nPerforming final preprocessing on the combined data...\")\n",
        "\n",
        "    # (A) Fill any remaining NaN values (e.g., sentiment on days without news)\n",
        "    combined_df.fillna(0, inplace=True)\n",
        "\n",
        "    # (B) Scale all numeric columns except 'Date' and sentiment scores\n",
        "    numeric_cols = combined_df.select_dtypes(include=[np.number]).columns.difference(['sentiment_polarity', 'sentiment_subjectivity'])\n",
        "    scaler = MinMaxScaler()\n",
        "    combined_df[numeric_cols] = scaler.fit_transform(combined_df[numeric_cols])\n",
        "\n",
        "    # Save the scaler\n",
        "    joblib.dump(scaler, 'scaler.pkl')\n",
        "\n",
        "    # Reshape for RNN/LSTM/CNN (batch_size, timesteps, features)\n",
        "    X_values = combined_df.drop(columns=['Date']).values\n",
        "    X_reshaped = X_values.reshape((X_values.shape[0], 1, X_values.shape[1]))\n",
        "\n",
        "    # === Step 7: Save final combined data to CSV ===\n",
        "    final_csv = f\"combined_{ticker}_{start_date}_to_{end_date}.csv\"\n",
        "    combined_df.to_csv(final_csv, index=False)\n",
        "\n",
        "    print(f\"\\nAll done! Preprocessed data (scaled technical indicators + daily sentiment) saved to {final_csv} and scaler saved to scaler.pkl\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}