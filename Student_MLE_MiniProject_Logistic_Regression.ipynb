{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzr5Eo_qxHQi"
      },
      "source": [
        "# Mini Project: Logistic Regression\n",
        "\n",
        "Logistic Regression models the probability that a given input belongs to a certain class. It's suitable when the target variable is categorical and represents two classes (e.g., 0 or 1, True or False, Yes or No), although it can also be extended for problems with more than two classes. The key idea behind logistic regression is to model the relationship between the input variables (features) and the probability of the outcome.\n",
        "\n",
        "In logistic regression, the linear combination of input features is transformed using a logistic function (also known as the sigmoid function), which ensures that the output is between 0 and 1. This output can be interpreted as the probability of the instance belonging to a particular class.\n",
        "\n",
        "**Advantages of Logistic Regression:**\n",
        "\n",
        "1. **Simple and Interpretable:** Logistic regression is a straightforward algorithm that's relatively easy to understand and interpret. The output is the probability of belonging to a certain class, and the coefficients associated with each feature provide insights into feature importance.\n",
        "\n",
        "2. **Efficient for Small Datasets:** It works well with small datasets where the number of samples is not very large. It's less prone to overfitting in such cases compared to complex models.\n",
        "\n",
        "3. **Works Well for Linearly Separable Data:** When the classes are separable by a linear decision boundary, logistic regression can perform quite well.\n",
        "\n",
        "4. **Good Starting Point:** Logistic regression is often used as a starting point for understanding a classification problem. It can serve as a baseline model against which more complex algorithms can be compared.\n",
        "\n",
        "5. **Regularization:** Logistic regression can be regularized to prevent overfitting. Regularization techniques like L1 (Lasso) and L2 (Ridge) regularization can be applied to control the complexity of the model.\n",
        "\n",
        "6. **Probability Estimation:** Logistic regression not only provides class predictions but also outputs the probability of the prediction. This can be useful for decision-making in cases where the cost of misclassification varies.\n",
        "\n",
        "However, it's important to note that logistic regression also has its limitations. It assumes a linear relationship between features and the log-odds of the target variable, which might not be suitable for highly complex relationships. Additionally, it might struggle with handling non-linear data without feature transformations. In such cases, more advanced techniques like decision trees, random forests, or neural networks might be more appropriate.\n",
        "\n",
        "In this project you'll get some experience building a logistic regression model for the [Wisconsin Breast Cancer Detection dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html). Note, the task of training a logistic regression model has largely been asbtracted away by libraries like Scikit-Learn. In this mini-project we'll focus more on model evaluation and interpretation.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's import all the libraries we'll be using."
      ],
      "metadata": {
        "id": "2JZGx1fBfi08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import plotly.express as px"
      ],
      "metadata": {
        "id": "cEmNpWHLLZIQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression, like many statistical methods, comes with certain assumptions about the underlying data. Here are the main assumptions associated with logistic regression:\n",
        "\n",
        "1. **Binary Outcome:** Logistic regression is designed for binary classification problems, where the dependent variable (target) should have two categories or levels. If you have a multi-class problem, you would typically use multinomial logistic regression or other appropriate techniques.\n",
        "\n",
        "2. **Independence of Observations:** The observations (samples) should be independent of each other. This means that the outcome of one observation should not be influenced by the outcome of another observation. This assumption is often met when the data is collected using random sampling or experimental designs.\n",
        "\n",
        "3. **Linearity of Log-Odds:** The relationship between the log-odds of the outcome and the predictors should be linear. In other words, the log-odds of the outcome variable should change linearly with changes in the predictor variables. This assumption can be checked by examining scatter plots and residual plots.\n",
        "\n",
        "4. **No Multicollinearity:** There should not be high multicollinearity among the predictor variables. Multicollinearity can make it difficult to determine the individual effect of each predictor on the outcome. Techniques like variance inflation factor (VIF) can be used to assess multicollinearity.\n",
        "\n",
        "5. **Large Sample Size:** While logistic regression is generally more robust to violations of assumptions compared to linear regression, having a reasonably large sample size helps in obtaining stable and reliable parameter estimates.\n",
        "\n",
        "6. **Sufficient Variability in the Outcome:** The outcome variable should exhibit variation across different values of the predictor variables. If all values of a predictor are the same within a level of the outcome, the model may not be able to estimate the effect of that predictor.\n",
        "\n",
        "7. **No Extreme Outliers:** Extreme outliers can influence the estimation of coefficients and affect the overall performance of the model. It's a good practice to identify and handle outliers before fitting the model.\n",
        "\n",
        "It's important to note that while these assumptions are important to understand, logistic regression is often used in real-world scenarios where some of these assumptions may not be perfectly met. In such cases, it's crucial to assess the impact of potential violations on the model's results and make informed decisions about the model's suitability and reliability.\n",
        "\n",
        "If the assumptions are significantly violated, it might be worth considering other techniques like decision trees, random forests, or support vector machines, which might be more robust to certain types of data characteristics. One of the best ways to see if logistic regression is suitable for a problem is to simply train a logistic regression model and evaluate it on test data."
      ],
      "metadata": {
        "id": "SbzD9xCRgMY0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are your tasks:\n",
        "\n",
        "1. Load the [breast cancer data](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) into a Pandas dataframe and create variables for the features and target.\n",
        "2. Do a little exploratory data analysis to help familiarize yourself with the data. Look at the first few rows of data, for example. Generate some summary statistics for each feature. Look at the distribution of the target variable. Maybe create a pair-plot for some of the variables. Create a [heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html) of correlation between features. Is the multicollinearity assumption broken? Also, generate some boxplots to see how feature distributions change for each target. This part is a bit open-ended. Be creative!"
      ],
      "metadata": {
        "id": "n1fLmRtfgmdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and split into feature and target variables\n",
        "data = load_breast_cancer()\n",
        "#print(data)\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "X = df\n",
        "y = data.target\n",
        "#print(X)\n",
        "#print(y)"
      ],
      "metadata": {
        "id": "xzvYGCtqLhks"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View first 5 rows of the data\n",
        "print(X.head())\n",
        "print(y[:5])"
      ],
      "metadata": {
        "id": "cAzosOt3Q15Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f731595-37ea-4b3d-b5c2-19a023b8c3e3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
            "0                 0.07871  ...         25.38          17.33           184.60   \n",
            "1                 0.05667  ...         24.99          23.41           158.80   \n",
            "2                 0.05999  ...         23.57          25.53           152.50   \n",
            "3                 0.09744  ...         14.91          26.50            98.87   \n",
            "4                 0.05883  ...         22.54          16.67           152.20   \n",
            "\n",
            "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
            "0      2019.0            0.1622             0.6656           0.7119   \n",
            "1      1956.0            0.1238             0.1866           0.2416   \n",
            "2      1709.0            0.1444             0.4245           0.4504   \n",
            "3       567.7            0.2098             0.8663           0.6869   \n",
            "4      1575.0            0.1374             0.2050           0.4000   \n",
            "\n",
            "   worst concave points  worst symmetry  worst fractal dimension  \n",
            "0                0.2654          0.4601                  0.11890  \n",
            "1                0.1860          0.2750                  0.08902  \n",
            "2                0.2430          0.3613                  0.08758  \n",
            "3                0.2575          0.6638                  0.17300  \n",
            "4                0.1625          0.2364                  0.07678  \n",
            "\n",
            "[5 rows x 30 columns]\n",
            "[0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How frequently does the positive target occur?\n",
        "Positive = np.count_nonzero(y == 1)\n",
        "print(f\"amount of positives {Positive}\")\n",
        "Positive_rate = Positive / y.shape[0]\n",
        "print(f\"Percentage of Positives {Positive_rate}\")"
      ],
      "metadata": {
        "id": "Rb0Dyik9OXvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54de7934-0bfb-4993-f827-e6e05e507db6"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amount of positives 357\n",
            "Percentage of Positives 0.6274165202108963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summary statistics for the data\n",
        "X_summary = X.describe()\n",
        "print(X_summary)"
      ],
      "metadata": {
        "id": "LvD5RS4dONlM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de7d6324-0e71-4860-a6bb-cd9bb4394d46"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       mean radius  mean texture  mean perimeter    mean area  \\\n",
            "count   569.000000    569.000000      569.000000   569.000000   \n",
            "mean     14.127292     19.289649       91.969033   654.889104   \n",
            "std       3.524049      4.301036       24.298981   351.914129   \n",
            "min       6.981000      9.710000       43.790000   143.500000   \n",
            "25%      11.700000     16.170000       75.170000   420.300000   \n",
            "50%      13.370000     18.840000       86.240000   551.100000   \n",
            "75%      15.780000     21.800000      104.100000   782.700000   \n",
            "max      28.110000     39.280000      188.500000  2501.000000   \n",
            "\n",
            "       mean smoothness  mean compactness  mean concavity  mean concave points  \\\n",
            "count       569.000000        569.000000      569.000000           569.000000   \n",
            "mean          0.096360          0.104341        0.088799             0.048919   \n",
            "std           0.014064          0.052813        0.079720             0.038803   \n",
            "min           0.052630          0.019380        0.000000             0.000000   \n",
            "25%           0.086370          0.064920        0.029560             0.020310   \n",
            "50%           0.095870          0.092630        0.061540             0.033500   \n",
            "75%           0.105300          0.130400        0.130700             0.074000   \n",
            "max           0.163400          0.345400        0.426800             0.201200   \n",
            "\n",
            "       mean symmetry  mean fractal dimension  ...  worst radius  \\\n",
            "count     569.000000              569.000000  ...    569.000000   \n",
            "mean        0.181162                0.062798  ...     16.269190   \n",
            "std         0.027414                0.007060  ...      4.833242   \n",
            "min         0.106000                0.049960  ...      7.930000   \n",
            "25%         0.161900                0.057700  ...     13.010000   \n",
            "50%         0.179200                0.061540  ...     14.970000   \n",
            "75%         0.195700                0.066120  ...     18.790000   \n",
            "max         0.304000                0.097440  ...     36.040000   \n",
            "\n",
            "       worst texture  worst perimeter   worst area  worst smoothness  \\\n",
            "count     569.000000       569.000000   569.000000        569.000000   \n",
            "mean       25.677223       107.261213   880.583128          0.132369   \n",
            "std         6.146258        33.602542   569.356993          0.022832   \n",
            "min        12.020000        50.410000   185.200000          0.071170   \n",
            "25%        21.080000        84.110000   515.300000          0.116600   \n",
            "50%        25.410000        97.660000   686.500000          0.131300   \n",
            "75%        29.720000       125.400000  1084.000000          0.146000   \n",
            "max        49.540000       251.200000  4254.000000          0.222600   \n",
            "\n",
            "       worst compactness  worst concavity  worst concave points  \\\n",
            "count         569.000000       569.000000            569.000000   \n",
            "mean            0.254265         0.272188              0.114606   \n",
            "std             0.157336         0.208624              0.065732   \n",
            "min             0.027290         0.000000              0.000000   \n",
            "25%             0.147200         0.114500              0.064930   \n",
            "50%             0.211900         0.226700              0.099930   \n",
            "75%             0.339100         0.382900              0.161400   \n",
            "max             1.058000         1.252000              0.291000   \n",
            "\n",
            "       worst symmetry  worst fractal dimension  \n",
            "count      569.000000               569.000000  \n",
            "mean         0.290076                 0.083946  \n",
            "std          0.061867                 0.018061  \n",
            "min          0.156500                 0.055040  \n",
            "25%          0.250400                 0.071460  \n",
            "50%          0.282200                 0.080040  \n",
            "75%          0.317900                 0.092080  \n",
            "max          0.663800                 0.207500  \n",
            "\n",
            "[8 rows x 30 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_data = df.sample(n=10, random_state=42)\n",
        "#sns.pairplot(sampled_data)\n",
        "#I dont think this is how its supposed to look. It graphed 30 by 30 graphs"
      ],
      "metadata": {
        "id": "ZaMEO8Mojxu2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pairplot for the first few features\n",
        "#sns.pairplot(df)\n",
        "#sns.pairplot(sampled_data, x_vars = [mean radius, mean texture, mean perimeter, mean smoothness], y_vars = sampled_data.target)"
      ],
      "metadata": {
        "id": "C08Lg83rRQuD"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a correlation coefficeint heatmap\n"
      ],
      "metadata": {
        "id": "CLgRP5D-RpuW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a boxplot for mean radius by target type\n",
        "mean_radius_positive = df[y == 1]['mean radius']\n",
        "mean_radius_negative = df[y == 0]['mean radius']\n",
        "plt.boxplot(mean_radius_positive)\n"
      ],
      "metadata": {
        "id": "mbWIPJThR2FT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "outputId": "670e8c3e-7f21-4075-bc68-e1b1b867b4b8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'whiskers': [<matplotlib.lines.Line2D at 0x7d4feb7c2950>,\n",
              "  <matplotlib.lines.Line2D at 0x7d4feb7c2bf0>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7d4feb7c2e90>,\n",
              "  <matplotlib.lines.Line2D at 0x7d4feb7c3130>],\n",
              " 'boxes': [<matplotlib.lines.Line2D at 0x7d4feb7c26b0>],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7d4feb7c33d0>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7d4feb7c3670>],\n",
              " 'means': []}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(mean_radius_negative)"
      ],
      "metadata": {
        "id": "Td6Ka4TfIaYC",
        "outputId": "7e1a9ec8-4c4f-4b69-d590-a729a61becf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'whiskers': [<matplotlib.lines.Line2D at 0x7d4feb61baf0>,\n",
              "  <matplotlib.lines.Line2D at 0x7d4feb61bd90>],\n",
              " 'caps': [<matplotlib.lines.Line2D at 0x7d4feb64c070>,\n",
              "  <matplotlib.lines.Line2D at 0x7d4feb64c310>],\n",
              " 'boxes': [<matplotlib.lines.Line2D at 0x7d4feb61b850>],\n",
              " 'medians': [<matplotlib.lines.Line2D at 0x7d4feb64c5b0>],\n",
              " 'fliers': [<matplotlib.lines.Line2D at 0x7d4feb64c850>],\n",
              " 'means': []}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a better feel for the data, it's time to attempt to build a logistic regression model.\n",
        "\n",
        "1. Use [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to create a training and test sets for the data.\n",
        "2. Use [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to train a model on the training data. Make sure you understand the inputs to the model. Try using the \"liblinear\" solver here."
      ],
      "metadata": {
        "id": "B0zluuwgjNYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and test sets\n",
        "X_test, X_train, y_test, y_train = train_test_split(X, y, test_size = .3, random_state = 42)"
      ],
      "metadata": {
        "id": "PbBXtjw9QP-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and train logistic regression model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train, y_train)\n",
        "y_pred = log_reg.predict(X_test)"
      ],
      "metadata": {
        "id": "IoNXSjQaUfw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, training a logistic regression model is simple. The more important task is evaluating the model and determining if it's any good. For classification problems, a good starting point for model evaluation is the [confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n",
        "\n",
        "A confusion matrix is a fundamental tool for evaluating the performance of a classification model. It provides a clear and detailed breakdown of how well a model's predictions align with the actual outcomes in a binary classification problem. It's particularly useful for understanding the types of errors a model is making.\n",
        "\n",
        "A confusion matrix is typically presented as a table with four entries:\n",
        "\n",
        "- **True Positives (TP):** The number of instances that were correctly predicted as positive (belonging to the positive class).\n",
        "\n",
        "- **True Negatives (TN):** The number of instances that were correctly predicted as negative (belonging to the negative class).\n",
        "\n",
        "- **False Positives (FP):** Also known as a Type I error. The number of instances that were predicted as positive but actually belong to the negative class.\n",
        "\n",
        "- **False Negatives (FN):** Also known as a Type II error. The number of instances that were predicted as negative but actually belong to the positive class.\n",
        "\n",
        "Here's how these four components fit into the confusion matrix:\n",
        "\n",
        "```\n",
        "                Predicted\n",
        "               |  Positive  |  Negative\n",
        "Actual  Positive |    TP      |    FN\n",
        "        Negative |    FP      |    TN\n",
        "```\n",
        "\n",
        "Each cell of the confusion matrix represents a specific classification outcome. The goal is to have as many instances as possible in the TP and TN cells, and as few as possible in the FP and FN cells.\n",
        "\n",
        "From the confusion matrix, several evaluation metrics can be calculated:\n",
        "\n",
        "- **Accuracy:** The proportion of correctly classified instances out of the total instances.\n",
        "\n",
        "   `Accuracy = (TP + TN) / (TP + TN + FP + FN)`\n",
        "\n",
        "- **Precision:** The proportion of correctly predicted positive instances out of all predicted positive instances. It measures the model's ability to avoid false positives.\n",
        "\n",
        "   `Precision = TP / (TP + FP)`\n",
        "\n",
        "- **Recall (Sensitivity or True Positive Rate):** The proportion of correctly predicted positive instances out of all actual positive instances. It measures the model's ability to capture all positive instances.\n",
        "\n",
        "   `Recall = TP / (TP + FN)`\n",
        "\n",
        "- **F1-Score:** The harmonic mean of precision and recall. It provides a balanced measure that takes into account both false positives and false negatives.\n",
        "\n",
        "   `F1-Score = 2 * (Precision * Recall) / (Precision + Recall)`\n",
        "\n",
        "Confusion matrices provide valuable insights into the strengths and weaknesses of a classification model. They allow you to understand where the model is making mistakes and guide further improvements or adjustments."
      ],
      "metadata": {
        "id": "UVoRpZbHmGUd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are your tasks:\n",
        "\n",
        "1. Use your model to make predictions on the test data.\n",
        "2. Generate a confusion matrix with the test results. How many false positives and false negatives did the model predict?\n",
        "3. Use [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) to generate further analysis of your model's predictions. Make sure you understand everything in the report and are able to explain what all the metrics mean.\n",
        "\n",
        "Note, the macro average in the report calculates the metrics independently for each class and then takes the average across all classes. In other words, it treats all classes equally, regardless of their frequency in the dataset. This can be useful when you want to assess the model's overall performance without being biased by the class imbalances.\n",
        "\n",
        "The weighted average in the report, on the other hand, calculates the metrics for each class and then takes the average, weighted by the number of true instances for each class. This gives more weight to classes with more instances, which can be particularly useful in imbalanced datasets where some classes might have much fewer instances than others."
      ],
      "metadata": {
        "id": "3BZF4djWEUIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model"
      ],
      "metadata": {
        "id": "az80VdlUXPyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a confusion matrix"
      ],
      "metadata": {
        "id": "jZKYFhcyXQw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TpNI1wmIb_w"
      },
      "outputs": [],
      "source": [
        "# Generate a classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature importance refers to the process of determining and quantifying the contribution of each feature (also known as predictor variable or attribute) in a machine learning model towards making accurate predictions. It helps in understanding which features have the most significant impact on the model's output and can be crucial for interpreting and explaining the model's behavior.\n",
        "\n",
        "In logistic regression models, you can calculate feature importance by examining the coefficients associated with each feature. These coefficients indicate the change in the log-odds of the target variable for a one-unit change in the corresponding feature, while keeping other features constant. The magnitude of the coefficient reflects the strength of the impact that the feature has on the predicted outcome.\n",
        "\n",
        "The magnitude of the coefficients indicates the importance of each feature. Larger magnitudes imply a stronger impact on the predicted probability of the positive class.\n",
        "\n",
        "**Positive Coefficient**: An increase in the feature value leads to an increase in the log-odds of the positive class, implying a higher probability of belonging to the positive class.\n",
        "\n",
        "**Negative Coefficient**: An increase in the feature value leads to a decrease in the log-odds of the positive class, implying a lower probability of belonging to the positive class.\n",
        "\n",
        "Remember that the scale of the features matters when interpreting coefficients. If features are on different scales, their coefficients won't be directly comparable. This is where normalization can be helpful. Also, keep in mind that this interpretation assumes a linear relationship between the features and the log-odds of the target variable. If your logistic regression model includes interactions or polynomial terms, the interpretation can become more complex. Additionally, be cautious about interpreting coefficients as causal relationships, as logistic regression only captures associations, not causal effects."
      ],
      "metadata": {
        "id": "fsXZ_b4THXpy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are your tasks:\n",
        "\n",
        "1. Extract the model coefficients from your trained model.\n",
        "2. Normalize the coefficients by the standard deviation of each feature in the training data.\n",
        "3. Sort feature names and coefficients by absolute value of coefficients.\n",
        "4. Visualize the feature importances by creating a horizontal bar chart using e.g. [barh](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.barh.html). Based on magnitude, what appears to be the most important predictor of cancer in this dataset?"
      ],
      "metadata": {
        "id": "Cl353zFKK-oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract coefficients"
      ],
      "metadata": {
        "id": "i0_8RkLSXU9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the coefficients by the standard deviation"
      ],
      "metadata": {
        "id": "Pc-uQLEAXXWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort feature names and coefficients by absolute value of coefficients"
      ],
      "metadata": {
        "id": "jnAmNhTiXZWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize feature importances"
      ],
      "metadata": {
        "id": "TKjQzru3MqzB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}