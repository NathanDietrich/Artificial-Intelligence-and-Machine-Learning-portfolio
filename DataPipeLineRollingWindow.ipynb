{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZ9SfGuQ8gvQyX+JbG1NRV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NathanDietrich/Artificial-Intelligence-and-Machine-Learning-portfolio/blob/main/DataPipeLineRollingWindow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M04XYdOZ3QDe",
        "outputId": "2d391eec-42c8-4700-ea9d-914f73b78705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.54)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=21cdb0937c20725cb6a7654e1a366c91a18d9b83d474c9939671074810d7b285\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/d7/29/7781cc5eb9a3659d032d7d15bdd0f49d07d2b24fec29f44bc4\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance textblob ta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs5iSJiPDOyr",
        "outputId": "8438d8dd-809c-4ae9-8bc7-7a4b39a79ef9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# ✅ Enable GPU & Force TensorFlow to Use It\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "        print(f\"✅ GPU detected: {gpu_devices[0].name} (Memory Growth Enabled)\")\n",
        "    except:\n",
        "        print(\"⚠️ GPU found, but could not enable memory growth.\")\n",
        "else:\n",
        "    print(\"❌ No GPU detected. Running on CPU.\")\n",
        "\n",
        "# ✅ Enable Mixed Precision for Faster Training (Uses float16 on GPU)\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"✅ Mixed Precision Enabled (float16) for Faster GPU Training\")\n",
        "\n",
        "# ✅ Check GPU Usage Before Training\n",
        "!nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n",
        "\n",
        "# ✅ Function to Monitor GPU Usage Live\n",
        "def monitor_gpu():\n",
        "    print(\"\\n🔍 Checking GPU Usage...\")\n",
        "    os.system(\"nvidia-smi --query-gpu=memory.used,memory.total --format=csv\")\n",
        "\n",
        "monitor_gpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0C2iFHWDRtR",
        "outputId": "b83fb8a7-13d6-472f-eb8b-4ca89dec3b36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GPU detected: /physical_device:GPU:0 (Memory Growth Enabled)\n",
            "✅ Mixed Precision Enabled (float16) for Faster GPU Training\n",
            "memory.used [MiB], memory.total [MiB]\n",
            "2 MiB, 15360 MiB\n",
            "\n",
            "🔍 Checking GPU Usage...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# ===== File selection functions =====\n",
        "def get_valid_files(directory=\"/content/drive/MyDrive/StockData\"):\n",
        "    \"\"\"Detects and lists valid raw stock data files in the specified directory.\"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        print(f\"❌ Directory {directory} not found. Check if Google Drive is mounted correctly.\")\n",
        "        return {}\n",
        "    uploaded_files = os.listdir(directory)\n",
        "    pattern = re.compile(r\"^([A-Za-z0-9]+)_\\d{4}-\\d{2}-\\d{2}_to_\\d{4}-\\d{2}-\\d{2}_raw\\.csv$\", re.IGNORECASE)\n",
        "    valid_files = {os.path.join(directory, f): pattern.match(f).group(1).upper()\n",
        "                   for f in uploaded_files if pattern.match(f)}\n",
        "    if not valid_files:\n",
        "        print(\"❌ No valid raw stock data files found in the folder.\")\n",
        "    return valid_files\n",
        "\n",
        "def select_file(directory=\"/content/drive/MyDrive/StockData\"):\n",
        "    \"\"\"Selects a raw stock data file from the specified directory.\"\"\"\n",
        "    valid_files = get_valid_files(directory)\n",
        "    if not valid_files:\n",
        "        print(\"❌ No valid raw stock data files found. Please upload one to the folder.\")\n",
        "        return None, None\n",
        "    if len(valid_files) == 1:\n",
        "        full_path = list(valid_files.keys())[0]\n",
        "        stock_ticker = valid_files[full_path]\n",
        "        print(f\"✅ Automatically selected: {os.path.basename(full_path)} ({stock_ticker})\")\n",
        "        return full_path, stock_ticker\n",
        "    print(\"🔍 Multiple stock raw files detected. Please choose one:\")\n",
        "    full_paths = list(valid_files.keys())\n",
        "    for i, full_path in enumerate(full_paths):\n",
        "        print(f\"{i + 1}. {os.path.basename(full_path)} ({valid_files[full_path]})\")\n",
        "    choice = int(input(\"Enter the number of the file to use: \")) - 1\n",
        "    selected_path = full_paths[choice]\n",
        "    stock_ticker = valid_files[selected_path]\n",
        "    print(f\"✅ Selected: {os.path.basename(selected_path)} ({stock_ticker})\")\n",
        "    return selected_path, stock_ticker\n",
        "\n",
        "# ===== Technical Indicator Functions =====\n",
        "def compute_sma(df, window=14):\n",
        "    return df['Close'].rolling(window=window).mean()\n",
        "\n",
        "def compute_ema(df, span=14):\n",
        "    return df['Close'].ewm(span=span, adjust=False).mean()\n",
        "\n",
        "def compute_rsi(df, window=14):\n",
        "    delta = df['Close'].diff()\n",
        "    gain = delta.clip(lower=0)\n",
        "    loss = -delta.clip(upper=0)\n",
        "    avg_gain = gain.rolling(window=window, min_periods=window).mean()\n",
        "    avg_loss = loss.rolling(window=window, min_periods=window).mean()\n",
        "    rs = avg_gain / (avg_loss + 1e-10)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "def compute_macd(df, span_short=12, span_long=26, span_signal=9):\n",
        "    ema_short = df['Close'].ewm(span=span_short, adjust=False).mean()\n",
        "    ema_long = df['Close'].ewm(span=span_long, adjust=False).mean()\n",
        "    macd_line = ema_short - ema_long\n",
        "    signal_line = macd_line.ewm(span=span_signal, adjust=False).mean()\n",
        "    return macd_line, signal_line\n",
        "\n",
        "def compute_bollinger_bands(df, window=20, num_std=2):\n",
        "    sma = df['Close'].rolling(window=window).mean()\n",
        "    rstd = df['Close'].rolling(window=window).std()\n",
        "    upper_band = sma + num_std * rstd\n",
        "    lower_band = sma - num_std * rstd\n",
        "    return sma, upper_band, lower_band\n",
        "\n",
        "# ===== Preprocessing for Predictive Problem =====\n",
        "def preprocess_data(df):\n",
        "    # Convert Date column to datetime if needed\n",
        "    if not np.issubdtype(df['Date'].dtype, np.datetime64):\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "    df.sort_values('Date', inplace=True)\n",
        "\n",
        "    # Compute technical indicators\n",
        "    df['SMA_14'] = compute_sma(df, window=14)\n",
        "    df['EMA_14'] = compute_ema(df, span=14)\n",
        "    df['RSI_14'] = compute_rsi(df, window=14)\n",
        "    df['MACD'], df['MACD_Signal'] = compute_macd(df)\n",
        "    bb_sma, bb_upper, bb_lower = compute_bollinger_bands(df)\n",
        "    df['BB_Middle'] = bb_sma\n",
        "    df['BB_Upper'] = bb_upper\n",
        "    df['BB_Lower'] = bb_lower\n",
        "\n",
        "    # Fill missing values and drop any remaining NAs\n",
        "    df.ffill(inplace=True)\n",
        "    df.bfill(inplace=True)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Shift target column by one day (predict tomorrow's Close)\n",
        "    df['Close_tomorrow'] = df['Close'].shift(-1)\n",
        "    df = df.iloc[:-1]  # Drop last row as it has no next day value\n",
        "\n",
        "    # Drop the Date column (keep Close as a feature)\n",
        "    df.drop(columns=['Date'], inplace=True)\n",
        "    return df\n",
        "\n",
        "def split_data(df, train_ratio=0.7, val_ratio=0.15):\n",
        "    \"\"\"Splits the DataFrame into train, validation, and test sets.\"\"\"\n",
        "    n = len(df)\n",
        "    train_end = int(n * train_ratio)\n",
        "    val_end = train_end + int(n * val_ratio)\n",
        "    train_df = df.iloc[:train_end]\n",
        "    val_df = df.iloc[train_end:val_end]\n",
        "    test_df = df.iloc[val_end:]\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "def scale_train_val_test(train_df, val_df, test_df, target_col='Close_tomorrow'):\n",
        "    \"\"\"\n",
        "    Separates the target column from features and fits separate MinMaxScalers\n",
        "    on the training set for X and y. Then transforms the train, val, and test sets.\n",
        "    \"\"\"\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "    sentiment_cols = ['sentiment_polarity', 'sentiment_subjectivity']\n",
        "\n",
        "    # For training set, separate target and features\n",
        "    if target_col not in train_df.columns:\n",
        "        raise ValueError(f\"Target column '{target_col}' not found.\")\n",
        "    y_train = train_df[target_col]\n",
        "    X_train = train_df.drop(columns=[target_col])\n",
        "\n",
        "    # Determine feature columns for scaling (exclude sentiments)\n",
        "    x_cols = [col for col in X_train.columns if col not in sentiment_cols]\n",
        "\n",
        "    # Fit scaler on training features\n",
        "    scaler_x = MinMaxScaler(feature_range=(-1, 1))\n",
        "    X_train_scaled = scaler_x.fit_transform(X_train[x_cols])\n",
        "    X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=x_cols, index=X_train.index)\n",
        "    # Add sentiment columns unchanged\n",
        "    for col in sentiment_cols:\n",
        "        if col in X_train.columns:\n",
        "            X_train_scaled_df[col] = X_train[col].values\n",
        "\n",
        "    # Fit scaler on training target\n",
        "    scaler_y = MinMaxScaler(feature_range=(-1, 1))\n",
        "    y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1))\n",
        "\n",
        "    # Transform function for val/test\n",
        "    def transform_features(df_in):\n",
        "        y_temp = df_in[target_col]\n",
        "        X_temp = df_in.drop(columns=[target_col])\n",
        "        X_temp_scaled = scaler_x.transform(X_temp[x_cols])\n",
        "        X_temp_scaled_df = pd.DataFrame(X_temp_scaled, columns=x_cols, index=X_temp.index)\n",
        "        for col in sentiment_cols:\n",
        "            if col in X_temp.columns:\n",
        "                X_temp_scaled_df[col] = X_temp[col].values\n",
        "        y_temp_scaled = scaler_y.transform(y_temp.values.reshape(-1, 1))\n",
        "        return X_temp_scaled_df, y_temp_scaled\n",
        "\n",
        "    X_val_scaled_df, y_val_scaled = transform_features(val_df)\n",
        "    X_test_scaled_df, y_test_scaled = transform_features(test_df)\n",
        "\n",
        "    return (X_train_scaled_df, y_train_scaled,\n",
        "            X_val_scaled_df, y_val_scaled,\n",
        "            X_test_scaled_df, y_test_scaled,\n",
        "            scaler_x, scaler_y)\n",
        "\n",
        "# ===== New: Create Time-Series Sequences =====\n",
        "def create_sequences(X, y, window_size=10):\n",
        "    \"\"\"\n",
        "    Create sequences from features X and targets y.\n",
        "    X: pandas DataFrame or numpy array of shape (n_samples, n_features)\n",
        "    y: numpy array of shape (n_samples, 1)\n",
        "    Returns:\n",
        "      X_seq: numpy array of shape (n_samples - window_size, window_size, n_features)\n",
        "      y_seq: numpy array of shape (n_samples - window_size, 1)\n",
        "    \"\"\"\n",
        "    X_arr = X.values if isinstance(X, pd.DataFrame) else X\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(X_arr) - window_size):\n",
        "        sequences.append(X_arr[i:i+window_size])\n",
        "        targets.append(y[i+window_size])  # predict the target following the window\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "# ===== New: Save Pipeline Outputs as Sequences =====\n",
        "def save_pipeline_outputs_sequences(stock_ticker, scaler_x, scaler_y,\n",
        "                                    X_train_seq, X_val_seq, X_test_seq,\n",
        "                                    y_train_seq, y_val_seq, y_test_seq):\n",
        "    \"\"\"\n",
        "    Saves the two scalers and sequence dataset splits in the stock folder.\n",
        "    Overwrites the old files.\n",
        "    \"\"\"\n",
        "    base_path = \"/content/drive/MyDrive/stocks\"\n",
        "    stock_path = os.path.join(base_path, stock_ticker)\n",
        "    os.makedirs(stock_path, exist_ok=True)\n",
        "\n",
        "    # Save scalers (overwrite old files)\n",
        "    scaler_x_path = os.path.join(stock_path, \"scaler_x_stock.pkl\")\n",
        "    with open(scaler_x_path, \"wb\") as f:\n",
        "        pickle.dump(scaler_x, f)\n",
        "    print(f\"✅ Saved scaler_x to {scaler_x_path}\")\n",
        "\n",
        "    scaler_y_path = os.path.join(stock_path, \"scaler_y.pkl\")\n",
        "    with open(scaler_y_path, \"wb\") as f:\n",
        "        pickle.dump(scaler_y, f)\n",
        "    print(f\"✅ Saved scaler_y to {scaler_y_path}\")\n",
        "\n",
        "    # Save sequence splits as .npy files\n",
        "    np.save(os.path.join(stock_path, \"X_train.npy\"), X_train_seq)\n",
        "    print(f\"✅ Saved X_train.npy (sequences) to {stock_path}\")\n",
        "    np.save(os.path.join(stock_path, \"X_val.npy\"), X_val_seq)\n",
        "    print(f\"✅ Saved X_val.npy (sequences) to {stock_path}\")\n",
        "    np.save(os.path.join(stock_path, \"X_test.npy\"), X_test_seq)\n",
        "    print(f\"✅ Saved X_test.npy (sequences) to {stock_path}\")\n",
        "\n",
        "    np.save(os.path.join(stock_path, \"y_train.npy\"), y_train_seq)\n",
        "    print(f\"✅ Saved y_train.npy (sequences) to {stock_path}\")\n",
        "    np.save(os.path.join(stock_path, \"y_val.npy\"), y_val_seq)\n",
        "    print(f\"✅ Saved y_val.npy (sequences) to {stock_path}\")\n",
        "    np.save(os.path.join(stock_path, \"y_test.npy\"), y_test_seq)\n",
        "    print(f\"✅ Saved y_test.npy (sequences) to {stock_path}\")\n",
        "\n",
        "    print(f\"✅ All sequence dataset splits saved in folder: {stock_path}\")\n",
        "\n",
        "# ===== New: Process All Stock Files =====\n",
        "def process_all_stock_files():\n",
        "    valid_files = get_valid_files()  # uses default directory \"/content/drive/MyDrive/StockData\"\n",
        "    if not valid_files:\n",
        "        return\n",
        "    for filename, stock_ticker in valid_files.items():\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(f\"Processing file: {os.path.basename(filename)} for stock {stock_ticker}\")\n",
        "\n",
        "        # Load raw data\n",
        "        df = pd.read_csv(filename)\n",
        "\n",
        "        # Preprocess the data (predict tomorrow's Close)\n",
        "        df = preprocess_data(df)\n",
        "\n",
        "        # Split the DataFrame into train, validation, and test sets\n",
        "        train_df, val_df, test_df = split_data(df)\n",
        "\n",
        "        # Scale train/val/test sets using the shifted target column \"Close_tomorrow\"\n",
        "        (X_train_scaled_df, y_train_scaled,\n",
        "         X_val_scaled_df, y_val_scaled,\n",
        "         X_test_scaled_df, y_test_scaled,\n",
        "         scaler_x, scaler_y) = scale_train_val_test(train_df, val_df, test_df, target_col=\"Close_tomorrow\")\n",
        "\n",
        "        # --- Create time-series sequences from the scaled data ---\n",
        "        window_size = 10  # Adjust window size as needed\n",
        "        X_train_seq, y_train_seq = create_sequences(X_train_scaled_df, y_train_scaled, window_size)\n",
        "        X_val_seq, y_val_seq = create_sequences(X_val_scaled_df, y_val_scaled, window_size)\n",
        "        X_test_seq, y_test_seq = create_sequences(X_test_scaled_df, y_test_scaled, window_size)\n",
        "\n",
        "        # Save results (this will overwrite the old files in the stock folder)\n",
        "        save_pipeline_outputs_sequences(stock_ticker, scaler_x, scaler_y,\n",
        "                                        X_train_seq, X_val_seq, X_test_seq,\n",
        "                                        y_train_seq, y_val_seq, y_test_seq)\n",
        "        print(f\"✅ Preprocessing pipeline complete with time-series sequences for {stock_ticker}\")\n",
        "\n",
        "# Run the pipeline for all stocks\n",
        "process_all_stock_files()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzBMeo7E3mo_",
        "outputId": "99fbef60-e22c-442a-96c5-87b951f7b6a6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "==================================================\n",
            "Processing file: QQQ_2021-01-01_to_2025-02-27_raw.csv for stock QQQ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1f3215620e51>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=['Date'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved scaler_x to /content/drive/MyDrive/stocks/QQQ/scaler_x_stock.pkl\n",
            "✅ Saved scaler_y to /content/drive/MyDrive/stocks/QQQ/scaler_y.pkl\n",
            "✅ Saved X_train.npy (sequences) to /content/drive/MyDrive/stocks/QQQ\n",
            "✅ Saved X_val.npy (sequences) to /content/drive/MyDrive/stocks/QQQ\n",
            "✅ Saved X_test.npy (sequences) to /content/drive/MyDrive/stocks/QQQ\n",
            "✅ Saved y_train.npy (sequences) to /content/drive/MyDrive/stocks/QQQ\n",
            "✅ Saved y_val.npy (sequences) to /content/drive/MyDrive/stocks/QQQ\n",
            "✅ Saved y_test.npy (sequences) to /content/drive/MyDrive/stocks/QQQ\n",
            "✅ All sequence dataset splits saved in folder: /content/drive/MyDrive/stocks/QQQ\n",
            "✅ Preprocessing pipeline complete with time-series sequences for QQQ\n",
            "\n",
            "==================================================\n",
            "Processing file: SPY_2021-01-01_to_2025-03-04_raw.csv for stock SPY\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1f3215620e51>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=['Date'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved scaler_x to /content/drive/MyDrive/stocks/SPY/scaler_x_stock.pkl\n",
            "✅ Saved scaler_y to /content/drive/MyDrive/stocks/SPY/scaler_y.pkl\n",
            "✅ Saved X_train.npy (sequences) to /content/drive/MyDrive/stocks/SPY\n",
            "✅ Saved X_val.npy (sequences) to /content/drive/MyDrive/stocks/SPY\n",
            "✅ Saved X_test.npy (sequences) to /content/drive/MyDrive/stocks/SPY\n",
            "✅ Saved y_train.npy (sequences) to /content/drive/MyDrive/stocks/SPY\n",
            "✅ Saved y_val.npy (sequences) to /content/drive/MyDrive/stocks/SPY\n",
            "✅ Saved y_test.npy (sequences) to /content/drive/MyDrive/stocks/SPY\n",
            "✅ All sequence dataset splits saved in folder: /content/drive/MyDrive/stocks/SPY\n",
            "✅ Preprocessing pipeline complete with time-series sequences for SPY\n",
            "\n",
            "==================================================\n",
            "Processing file: TSLA_2021-01-01_to_2025-03-04_raw.csv for stock TSLA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1f3215620e51>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=['Date'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved scaler_x to /content/drive/MyDrive/stocks/TSLA/scaler_x_stock.pkl\n",
            "✅ Saved scaler_y to /content/drive/MyDrive/stocks/TSLA/scaler_y.pkl\n",
            "✅ Saved X_train.npy (sequences) to /content/drive/MyDrive/stocks/TSLA\n",
            "✅ Saved X_val.npy (sequences) to /content/drive/MyDrive/stocks/TSLA\n",
            "✅ Saved X_test.npy (sequences) to /content/drive/MyDrive/stocks/TSLA\n",
            "✅ Saved y_train.npy (sequences) to /content/drive/MyDrive/stocks/TSLA\n",
            "✅ Saved y_val.npy (sequences) to /content/drive/MyDrive/stocks/TSLA\n",
            "✅ Saved y_test.npy (sequences) to /content/drive/MyDrive/stocks/TSLA\n",
            "✅ All sequence dataset splits saved in folder: /content/drive/MyDrive/stocks/TSLA\n",
            "✅ Preprocessing pipeline complete with time-series sequences for TSLA\n",
            "\n",
            "==================================================\n",
            "Processing file: MSFT_2021-01-01_to_2025-03-04_raw.csv for stock MSFT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1f3215620e51>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=['Date'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved scaler_x to /content/drive/MyDrive/stocks/MSFT/scaler_x_stock.pkl\n",
            "✅ Saved scaler_y to /content/drive/MyDrive/stocks/MSFT/scaler_y.pkl\n",
            "✅ Saved X_train.npy (sequences) to /content/drive/MyDrive/stocks/MSFT\n",
            "✅ Saved X_val.npy (sequences) to /content/drive/MyDrive/stocks/MSFT\n",
            "✅ Saved X_test.npy (sequences) to /content/drive/MyDrive/stocks/MSFT\n",
            "✅ Saved y_train.npy (sequences) to /content/drive/MyDrive/stocks/MSFT\n",
            "✅ Saved y_val.npy (sequences) to /content/drive/MyDrive/stocks/MSFT\n",
            "✅ Saved y_test.npy (sequences) to /content/drive/MyDrive/stocks/MSFT\n",
            "✅ All sequence dataset splits saved in folder: /content/drive/MyDrive/stocks/MSFT\n",
            "✅ Preprocessing pipeline complete with time-series sequences for MSFT\n",
            "\n",
            "==================================================\n",
            "Processing file: AMZN_2021-01-01_to_2025-03-05_raw.csv for stock AMZN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1f3215620e51>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=['Date'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved scaler_x to /content/drive/MyDrive/stocks/AMZN/scaler_x_stock.pkl\n",
            "✅ Saved scaler_y to /content/drive/MyDrive/stocks/AMZN/scaler_y.pkl\n",
            "✅ Saved X_train.npy (sequences) to /content/drive/MyDrive/stocks/AMZN\n",
            "✅ Saved X_val.npy (sequences) to /content/drive/MyDrive/stocks/AMZN\n",
            "✅ Saved X_test.npy (sequences) to /content/drive/MyDrive/stocks/AMZN\n",
            "✅ Saved y_train.npy (sequences) to /content/drive/MyDrive/stocks/AMZN\n",
            "✅ Saved y_val.npy (sequences) to /content/drive/MyDrive/stocks/AMZN\n",
            "✅ Saved y_test.npy (sequences) to /content/drive/MyDrive/stocks/AMZN\n",
            "✅ All sequence dataset splits saved in folder: /content/drive/MyDrive/stocks/AMZN\n",
            "✅ Preprocessing pipeline complete with time-series sequences for AMZN\n",
            "\n",
            "==================================================\n",
            "Processing file: CAT_2021-01-01_to_2025-03-05_raw.csv for stock CAT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1f3215620e51>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=['Date'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved scaler_x to /content/drive/MyDrive/stocks/CAT/scaler_x_stock.pkl\n",
            "✅ Saved scaler_y to /content/drive/MyDrive/stocks/CAT/scaler_y.pkl\n",
            "✅ Saved X_train.npy (sequences) to /content/drive/MyDrive/stocks/CAT\n",
            "✅ Saved X_val.npy (sequences) to /content/drive/MyDrive/stocks/CAT\n",
            "✅ Saved X_test.npy (sequences) to /content/drive/MyDrive/stocks/CAT\n",
            "✅ Saved y_train.npy (sequences) to /content/drive/MyDrive/stocks/CAT\n",
            "✅ Saved y_val.npy (sequences) to /content/drive/MyDrive/stocks/CAT\n",
            "✅ Saved y_test.npy (sequences) to /content/drive/MyDrive/stocks/CAT\n",
            "✅ All sequence dataset splits saved in folder: /content/drive/MyDrive/stocks/CAT\n",
            "✅ Preprocessing pipeline complete with time-series sequences for CAT\n",
            "\n",
            "==================================================\n",
            "Processing file: TQQQ_2021-01-01_to_2025-03-05_raw.csv for stock TQQQ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1f3215620e51>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=['Date'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved scaler_x to /content/drive/MyDrive/stocks/TQQQ/scaler_x_stock.pkl\n",
            "✅ Saved scaler_y to /content/drive/MyDrive/stocks/TQQQ/scaler_y.pkl\n",
            "✅ Saved X_train.npy (sequences) to /content/drive/MyDrive/stocks/TQQQ\n",
            "✅ Saved X_val.npy (sequences) to /content/drive/MyDrive/stocks/TQQQ\n",
            "✅ Saved X_test.npy (sequences) to /content/drive/MyDrive/stocks/TQQQ\n",
            "✅ Saved y_train.npy (sequences) to /content/drive/MyDrive/stocks/TQQQ\n",
            "✅ Saved y_val.npy (sequences) to /content/drive/MyDrive/stocks/TQQQ\n",
            "✅ Saved y_test.npy (sequences) to /content/drive/MyDrive/stocks/TQQQ\n",
            "✅ All sequence dataset splits saved in folder: /content/drive/MyDrive/stocks/TQQQ\n",
            "✅ Preprocessing pipeline complete with time-series sequences for TQQQ\n",
            "\n",
            "==================================================\n",
            "Processing file: AAPL_2021-01-01_to_2025-03-05_raw.csv for stock AAPL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1f3215620e51>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=['Date'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved scaler_x to /content/drive/MyDrive/stocks/AAPL/scaler_x_stock.pkl\n",
            "✅ Saved scaler_y to /content/drive/MyDrive/stocks/AAPL/scaler_y.pkl\n",
            "✅ Saved X_train.npy (sequences) to /content/drive/MyDrive/stocks/AAPL\n",
            "✅ Saved X_val.npy (sequences) to /content/drive/MyDrive/stocks/AAPL\n",
            "✅ Saved X_test.npy (sequences) to /content/drive/MyDrive/stocks/AAPL\n",
            "✅ Saved y_train.npy (sequences) to /content/drive/MyDrive/stocks/AAPL\n",
            "✅ Saved y_val.npy (sequences) to /content/drive/MyDrive/stocks/AAPL\n",
            "✅ Saved y_test.npy (sequences) to /content/drive/MyDrive/stocks/AAPL\n",
            "✅ All sequence dataset splits saved in folder: /content/drive/MyDrive/stocks/AAPL\n",
            "✅ Preprocessing pipeline complete with time-series sequences for AAPL\n",
            "\n",
            "==================================================\n",
            "Processing file: F_2021-01-01_to_2025-03-05_raw.csv for stock F\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1f3215620e51>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=['Date'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved scaler_x to /content/drive/MyDrive/stocks/F/scaler_x_stock.pkl\n",
            "✅ Saved scaler_y to /content/drive/MyDrive/stocks/F/scaler_y.pkl\n",
            "✅ Saved X_train.npy (sequences) to /content/drive/MyDrive/stocks/F\n",
            "✅ Saved X_val.npy (sequences) to /content/drive/MyDrive/stocks/F\n",
            "✅ Saved X_test.npy (sequences) to /content/drive/MyDrive/stocks/F\n",
            "✅ Saved y_train.npy (sequences) to /content/drive/MyDrive/stocks/F\n",
            "✅ Saved y_val.npy (sequences) to /content/drive/MyDrive/stocks/F\n",
            "✅ Saved y_test.npy (sequences) to /content/drive/MyDrive/stocks/F\n",
            "✅ All sequence dataset splits saved in folder: /content/drive/MyDrive/stocks/F\n",
            "✅ Preprocessing pipeline complete with time-series sequences for F\n",
            "\n",
            "==================================================\n",
            "Processing file: BTC_2021-01-01_to_2025-03-06_raw.csv for stock BTC\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1f3215620e51>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=['Date'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved scaler_x to /content/drive/MyDrive/stocks/BTC/scaler_x_stock.pkl\n",
            "✅ Saved scaler_y to /content/drive/MyDrive/stocks/BTC/scaler_y.pkl\n",
            "✅ Saved X_train.npy (sequences) to /content/drive/MyDrive/stocks/BTC\n",
            "✅ Saved X_val.npy (sequences) to /content/drive/MyDrive/stocks/BTC\n",
            "✅ Saved X_test.npy (sequences) to /content/drive/MyDrive/stocks/BTC\n",
            "✅ Saved y_train.npy (sequences) to /content/drive/MyDrive/stocks/BTC\n",
            "✅ Saved y_val.npy (sequences) to /content/drive/MyDrive/stocks/BTC\n",
            "✅ Saved y_test.npy (sequences) to /content/drive/MyDrive/stocks/BTC\n",
            "✅ All sequence dataset splits saved in folder: /content/drive/MyDrive/stocks/BTC\n",
            "✅ Preprocessing pipeline complete with time-series sequences for BTC\n",
            "\n",
            "==================================================\n",
            "Processing file: MAIN_2021-01-01_to_2025-03-06_raw.csv for stock MAIN\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1f3215620e51>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=['Date'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved scaler_x to /content/drive/MyDrive/stocks/MAIN/scaler_x_stock.pkl\n",
            "✅ Saved scaler_y to /content/drive/MyDrive/stocks/MAIN/scaler_y.pkl\n",
            "✅ Saved X_train.npy (sequences) to /content/drive/MyDrive/stocks/MAIN\n",
            "✅ Saved X_val.npy (sequences) to /content/drive/MyDrive/stocks/MAIN\n",
            "✅ Saved X_test.npy (sequences) to /content/drive/MyDrive/stocks/MAIN\n",
            "✅ Saved y_train.npy (sequences) to /content/drive/MyDrive/stocks/MAIN\n",
            "✅ Saved y_val.npy (sequences) to /content/drive/MyDrive/stocks/MAIN\n",
            "✅ Saved y_test.npy (sequences) to /content/drive/MyDrive/stocks/MAIN\n",
            "✅ All sequence dataset splits saved in folder: /content/drive/MyDrive/stocks/MAIN\n",
            "✅ Preprocessing pipeline complete with time-series sequences for MAIN\n",
            "\n",
            "==================================================\n",
            "Processing file: PEP_2021-01-01_to_2025-03-06_raw.csv for stock PEP\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-1f3215620e51>:107: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df.drop(columns=['Date'], inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved scaler_x to /content/drive/MyDrive/stocks/PEP/scaler_x_stock.pkl\n",
            "✅ Saved scaler_y to /content/drive/MyDrive/stocks/PEP/scaler_y.pkl\n",
            "✅ Saved X_train.npy (sequences) to /content/drive/MyDrive/stocks/PEP\n",
            "✅ Saved X_val.npy (sequences) to /content/drive/MyDrive/stocks/PEP\n",
            "✅ Saved X_test.npy (sequences) to /content/drive/MyDrive/stocks/PEP\n",
            "✅ Saved y_train.npy (sequences) to /content/drive/MyDrive/stocks/PEP\n",
            "✅ Saved y_val.npy (sequences) to /content/drive/MyDrive/stocks/PEP\n",
            "✅ Saved y_test.npy (sequences) to /content/drive/MyDrive/stocks/PEP\n",
            "✅ All sequence dataset splits saved in folder: /content/drive/MyDrive/stocks/PEP\n",
            "✅ Preprocessing pipeline complete with time-series sequences for PEP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import json\n",
        "import keras_tuner as kt\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, Flatten,\n",
        "                                     SimpleRNN, LSTM, Dense, Dropout,\n",
        "                                     Concatenate, Multiply, Attention, Lambda)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === Get List of Stock Folders ===\n",
        "base_stocks_folder = '/content/drive/MyDrive/stocks'\n",
        "stock_folders = [f for f in os.listdir(base_stocks_folder) if os.path.isdir(os.path.join(base_stocks_folder, f))]\n",
        "if not stock_folders:\n",
        "    print(\"No stock folders found in\", base_stocks_folder)\n",
        "    raise SystemExit\n",
        "\n",
        "print(\"Processing the following stock tickers:\")\n",
        "for stock in stock_folders:\n",
        "    print(f\" - {stock}\")\n",
        "\n",
        "# === Checkpoint Setup ===\n",
        "checkpoint_path = os.path.join(base_stocks_folder, \"processed_stocks_rolling.txt\")\n",
        "if os.path.exists(checkpoint_path):\n",
        "    with open(checkpoint_path, \"r\") as f:\n",
        "        processed_stocks = f.read().splitlines()\n",
        "else:\n",
        "    processed_stocks = []\n",
        "\n",
        "# Loop through each stock folder (process ALL stocks)\n",
        "for selected_ticker in stock_folders:\n",
        "    if selected_ticker in processed_stocks:\n",
        "        print(f\"Skipping {selected_ticker} (already processed).\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Processing stock: {selected_ticker}\")\n",
        "\n",
        "    stock_path = os.path.join(base_stocks_folder, selected_ticker)\n",
        "\n",
        "    # === Define Ensemble Save Folder (inside stock folder) as EnsembleRolling ===\n",
        "    ensemble_folder = os.path.join(stock_path, \"EnsembleRolling\")\n",
        "    os.makedirs(ensemble_folder, exist_ok=True)\n",
        "\n",
        "    # === Load Preprocessed Data from Stock Folder ===\n",
        "    X_train = np.load(os.path.join(stock_path, \"X_train.npy\"))\n",
        "    y_train = np.load(os.path.join(stock_path, \"y_train.npy\"))\n",
        "    X_val   = np.load(os.path.join(stock_path, \"X_val.npy\"))\n",
        "    y_val   = np.load(os.path.join(stock_path, \"y_val.npy\"))\n",
        "    X_test  = np.load(os.path.join(stock_path, \"X_test.npy\"))\n",
        "    y_test  = np.load(os.path.join(stock_path, \"y_test.npy\"))\n",
        "\n",
        "    # Load scaler for target variable (for inverse scaling later)\n",
        "    scaler_y = joblib.load(os.path.join(stock_path, \"scaler_y.pkl\"))\n",
        "\n",
        "    print(f\"✅ Data Loaded for {selected_ticker}:\")\n",
        "    print(f\"   X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "    print(f\"   X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "    print(f\"   X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "    # === Reshape Data if Needed ===\n",
        "    # If data is 2D, add a time-axis (samples, 1, features); if already time-series, do nothing.\n",
        "    if X_train.ndim == 2:\n",
        "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "        X_val   = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
        "        X_test  = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    print(f\"✅ Input shape for model: {input_shape}\")\n",
        "\n",
        "    # === Hyperparameter Tuning / Load Best Hyperparameters ===\n",
        "    best_hps_path = os.path.join(ensemble_folder, \"best_hyperparameters.json\")\n",
        "    tuning_flag_path = os.path.join(ensemble_folder, \"hp_tuning_complete.flag\")\n",
        "\n",
        "    # Define the model building function for the sigmoid ensemble\n",
        "    def build_ensemble_model(hp):\n",
        "        inputs = Input(shape=input_shape)\n",
        "        # Determine kernel size options based on timesteps\n",
        "        if input_shape[0] == 1:\n",
        "            kernel_size_options = [1]\n",
        "            apply_pooling = False\n",
        "        else:\n",
        "            kernel_size_options = [3, 5, 7]\n",
        "            apply_pooling = True\n",
        "\n",
        "        # === CNN Branch ===\n",
        "        cnn = Conv1D(filters=hp.Choice('cnn_filters', [64, 128, 256]),\n",
        "                     kernel_size=hp.Choice('cnn_kernel_size', kernel_size_options),\n",
        "                     activation='relu')(inputs)\n",
        "        if apply_pooling:\n",
        "            cnn = MaxPooling1D(pool_size=2)(cnn)\n",
        "        cnn = Flatten()(cnn)\n",
        "        # Sigmoid weight for CNN branch\n",
        "        cnn_weight = Dense(1, activation='sigmoid')(cnn)\n",
        "\n",
        "        # === RNN Branch ===\n",
        "        rnn = SimpleRNN(units=hp.Choice('rnn_units', [75, 100, 125]), return_sequences=True)(inputs)\n",
        "        rnn = SimpleRNN(units=hp.Choice('rnn_units_2', [75, 100, 125]), return_sequences=True)(rnn)\n",
        "        rnn = Attention()([rnn, rnn])\n",
        "        rnn = Flatten()(rnn)\n",
        "        # Sigmoid weight for RNN branch\n",
        "        rnn_weight = Dense(1, activation='sigmoid')(rnn)\n",
        "\n",
        "        # === LSTM Branch ===\n",
        "        lstm = LSTM(units=hp.Choice('lstm_units', [50, 75, 100]), return_sequences=True)(inputs)\n",
        "        lstm = LSTM(units=hp.Choice('lstm_units_2', [50, 75, 100]), return_sequences=True)(lstm)\n",
        "        lstm = Attention()([lstm, lstm])\n",
        "        lstm = Flatten()(lstm)\n",
        "        # Sigmoid weight for LSTM branch\n",
        "        lstm_weight = Dense(1, activation='sigmoid')(lstm)\n",
        "\n",
        "        # === Adaptive Weighted Fusion ===\n",
        "        cnn_scaled = Multiply()([cnn, cnn_weight])\n",
        "        rnn_scaled = Multiply()([rnn, rnn_weight])\n",
        "        lstm_scaled = Multiply()([lstm, lstm_weight])\n",
        "        merged = Concatenate()([cnn_scaled, rnn_scaled, lstm_scaled])\n",
        "        merged = Dense(units=hp.Choice('dense_units', [50, 100, 150]),\n",
        "                       activation=\"relu\")(merged)\n",
        "        merged = Dropout(hp.Choice('dropout_rate', [0.1, 0.2, 0.3]))(merged)\n",
        "        output = Dense(1)(merged)\n",
        "\n",
        "        model = Model(inputs, output)\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=hp.Choice('learning_rate', [0.001, 0.0005, 0.0001])),\n",
        "            loss=\"mse\",\n",
        "            metrics=[\"mae\"]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    if not os.path.exists(tuning_flag_path):\n",
        "        # Force re-tuning the first time: remove existing hyperparameters if present\n",
        "        if os.path.exists(best_hps_path):\n",
        "            os.remove(best_hps_path)\n",
        "        print(f\"🔍 No tuning flag found for {selected_ticker}. Running hyperparameter tuning for Sigmoid Ensemble...\")\n",
        "        tuner = kt.RandomSearch(\n",
        "            build_ensemble_model,\n",
        "            objective=\"val_loss\",\n",
        "            max_trials=15,\n",
        "            executions_per_trial=3,\n",
        "            directory=os.path.join(ensemble_folder, \"tuning\"),\n",
        "            project_name=\"stock_prediction_ensemble\"\n",
        "        )\n",
        "        tuner.search(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=1)\n",
        "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "        best_hps_dict = {param: best_hps.get(param) for param in best_hps.values.keys()}\n",
        "        with open(best_hps_path, \"w\") as f:\n",
        "            json.dump(best_hps_dict, f)\n",
        "        # Create a flag file to indicate that tuning has been completed\n",
        "        with open(tuning_flag_path, \"w\") as f:\n",
        "            f.write(\"tuning complete\")\n",
        "        best_model = tuner.hypermodel.build(best_hps)\n",
        "    else:\n",
        "        print(f\"✅ Loading best hyperparameters from file for {selected_ticker}\")\n",
        "        with open(best_hps_path, \"r\") as f:\n",
        "            best_hps_dict = json.load(f)\n",
        "        best_hps = kt.HyperParameters()\n",
        "        for key, value in best_hps_dict.items():\n",
        "            best_hps.Fixed(key, value)\n",
        "        best_model = build_ensemble_model(best_hps)\n",
        "\n",
        "    print(f\"✅ Best hyperparameters for {selected_ticker}:\")\n",
        "    print(best_hps_dict)\n",
        "\n",
        "    # === Build and Train the Best Model ===\n",
        "    BATCH_SIZE = 32\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1)\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "    history = best_model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=500,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[reduce_lr, early_stop],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # === Save the Best Model and Training History ===\n",
        "    best_model_path = os.path.join(ensemble_folder, \"best_ensemble_model.keras\")\n",
        "    best_model.save(best_model_path)\n",
        "    print(f\"✅ Best Ensemble Model for {selected_ticker} saved to {best_model_path}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training & Validation Loss\")\n",
        "    plt.legend()\n",
        "    history_plot_path = os.path.join(ensemble_folder, \"training_history.png\")\n",
        "    plt.savefig(history_plot_path)\n",
        "    plt.show()\n",
        "    print(f\"✅ Training history graph for {selected_ticker} saved to {history_plot_path}\")\n",
        "\n",
        "    # === Evaluate the Model ===\n",
        "    loss, mae = best_model.evaluate(X_test, y_test)\n",
        "    print(f\"✅ Best Model Test Loss for {selected_ticker}: {loss:.4f}\")\n",
        "    print(f\"✅ Best Model Test MAE for {selected_ticker}: {mae:.4f}\")\n",
        "\n",
        "    # === Make Predictions and Inverse Scale ===\n",
        "    predictions = best_model.predict(X_test)\n",
        "    def inverse_transform_single_feature(scaler, data):\n",
        "        data = np.array(data).reshape(-1, 1)\n",
        "        return scaler.inverse_transform(data)\n",
        "    predictions_rescaled = inverse_transform_single_feature(scaler_y, predictions)\n",
        "    y_test_rescaled = inverse_transform_single_feature(scaler_y, y_test)\n",
        "\n",
        "    # === Plot Predicted vs Actual Prices ===\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(y_test_rescaled, label=\"Actual Price\", color=\"blue\")\n",
        "    plt.plot(predictions_rescaled, label=\"Predicted Price\", color=\"red\", linestyle=\"dashed\")\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Stock Price\")\n",
        "    plt.title(f\"{selected_ticker} - Predicted vs. Actual Stock Price\")\n",
        "    plt.legend()\n",
        "    pred_vs_actual_path = os.path.join(ensemble_folder, \"pred_vs_actual.png\")\n",
        "    plt.savefig(pred_vs_actual_path)\n",
        "    plt.show()\n",
        "    print(f\"✅ Prediction vs. Actual plot for {selected_ticker} saved to {pred_vs_actual_path}\")\n",
        "\n",
        "    print(f\"\\n🎯 Sigmoid Ensemble Model Training & Prediction Complete for {selected_ticker}! 🚀\")\n",
        "\n",
        "    # Update checkpoint file\n",
        "    processed_stocks.append(selected_ticker)\n",
        "    with open(os.path.join(base_stocks_folder, \"processed_stocks_rolling.txt\"), \"w\") as f:\n",
        "        for s in processed_stocks:\n",
        "            f.write(s + \"\\n\")\n",
        "    print(f\"✅ Checkpoint updated. Processed stocks: {processed_stocks}\")\n"
      ],
      "metadata": {
        "id": "hLDSGFyBDGWb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}