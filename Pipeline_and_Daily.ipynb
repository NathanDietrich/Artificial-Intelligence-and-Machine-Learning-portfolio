{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMj1CVzQ+SRH/Geqf4/C8mk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NathanDietrich/Artificial-Intelligence-and-Machine-Learning-portfolio/blob/main/Pipeline_and_Daily.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjtM3wYeuP2B",
        "outputId": "0a503d90-3734-417e-c238-aa1892b2a958"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.54)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3pUTpBPuX94",
        "outputId": "fc8691ca-e542-4391-8294-a50ec14a9609"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# âœ… Enable GPU & Force TensorFlow to Use It\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "        print(f\"âœ… GPU detected: {gpu_devices[0].name} (Memory Growth Enabled)\")\n",
        "    except:\n",
        "        print(\"âš ï¸ GPU found, but could not enable memory growth.\")\n",
        "else:\n",
        "    print(\"âŒ No GPU detected. Running on CPU.\")\n",
        "\n",
        "# âœ… Enable Mixed Precision for Faster Training (Uses float16 on GPU)\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"âœ… Mixed Precision Enabled (float16) for Faster GPU Training\")\n",
        "\n",
        "# âœ… Check GPU Usage Before Training\n",
        "!nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n",
        "\n",
        "# âœ… Function to Monitor GPU Usage Live\n",
        "def monitor_gpu():\n",
        "    print(\"\\nğŸ” Checking GPU Usage...\")\n",
        "    os.system(\"nvidia-smi --query-gpu=memory.used,memory.total --format=csv\")\n",
        "\n",
        "monitor_gpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snHPji8yueOp",
        "outputId": "cffa4767-3d66-457b-a251-9e11e2ee9f8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… GPU detected: /physical_device:GPU:0 (Memory Growth Enabled)\n",
            "âœ… Mixed Precision Enabled (float16) for Faster GPU Training\n",
            "memory.used [MiB], memory.total [MiB]\n",
            "2 MiB, 15360 MiB\n",
            "\n",
            "ğŸ” Checking GPU Usage...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94JfqCFRuKKU",
        "outputId": "9f4c30cb-ec5e-4308-9384-ff3b154f3967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "================== Processing AAPL ==================\n",
            "ğŸ“Š Fetching stock data for AAPL from 2021-01-01 to 2025-03-16...\n",
            "ğŸ“° Fetching sentiment data for AAPL from 2021-01-01 to 2025-03-16...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-01-01 to 2021-01-31...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-01-31 to 2021-03-02...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-03-02 to 2021-04-01...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-04-01 to 2021-05-01...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-05-01 to 2021-05-31...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-05-31 to 2021-06-30...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-06-30 to 2021-07-30...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-07-30 to 2021-08-29...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-08-29 to 2021-09-28...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-09-28 to 2021-10-28...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-10-28 to 2021-11-27...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-11-27 to 2021-12-27...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2021-12-27 to 2022-01-26...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2022-01-26 to 2022-02-25...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2022-02-25 to 2022-03-27...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2022-03-27 to 2022-04-26...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2022-04-26 to 2022-05-26...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2022-05-26 to 2022-06-25...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2022-06-25 to 2022-07-25...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2022-07-25 to 2022-08-24...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2022-08-24 to 2022-09-23...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2022-09-23 to 2022-10-23...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2022-10-23 to 2022-11-22...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2022-11-22 to 2022-12-22...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2022-12-22 to 2023-01-21...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2023-01-21 to 2023-02-20...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2023-02-20 to 2023-03-22...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2023-03-22 to 2023-04-21...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2023-04-21 to 2023-05-21...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2023-05-21 to 2023-06-20...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2023-06-20 to 2023-07-20...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2023-07-20 to 2023-08-19...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2023-08-19 to 2023-09-18...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2023-09-18 to 2023-10-18...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2023-10-18 to 2023-11-17...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2023-11-17 to 2023-12-17...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2023-12-17 to 2024-01-16...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2024-01-16 to 2024-02-15...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2024-02-15 to 2024-03-16...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2024-03-16 to 2024-04-15...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2024-04-15 to 2024-05-15...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2024-05-15 to 2024-06-14...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2024-06-14 to 2024-07-14...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2024-07-14 to 2024-08-13...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2024-08-13 to 2024-09-12...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2024-09-12 to 2024-10-12...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2024-10-12 to 2024-11-11...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2024-11-11 to 2024-12-11...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2024-12-11 to 2025-01-10...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2025-01-10 to 2025-02-09...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2025-02-09 to 2025-03-11...\n",
            "ğŸ“¡ Fetching sentiment data for AAPL from 2025-03-11 to 2025-03-16...\n",
            "ğŸ’¡ Performing sentiment analysis...\n",
            "ğŸ”— Merging stock and sentiment data...\n",
            "âœ… Raw data for AAPL saved to: /content/drive/MyDrive/StockData/AAPL_2021-01-01_to_2025-03-16_raw.csv\n",
            "\n",
            "================== Processing AMZN ==================\n",
            "ğŸ“Š Fetching stock data for AMZN from 2021-01-01 to 2025-03-16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-434bf50a68f7>:112: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  .ffill()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“° Fetching sentiment data for AMZN from 2021-01-01 to 2025-03-16...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-01-01 to 2021-01-31...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-01-31 to 2021-03-02...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-03-02 to 2021-04-01...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-04-01 to 2021-05-01...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-05-01 to 2021-05-31...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-05-31 to 2021-06-30...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-06-30 to 2021-07-30...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-07-30 to 2021-08-29...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-08-29 to 2021-09-28...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-09-28 to 2021-10-28...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-10-28 to 2021-11-27...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-11-27 to 2021-12-27...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2021-12-27 to 2022-01-26...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2022-01-26 to 2022-02-25...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2022-02-25 to 2022-03-27...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2022-03-27 to 2022-04-26...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2022-04-26 to 2022-05-26...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2022-05-26 to 2022-06-25...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2022-06-25 to 2022-07-25...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2022-07-25 to 2022-08-24...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2022-08-24 to 2022-09-23...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2022-09-23 to 2022-10-23...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2022-10-23 to 2022-11-22...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2022-11-22 to 2022-12-22...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2022-12-22 to 2023-01-21...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2023-01-21 to 2023-02-20...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2023-02-20 to 2023-03-22...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2023-03-22 to 2023-04-21...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2023-04-21 to 2023-05-21...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2023-05-21 to 2023-06-20...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2023-06-20 to 2023-07-20...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2023-07-20 to 2023-08-19...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2023-08-19 to 2023-09-18...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2023-09-18 to 2023-10-18...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2023-10-18 to 2023-11-17...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2023-11-17 to 2023-12-17...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2023-12-17 to 2024-01-16...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2024-01-16 to 2024-02-15...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2024-02-15 to 2024-03-16...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2024-03-16 to 2024-04-15...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2024-04-15 to 2024-05-15...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2024-05-15 to 2024-06-14...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2024-06-14 to 2024-07-14...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2024-07-14 to 2024-08-13...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2024-08-13 to 2024-09-12...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2024-09-12 to 2024-10-12...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2024-10-12 to 2024-11-11...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2024-11-11 to 2024-12-11...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2024-12-11 to 2025-01-10...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2025-01-10 to 2025-02-09...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2025-02-09 to 2025-03-11...\n",
            "ğŸ“¡ Fetching sentiment data for AMZN from 2025-03-11 to 2025-03-16...\n",
            "ğŸ’¡ Performing sentiment analysis...\n",
            "ğŸ”— Merging stock and sentiment data...\n",
            "âœ… Raw data for AMZN saved to: /content/drive/MyDrive/StockData/AMZN_2021-01-01_to_2025-03-16_raw.csv\n",
            "\n",
            "================== Processing MSFT ==================\n",
            "ğŸ“Š Fetching stock data for MSFT from 2021-01-01 to 2025-03-16...\n",
            "ğŸ“° Fetching sentiment data for MSFT from 2021-01-01 to 2025-03-16...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-01-01 to 2021-01-31...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-01-31 to 2021-03-02...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-03-02 to 2021-04-01...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-04-01 to 2021-05-01...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-05-01 to 2021-05-31...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-05-31 to 2021-06-30...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-06-30 to 2021-07-30...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-07-30 to 2021-08-29...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-08-29 to 2021-09-28...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-09-28 to 2021-10-28...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-10-28 to 2021-11-27...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-11-27 to 2021-12-27...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2021-12-27 to 2022-01-26...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2022-01-26 to 2022-02-25...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2022-02-25 to 2022-03-27...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2022-03-27 to 2022-04-26...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2022-04-26 to 2022-05-26...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2022-05-26 to 2022-06-25...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2022-06-25 to 2022-07-25...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2022-07-25 to 2022-08-24...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2022-08-24 to 2022-09-23...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2022-09-23 to 2022-10-23...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2022-10-23 to 2022-11-22...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2022-11-22 to 2022-12-22...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2022-12-22 to 2023-01-21...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2023-01-21 to 2023-02-20...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2023-02-20 to 2023-03-22...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2023-03-22 to 2023-04-21...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2023-04-21 to 2023-05-21...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2023-05-21 to 2023-06-20...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2023-06-20 to 2023-07-20...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2023-07-20 to 2023-08-19...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2023-08-19 to 2023-09-18...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2023-09-18 to 2023-10-18...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2023-10-18 to 2023-11-17...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2023-11-17 to 2023-12-17...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2023-12-17 to 2024-01-16...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2024-01-16 to 2024-02-15...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2024-02-15 to 2024-03-16...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2024-03-16 to 2024-04-15...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2024-04-15 to 2024-05-15...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2024-05-15 to 2024-06-14...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2024-06-14 to 2024-07-14...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2024-07-14 to 2024-08-13...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2024-08-13 to 2024-09-12...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2024-09-12 to 2024-10-12...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2024-10-12 to 2024-11-11...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2024-11-11 to 2024-12-11...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2024-12-11 to 2025-01-10...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2025-01-10 to 2025-02-09...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2025-02-09 to 2025-03-11...\n",
            "ğŸ“¡ Fetching sentiment data for MSFT from 2025-03-11 to 2025-03-16...\n",
            "ğŸ’¡ Performing sentiment analysis...\n",
            "ğŸ”— Merging stock and sentiment data...\n",
            "âœ… Raw data for MSFT saved to: /content/drive/MyDrive/StockData/MSFT_2021-01-01_to_2025-03-16_raw.csv\n",
            "\n",
            "================== Processing SPY ==================\n",
            "ğŸ“Š Fetching stock data for SPY from 2021-01-01 to 2025-03-16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-434bf50a68f7>:112: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  .ffill()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“° Fetching sentiment data for SPY from 2021-01-01 to 2025-03-16...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-01-01 to 2021-01-31...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-01-31 to 2021-03-02...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-03-02 to 2021-04-01...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-04-01 to 2021-05-01...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-05-01 to 2021-05-31...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-05-31 to 2021-06-30...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-06-30 to 2021-07-30...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-07-30 to 2021-08-29...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-08-29 to 2021-09-28...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-09-28 to 2021-10-28...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-10-28 to 2021-11-27...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-11-27 to 2021-12-27...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2021-12-27 to 2022-01-26...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2022-01-26 to 2022-02-25...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2022-02-25 to 2022-03-27...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2022-03-27 to 2022-04-26...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2022-04-26 to 2022-05-26...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2022-05-26 to 2022-06-25...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2022-06-25 to 2022-07-25...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2022-07-25 to 2022-08-24...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2022-08-24 to 2022-09-23...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2022-09-23 to 2022-10-23...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2022-10-23 to 2022-11-22...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2022-11-22 to 2022-12-22...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2022-12-22 to 2023-01-21...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2023-01-21 to 2023-02-20...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2023-02-20 to 2023-03-22...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2023-03-22 to 2023-04-21...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2023-04-21 to 2023-05-21...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2023-05-21 to 2023-06-20...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2023-06-20 to 2023-07-20...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2023-07-20 to 2023-08-19...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2023-08-19 to 2023-09-18...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2023-09-18 to 2023-10-18...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2023-10-18 to 2023-11-17...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2023-11-17 to 2023-12-17...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2023-12-17 to 2024-01-16...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2024-01-16 to 2024-02-15...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2024-02-15 to 2024-03-16...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2024-03-16 to 2024-04-15...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2024-04-15 to 2024-05-15...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2024-05-15 to 2024-06-14...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2024-06-14 to 2024-07-14...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2024-07-14 to 2024-08-13...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2024-08-13 to 2024-09-12...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2024-09-12 to 2024-10-12...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2024-10-12 to 2024-11-11...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2024-11-11 to 2024-12-11...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2024-12-11 to 2025-01-10...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2025-01-10 to 2025-02-09...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2025-02-09 to 2025-03-11...\n",
            "ğŸ“¡ Fetching sentiment data for SPY from 2025-03-11 to 2025-03-16...\n",
            "ğŸ’¡ Performing sentiment analysis...\n",
            "ğŸ”— Merging stock and sentiment data...\n",
            "âœ… Raw data for SPY saved to: /content/drive/MyDrive/StockData/SPY_2021-01-01_to_2025-03-16_raw.csv\n",
            "\n",
            "================== Processing QQQ ==================\n",
            "ğŸ“Š Fetching stock data for QQQ from 2021-01-01 to 2025-03-16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-434bf50a68f7>:112: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  .ffill()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“° Fetching sentiment data for QQQ from 2021-01-01 to 2025-03-16...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-01-01 to 2021-01-31...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-01-31 to 2021-03-02...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-03-02 to 2021-04-01...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-04-01 to 2021-05-01...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-05-01 to 2021-05-31...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-05-31 to 2021-06-30...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-06-30 to 2021-07-30...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-07-30 to 2021-08-29...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-08-29 to 2021-09-28...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-09-28 to 2021-10-28...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-10-28 to 2021-11-27...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-11-27 to 2021-12-27...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2021-12-27 to 2022-01-26...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2022-01-26 to 2022-02-25...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2022-02-25 to 2022-03-27...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2022-03-27 to 2022-04-26...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2022-04-26 to 2022-05-26...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2022-05-26 to 2022-06-25...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2022-06-25 to 2022-07-25...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2022-07-25 to 2022-08-24...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2022-08-24 to 2022-09-23...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2022-09-23 to 2022-10-23...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2022-10-23 to 2022-11-22...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2022-11-22 to 2022-12-22...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2022-12-22 to 2023-01-21...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2023-01-21 to 2023-02-20...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2023-02-20 to 2023-03-22...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2023-03-22 to 2023-04-21...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2023-04-21 to 2023-05-21...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2023-05-21 to 2023-06-20...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2023-06-20 to 2023-07-20...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2023-07-20 to 2023-08-19...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2023-08-19 to 2023-09-18...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2023-09-18 to 2023-10-18...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2023-10-18 to 2023-11-17...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2023-11-17 to 2023-12-17...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2023-12-17 to 2024-01-16...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2024-01-16 to 2024-02-15...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2024-02-15 to 2024-03-16...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2024-03-16 to 2024-04-15...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2024-04-15 to 2024-05-15...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2024-05-15 to 2024-06-14...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2024-06-14 to 2024-07-14...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2024-07-14 to 2024-08-13...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2024-08-13 to 2024-09-12...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2024-09-12 to 2024-10-12...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2024-10-12 to 2024-11-11...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2024-11-11 to 2024-12-11...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2024-12-11 to 2025-01-10...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2025-01-10 to 2025-02-09...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2025-02-09 to 2025-03-11...\n",
            "ğŸ“¡ Fetching sentiment data for QQQ from 2025-03-11 to 2025-03-16...\n",
            "ğŸ’¡ Performing sentiment analysis...\n",
            "ğŸ”— Merging stock and sentiment data...\n",
            "âœ… Raw data for QQQ saved to: /content/drive/MyDrive/StockData/QQQ_2021-01-01_to_2025-03-16_raw.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-434bf50a68f7>:112: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  .ffill()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "userdata.get('Polygon_Key')\n",
        "\n",
        "# Mount Google Drive for saving raw data\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Provided functions for stock & sentiment data collection ---\n",
        "def fetch_stock_data_polygon(ticker, start_date, end_date, api_key):\n",
        "    \"\"\"\n",
        "    Fetches historical stock data from Polygon.io.\n",
        "    \"\"\"\n",
        "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date}/{end_date}?apiKey={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error fetching stock data for {ticker}: {response.text}\")\n",
        "        return None\n",
        "    data = response.json()\n",
        "    if \"results\" not in data:\n",
        "        print(f\"No results found for {ticker}.\")\n",
        "        return None\n",
        "    df = pd.DataFrame(data[\"results\"])\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"t\"], unit=\"ms\").dt.date\n",
        "    df.rename(columns={\"o\": \"Open\", \"h\": \"High\", \"l\": \"Low\", \"c\": \"Close\", \"v\": \"Volume\"}, inplace=True)\n",
        "    df = df[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "    return df\n",
        "\n",
        "def fetch_sentiment_data_polygon(ticker, start_date, end_date, api_key, limit=1000):\n",
        "    \"\"\"\n",
        "    Fetches sentiment data from Polygon.io in chunks and computes daily sentiment scores.\n",
        "    \"\"\"\n",
        "    url = \"https://api.polygon.io/v2/reference/news\"\n",
        "    all_results = []\n",
        "    current_start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    final_end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "    while current_start_date < final_end_date:\n",
        "        chunk_end_date = current_start_date + datetime.timedelta(days=30)\n",
        "        if chunk_end_date > final_end_date:\n",
        "            chunk_end_date = final_end_date\n",
        "        chunk_start_str = current_start_date.strftime(\"%Y-%m-%d\")\n",
        "        chunk_end_str = chunk_end_date.strftime(\"%Y-%m-%d\")\n",
        "        print(f\"ğŸ“¡ Fetching sentiment data for {ticker} from {chunk_start_str} to {chunk_end_str}...\")\n",
        "        params = {\n",
        "            \"ticker\": ticker,\n",
        "            \"published_utc.gte\": chunk_start_str,\n",
        "            \"published_utc.lte\": chunk_end_str,\n",
        "            \"apiKey\": api_key,\n",
        "            \"limit\": limit\n",
        "        }\n",
        "        while True:\n",
        "            response = requests.get(url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                results = data.get(\"results\", [])\n",
        "                all_results.extend(results)\n",
        "                next_cursor = data.get(\"next_cursor\")\n",
        "                if not next_cursor:\n",
        "                    break\n",
        "                params[\"cursor\"] = next_cursor\n",
        "            else:\n",
        "                print(f\"âš ï¸ Error fetching sentiment data for {ticker}: {response.status_code}, {response.text}\")\n",
        "                break\n",
        "        current_start_date = chunk_end_date\n",
        "        time.sleep(14)  # Avoid hitting API rate limits\n",
        "    return all_results\n",
        "\n",
        "def analyze_sentiment(news_data):\n",
        "    \"\"\"\n",
        "    Uses TextBlob to compute sentiment polarity and subjectivity for each news article.\n",
        "    \"\"\"\n",
        "    analyzed_data = []\n",
        "    for article in news_data:\n",
        "        title = article.get(\"title\", \"\")\n",
        "        description = article.get(\"description\", \"\")\n",
        "        full_text = f\"{title} {description}\"\n",
        "        sentiment = TextBlob(full_text).sentiment\n",
        "        analyzed_data.append({\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "            \"published_date\": article.get(\"published_utc\", \"\"),\n",
        "            \"sentiment_polarity\": sentiment.polarity,\n",
        "            \"sentiment_subjectivity\": sentiment.subjectivity\n",
        "        })\n",
        "    return analyzed_data\n",
        "\n",
        "def merge_stock_and_sentiment(stock_df, sentiment_data):\n",
        "    \"\"\"\n",
        "    Merges stock data with sentiment data by date.\n",
        "    \"\"\"\n",
        "    sentiment_df = pd.DataFrame(sentiment_data)\n",
        "    if sentiment_df.empty:\n",
        "        print(\"âš ï¸ No sentiment data found, proceeding without sentiment.\")\n",
        "        stock_df[\"sentiment_polarity\"] = 0  # Default neutral\n",
        "        stock_df[\"sentiment_subjectivity\"] = 0\n",
        "        return stock_df\n",
        "    sentiment_df['published_date'] = pd.to_datetime(sentiment_df['published_date'], errors='coerce')\n",
        "    sentiment_df['Date'] = sentiment_df['published_date'].dt.date\n",
        "    daily_sentiment = sentiment_df.groupby('Date').agg({\n",
        "        'sentiment_polarity': 'mean',\n",
        "        'sentiment_subjectivity': 'mean'\n",
        "    }).reset_index()\n",
        "    merged_df = pd.merge(stock_df, daily_sentiment, on=\"Date\", how=\"left\")\n",
        "    merged_df[['sentiment_polarity', 'sentiment_subjectivity']] = (\n",
        "        merged_df[['sentiment_polarity', 'sentiment_subjectivity']]\n",
        "        .replace(0, pd.NA)\n",
        "        .ffill()\n",
        "    )\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "    return merged_df\n",
        "\n",
        "# --- Main raw data collection for selected tickers ---\n",
        "def collect_raw_data():\n",
        "    # Set fixed start/end dates (adjust as needed)\n",
        "    start_date = \"2021-01-01\"\n",
        "    end_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    # List of tickers for stock + sentiment collection\n",
        "    tickers = [\"AAPL\", \"AMZN\", \"MSFT\", \"SPY\", \"QQQ\"]\n",
        "\n",
        "    # Load your Polygon API key (assume stored in Google Colab user data or environment)\n",
        "    # For example, you can store it in a file or use environment variables.\n",
        "    # Here, we simulate fetching it:\n",
        "    api_key = userdata.get(\"Polygon_Key\")\n",
        "    if api_key == \"YOUR_POLYGON_API_KEY\":\n",
        "        print(\"Please set your Polygon API key in the environment variable POLYGON_API_KEY\")\n",
        "        return\n",
        "\n",
        "    for ticker in tickers:\n",
        "        print(f\"\\n================== Processing {ticker} ==================\")\n",
        "        print(f\"ğŸ“Š Fetching stock data for {ticker} from {start_date} to {end_date}...\")\n",
        "        stock_df = fetch_stock_data_polygon(ticker, start_date, end_date, api_key)\n",
        "        if stock_df is None:\n",
        "            print(f\"âŒ No stock data found for {ticker}. Skipping.\")\n",
        "            continue\n",
        "        print(f\"ğŸ“° Fetching sentiment data for {ticker} from {start_date} to {end_date}...\")\n",
        "        news_data = fetch_sentiment_data_polygon(ticker, start_date, end_date, api_key, limit=1000)\n",
        "        if not news_data:\n",
        "            print(f\"âš ï¸ No news data found for {ticker}. Proceeding without sentiment data.\")\n",
        "        print(\"ğŸ’¡ Performing sentiment analysis...\")\n",
        "        sentiment_data = analyze_sentiment(news_data)\n",
        "        print(\"ğŸ”— Merging stock and sentiment data...\")\n",
        "        merged_df = merge_stock_and_sentiment(stock_df, sentiment_data)\n",
        "        # Save raw merged data to Google Drive\n",
        "        save_dir = \"/content/drive/MyDrive/StockData\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        filename = os.path.join(save_dir, f\"{ticker}_{start_date}_to_{end_date}_raw.csv\")\n",
        "        merged_df.to_csv(filename, index=False)\n",
        "        print(f\"âœ… Raw data for {ticker} saved to: {filename}\")\n",
        "\n",
        "# Run raw data collection\n",
        "collect_raw_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calculate_technical_indicators(df):\n",
        "    \"\"\"\n",
        "    Adds technical indicators: SMA, EMA, RSI, MACD, and MACD Signal.\n",
        "    Drops edge cases (rows with NaN) after calculation.\n",
        "    Version: 2025-03-15\n",
        "    \"\"\"\n",
        "    df['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
        "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
        "    df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "\n",
        "    delta = df['Close'].diff()\n",
        "    gain = delta.clip(lower=0)\n",
        "    loss = -delta.clip(upper=0)\n",
        "    avg_gain = gain.rolling(window=14, min_periods=14).mean()\n",
        "    avg_loss = loss.rolling(window=14, min_periods=14).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    ema12 = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "    df['MACD'] = ema12 - ema26\n",
        "    df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    # Drop edge cases that have NaN values (e.g., initial rows)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "def preprocess_and_save(ticker, raw_filepath):\n",
        "    \"\"\"\n",
        "    Loads raw CSV, calculates technical indicators, drops edge cases and the Date column, then saves the processed file.\n",
        "    Version: 2025-03-15\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(raw_filepath, parse_dates=[\"Date\"])\n",
        "    df = calculate_technical_indicators(df)\n",
        "    # Drop the Date column as it's not used for training\n",
        "    df.drop(columns=[\"Date\"], inplace=True, errors=\"ignore\")\n",
        "\n",
        "    processed_filepath = raw_filepath.replace(\"_raw.csv\", \"_processed.csv\")\n",
        "    df.to_csv(processed_filepath, index=False)\n",
        "    print(f\"âœ… Processed data for {ticker} saved to: {processed_filepath}\")\n",
        "    return processed_filepath\n",
        "\n",
        "import glob, os\n",
        "raw_files = glob.glob(\"/content/drive/MyDrive/StockData/*_raw.csv\")\n",
        "for file in raw_files:\n",
        "    ticker = os.path.basename(file).split(\"_\")[0]\n",
        "    preprocess_and_save(ticker, file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4lgzdgaujo_",
        "outputId": "5c8805fd-0a80-4cfc-d409-54779ee3edef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Processed data for AAPL saved to: /content/drive/MyDrive/StockData/AAPL_2021-01-01_to_2025-03-16_processed.csv\n",
            "âœ… Processed data for AMZN saved to: /content/drive/MyDrive/StockData/AMZN_2021-01-01_to_2025-03-16_processed.csv\n",
            "âœ… Processed data for MSFT saved to: /content/drive/MyDrive/StockData/MSFT_2021-01-01_to_2025-03-16_processed.csv\n",
            "âœ… Processed data for SPY saved to: /content/drive/MyDrive/StockData/SPY_2021-01-01_to_2025-03-16_processed.csv\n",
            "âœ… Processed data for QQQ saved to: /content/drive/MyDrive/StockData/QQQ_2021-01-01_to_2025-03-16_processed.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "def scale_data(df, target_col='Close', exclude_cols=['sentiment_polarity', 'sentiment_subjectivity']):\n",
        "    \"\"\"\n",
        "    Scales numeric columns (excluding exclude_cols and target_col for separate scaler).\n",
        "    Returns the scaled dataframe, a features scaler, and a target scaler.\n",
        "    Version: 2025-03-15\n",
        "    \"\"\"\n",
        "    # Features: all numeric columns except target and excluded sentiment columns\n",
        "    feature_cols = [col for col in df.select_dtypes(include=['float64','int64']).columns\n",
        "                    if col not in exclude_cols + [target_col]]\n",
        "    scaler_features = MinMaxScaler()\n",
        "    df_features_scaled = df.copy()\n",
        "    df_features_scaled[feature_cols] = scaler_features.fit_transform(df_features_scaled[feature_cols])\n",
        "\n",
        "    # Target scaler: scale the target column separately\n",
        "    scaler_target = MinMaxScaler()\n",
        "    df_features_scaled[target_col] = scaler_target.fit_transform(df_features_scaled[[target_col]])\n",
        "\n",
        "    return df_features_scaled, scaler_features, scaler_target\n",
        "\n",
        "def scale_and_save(ticker, processed_filepath):\n",
        "    \"\"\"\n",
        "    Loads processed data, scales it, and saves the feature and target scalers with the ticker and current date.\n",
        "    Version: 2025-03-15\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(processed_filepath)\n",
        "    df_scaled, scaler_features, scaler_target = scale_data(df)\n",
        "\n",
        "    scaler_dir = \"/content/drive/MyDrive/StockScalers\"\n",
        "    os.makedirs(scaler_dir, exist_ok=True)\n",
        "    today_str = datetime.date.today().strftime(\"%Y%m%d\")\n",
        "    scaler_features_path = os.path.join(scaler_dir, f\"{ticker}_{today_str}_features_scaler.pkl\")\n",
        "    scaler_target_path = os.path.join(scaler_dir, f\"{ticker}_{today_str}_target_scaler.pkl\")\n",
        "    joblib.dump(scaler_features, scaler_features_path)\n",
        "    joblib.dump(scaler_target, scaler_target_path)\n",
        "    print(f\"âœ… Feature scaler for {ticker} saved to: {scaler_features_path}\")\n",
        "    print(f\"âœ… Target scaler for {ticker} saved to: {scaler_target_path}\")\n",
        "\n",
        "    scaled_filepath = processed_filepath.replace(\"_processed.csv\", \"_scaled.csv\")\n",
        "    df_scaled.to_csv(scaled_filepath, index=False)\n",
        "    print(f\"âœ… Scaled data for {ticker} saved to: {scaled_filepath}\")\n",
        "    return scaled_filepath, scaler_features, scaler_target\n",
        "\n",
        "import glob, os\n",
        "processed_files = glob.glob(\"/content/drive/MyDrive/StockData/*_processed.csv\")\n",
        "for file in processed_files:\n",
        "    ticker = os.path.basename(file).split(\"_\")[0]\n",
        "    scale_and_save(ticker, file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAWTGjXcur5i",
        "outputId": "a6b8c686-9b65-4e21-fa33-1cdf92e2a7e4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Feature scaler for AAPL saved to: /content/drive/MyDrive/StockScalers/AAPL_20250316_features_scaler.pkl\n",
            "âœ… Target scaler for AAPL saved to: /content/drive/MyDrive/StockScalers/AAPL_20250316_target_scaler.pkl\n",
            "âœ… Scaled data for AAPL saved to: /content/drive/MyDrive/StockData/AAPL_2021-01-01_to_2025-03-16_scaled.csv\n",
            "âœ… Feature scaler for AMZN saved to: /content/drive/MyDrive/StockScalers/AMZN_20250316_features_scaler.pkl\n",
            "âœ… Target scaler for AMZN saved to: /content/drive/MyDrive/StockScalers/AMZN_20250316_target_scaler.pkl\n",
            "âœ… Scaled data for AMZN saved to: /content/drive/MyDrive/StockData/AMZN_2021-01-01_to_2025-03-16_scaled.csv\n",
            "âœ… Feature scaler for MSFT saved to: /content/drive/MyDrive/StockScalers/MSFT_20250316_features_scaler.pkl\n",
            "âœ… Target scaler for MSFT saved to: /content/drive/MyDrive/StockScalers/MSFT_20250316_target_scaler.pkl\n",
            "âœ… Scaled data for MSFT saved to: /content/drive/MyDrive/StockData/MSFT_2021-01-01_to_2025-03-16_scaled.csv\n",
            "âœ… Feature scaler for SPY saved to: /content/drive/MyDrive/StockScalers/SPY_20250316_features_scaler.pkl\n",
            "âœ… Target scaler for SPY saved to: /content/drive/MyDrive/StockScalers/SPY_20250316_target_scaler.pkl\n",
            "âœ… Scaled data for SPY saved to: /content/drive/MyDrive/StockData/SPY_2021-01-01_to_2025-03-16_scaled.csv\n",
            "âœ… Feature scaler for QQQ saved to: /content/drive/MyDrive/StockScalers/QQQ_20250316_features_scaler.pkl\n",
            "âœ… Target scaler for QQQ saved to: /content/drive/MyDrive/StockScalers/QQQ_20250316_target_scaler.pkl\n",
            "âœ… Scaled data for QQQ saved to: /content/drive/MyDrive/StockData/QQQ_2021-01-01_to_2025-03-16_scaled.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Same helper function for creating sequences:\n",
        "def create_sequences(df, feature_cols, label_col='Close', sequence_length=60):\n",
        "    \"\"\"\n",
        "    Creates training sequences from the DataFrame.\n",
        "    Returns X with shape (num_samples, sequence_length, num_features) and y with shape (num_samples,).\n",
        "    Version: 2025-03-16\n",
        "    \"\"\"\n",
        "    data_array = df[feature_cols].values\n",
        "    labels = df[label_col].values\n",
        "    X, y = [], []\n",
        "    for i in range(len(df) - sequence_length):\n",
        "        seq_x = data_array[i : i + sequence_length]\n",
        "        seq_y = labels[i + sequence_length]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Customize these to match your columns and desired window length\n",
        "feature_cols = [\n",
        "    'Open', 'High', 'Low', 'Close', 'Volume',\n",
        "    'SMA_10', 'SMA_20', 'EMA_10', 'EMA_20', 'RSI',\n",
        "    'MACD', 'MACD_Signal', 'sentiment_polarity', 'sentiment_subjectivity'\n",
        "]\n",
        "sequence_length = 60\n",
        "\n",
        "# Location of your scaled CSV files\n",
        "scaled_files = glob.glob(\"/content/drive/MyDrive/StockData/*_scaled.csv\")\n",
        "\n",
        "for file in scaled_files:\n",
        "    filename = os.path.basename(file)  # e.g. \"AAPL_2021-01-01_to_2025-03-16_scaled.csv\"\n",
        "    # Parse the ticker from the first underscore-delimited segment\n",
        "    # e.g. \"AAPL\" from \"AAPL_2021-01-01_to_2025-03-16_scaled.csv\"\n",
        "    ticker = filename.split(\"_\")[0]\n",
        "\n",
        "    print(f\"\\n=== Building sequences for {ticker} from file: {filename} ===\")\n",
        "\n",
        "    # Load the scaled CSV\n",
        "    df_scaled = pd.read_csv(file)\n",
        "\n",
        "    # Create sequences\n",
        "    X_all, y_all = create_sequences(df_scaled, feature_cols, label_col='Close', sequence_length=sequence_length)\n",
        "    total_samples = len(X_all)\n",
        "    if total_samples == 0:\n",
        "        print(f\"âš ï¸ Not enough data to create sequences for {ticker}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Train/Validation/Test Split (70/15/15)\n",
        "    train_end = int(total_samples * 0.7)\n",
        "    val_end = int(total_samples * 0.85)\n",
        "\n",
        "    X_train = X_all[:train_end]\n",
        "    y_train = y_all[:train_end]\n",
        "    X_val   = X_all[train_end:val_end]\n",
        "    y_val   = y_all[train_end:val_end]\n",
        "    X_test  = X_all[val_end:]\n",
        "    y_test  = y_all[val_end:]\n",
        "\n",
        "    # Save the .npy files (so Code Cell 5 can load them)\n",
        "    # You can choose any folder structure. Here, weâ€™ll save in the same folder as the scaled CSV.\n",
        "    save_dir = os.path.dirname(file)\n",
        "    np.save(os.path.join(save_dir, f\"{ticker}_X_train.npy\"), X_train)\n",
        "    np.save(os.path.join(save_dir, f\"{ticker}_y_train.npy\"), y_train)\n",
        "    np.save(os.path.join(save_dir, f\"{ticker}_X_val.npy\"), X_val)\n",
        "    np.save(os.path.join(save_dir, f\"{ticker}_y_val.npy\"), y_val)\n",
        "    np.save(os.path.join(save_dir, f\"{ticker}_X_test.npy\"), X_test)\n",
        "    np.save(os.path.join(save_dir, f\"{ticker}_y_test.npy\"), y_test)\n",
        "\n",
        "    print(f\"âœ… Sequences created and saved for {ticker}.\")\n",
        "    print(f\"   X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "    print(f\"   X_val:   {X_val.shape}, y_val:   {y_val.shape}\")\n",
        "    print(f\"   X_test:  {X_test.shape}, y_test:  {y_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMXXTj9dJq2w",
        "outputId": "62e4101a-5690-4679-e4a8-f152352efee1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Building sequences for AAPL from file: AAPL_2021-01-01_to_2025-03-16_scaled.csv ===\n",
            "âœ… Sequences created and saved for AAPL.\n",
            "   X_train: (294, 60, 14), y_train: (294,)\n",
            "   X_val:   (63, 60, 14), y_val:   (63,)\n",
            "   X_test:  (64, 60, 14), y_test:  (64,)\n",
            "\n",
            "=== Building sequences for AMZN from file: AMZN_2021-01-01_to_2025-03-16_scaled.csv ===\n",
            "âœ… Sequences created and saved for AMZN.\n",
            "   X_train: (294, 60, 14), y_train: (294,)\n",
            "   X_val:   (63, 60, 14), y_val:   (63,)\n",
            "   X_test:  (64, 60, 14), y_test:  (64,)\n",
            "\n",
            "=== Building sequences for MSFT from file: MSFT_2021-01-01_to_2025-03-16_scaled.csv ===\n",
            "âœ… Sequences created and saved for MSFT.\n",
            "   X_train: (294, 60, 14), y_train: (294,)\n",
            "   X_val:   (63, 60, 14), y_val:   (63,)\n",
            "   X_test:  (64, 60, 14), y_test:  (64,)\n",
            "\n",
            "=== Building sequences for SPY from file: SPY_2021-01-01_to_2025-03-16_scaled.csv ===\n",
            "âœ… Sequences created and saved for SPY.\n",
            "   X_train: (294, 60, 14), y_train: (294,)\n",
            "   X_val:   (63, 60, 14), y_val:   (63,)\n",
            "   X_test:  (64, 60, 14), y_test:  (64,)\n",
            "\n",
            "=== Building sequences for QQQ from file: QQQ_2021-01-01_to_2025-03-16_scaled.csv ===\n",
            "âœ… Sequences created and saved for QQQ.\n",
            "   X_train: (294, 60, 14), y_train: (294,)\n",
            "   X_val:   (63, 60, 14), y_val:   (63,)\n",
            "   X_test:  (64, 60, 14), y_test:  (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import joblib\n",
        "import keras_tuner as kt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout,\n",
        "                                     SimpleRNN, LSTM, Concatenate, Multiply, Activation, Lambda)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Helper: inverse transform a single feature with a scaler\n",
        "# ----------------------------------------------------\n",
        "def inverse_transform_single_feature(scaler, data):\n",
        "    \"\"\"\n",
        "    Inverse-transforms a single feature using the given scaler.\n",
        "    Version: 2025-03-16\n",
        "    \"\"\"\n",
        "    data = np.array(data).reshape(-1, 1)\n",
        "    return scaler.inverse_transform(data)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Helper: Build the Ensemble Model (CNN, RNN, LSTM)\n",
        "# ----------------------------------------------------\n",
        "def build_ensemble_model(hp, input_shape):\n",
        "    \"\"\"\n",
        "    Builds an ensemble model combining CNN, RNN, and LSTM branches with adaptive fusion.\n",
        "    Version: 2025-03-16\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # --- CNN Branch ---\n",
        "    cnn = Conv1D(\n",
        "        filters=hp.Choice('cnn_filters', [64, 128, 256]),\n",
        "        kernel_size=hp.Choice('cnn_kernel_size', [3, 5, 7]),\n",
        "        activation='relu',\n",
        "        padding='same'\n",
        "    )(inputs)\n",
        "    if input_shape[0] > 1:\n",
        "        cnn = MaxPooling1D(pool_size=2)(cnn)\n",
        "    cnn = Flatten()(cnn)\n",
        "    cnn = Dense(50, activation='relu')(cnn)\n",
        "\n",
        "    # --- RNN Branch ---\n",
        "    rnn = SimpleRNN(units=hp.Choice('rnn_units', [75, 100, 125, 150]), return_sequences=True)(inputs)\n",
        "    rnn = Dropout(hp.Choice('dropout_rate', [0.05, 0.1, 0.2]))(rnn)\n",
        "    rnn = SimpleRNN(units=hp.Choice('rnn_units_2', [75, 100, 125, 150]))(rnn)\n",
        "    rnn = Dropout(hp.Choice('dropout_rate_2', [0.05, 0.1, 0.2]))(rnn)\n",
        "    rnn = Dense(50, activation='relu')(rnn)\n",
        "\n",
        "    # --- LSTM Branch ---\n",
        "    lstm = LSTM(units=hp.Choice('lstm_units', [50, 75, 100]), return_sequences=True)(inputs)\n",
        "    lstm = LSTM(units=hp.Choice('lstm_units_2', [50, 75, 100]))(lstm)\n",
        "    lstm = Dense(50, activation='relu')(lstm)\n",
        "    lstm = Dropout(hp.Choice('dropout_rate_lstm', [0.1, 0.2, 0.3]))(lstm)\n",
        "\n",
        "    # --- Adaptive Fusion ---\n",
        "    combined = Concatenate()([cnn, rnn, lstm])\n",
        "    weight_logits = Dense(3)(combined)\n",
        "    branch_weights = Activation('softmax')(weight_logits)\n",
        "    cnn_weight  = Lambda(lambda x: tf.reshape(x[:, 0], (-1, 1)))(branch_weights)\n",
        "    rnn_weight  = Lambda(lambda x: tf.reshape(x[:, 1], (-1, 1)))(branch_weights)\n",
        "    lstm_weight = Lambda(lambda x: tf.reshape(x[:, 2], (-1, 1)))(branch_weights)\n",
        "    cnn_scaled  = Multiply()([cnn, cnn_weight])\n",
        "    rnn_scaled  = Multiply()([rnn, rnn_weight])\n",
        "    lstm_scaled = Multiply()([lstm, lstm_weight])\n",
        "    merged = Concatenate()([cnn_scaled, rnn_scaled, lstm_scaled])\n",
        "\n",
        "    # --- Dense Layers ---\n",
        "    merged = Dense(\n",
        "        units=hp.Choice('dense_units', [50, 100, 150]),\n",
        "        activation=\"relu\",\n",
        "        kernel_regularizer=l2(0.001)\n",
        "    )(merged)\n",
        "    merged = Dropout(hp.Choice('dropout_rate_dense', [0.1, 0.2, 0.3]))(merged)\n",
        "    output = Dense(1)(merged)\n",
        "\n",
        "    model = Model(inputs, output)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp.Choice('learning_rate', [0.001, 0.0005, 0.0001])),\n",
        "        loss=\"mse\",\n",
        "        metrics=[\"mae\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# Main training pipeline (Version 2025-03-16)\n",
        "# ----------------------------------------------------\n",
        "\n",
        "# Where your *_X_train.npy, *_y_train.npy, etc. are located:\n",
        "data_dir = \"/content/drive/MyDrive/StockData\"\n",
        "\n",
        "# We'll look for files named e.g. AAPL_X_train.npy, AMZN_X_train.npy, etc.\n",
        "train_files = glob.glob(os.path.join(data_dir, \"*_X_train.npy\"))\n",
        "\n",
        "for xtrain_path in train_files:\n",
        "    # Example: xtrain_path = \"/content/drive/MyDrive/StockData/AAPL_X_train.npy\"\n",
        "    filename = os.path.basename(xtrain_path)  # e.g. \"AAPL_X_train.npy\"\n",
        "    ticker = filename.split(\"_\")[0]           # e.g. \"AAPL\"\n",
        "\n",
        "    # Construct paths for y_train, X_val, y_val, X_test, y_test\n",
        "    ytrain_path = os.path.join(data_dir, f\"{ticker}_y_train.npy\")\n",
        "    xval_path   = os.path.join(data_dir, f\"{ticker}_X_val.npy\")\n",
        "    yval_path   = os.path.join(data_dir, f\"{ticker}_y_val.npy\")\n",
        "    xtest_path  = os.path.join(data_dir, f\"{ticker}_X_test.npy\")\n",
        "    ytest_path  = os.path.join(data_dir, f\"{ticker}_y_test.npy\")\n",
        "\n",
        "    # Make sure all files exist\n",
        "    needed_files = [ytrain_path, xval_path, yval_path, xtest_path, ytest_path]\n",
        "    if not all(os.path.exists(p) for p in needed_files):\n",
        "        print(f\"âš ï¸ Missing some .npy files for {ticker}. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # Load the data\n",
        "    X_train = np.load(xtrain_path)\n",
        "    y_train = np.load(ytrain_path)\n",
        "    X_val   = np.load(xval_path)\n",
        "    y_val   = np.load(yval_path)\n",
        "    X_test  = np.load(xtest_path)\n",
        "    y_test  = np.load(ytest_path)\n",
        "\n",
        "    # Check shapes\n",
        "    if X_train.ndim == 2:\n",
        "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "        X_val   = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
        "        X_test  = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "    print(f\"\\n===== Training Ensemble Model for {ticker} =====\")\n",
        "    print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "    print(f\"X_val:   {X_val.shape},   y_val:   {y_val.shape}\")\n",
        "    print(f\"X_test:  {X_test.shape},  y_test:  {y_test.shape}\")\n",
        "\n",
        "    # Load target scaler for inverse transform of predictions (optional)\n",
        "    # Adjust if your naming is different. For example, if you saved them as {ticker}_YYYYMMDD_target_scaler.pkl\n",
        "    # We'll do a simple approach: pick the most recent target scaler for that ticker.\n",
        "    scaler_search = glob.glob(os.path.join(\"/content/drive/MyDrive/StockScalers\", f\"{ticker}_*_target_scaler.pkl\"))\n",
        "    if not scaler_search:\n",
        "        print(f\"âš ï¸ No target scaler found for {ticker}. Will skip inverse scaling.\")\n",
        "        scaler_y = None\n",
        "    else:\n",
        "        # Just pick the last (or only) match\n",
        "        scaler_search.sort()\n",
        "        scaler_y_path = scaler_search[-1]\n",
        "        scaler_y = joblib.load(scaler_y_path)\n",
        "        print(f\"Loaded target scaler for {ticker} from: {scaler_y_path}\")\n",
        "\n",
        "    # Create a folder for saving the model and outputs\n",
        "    model_folder = os.path.join(data_dir, f\"BestEnsembleModel_{ticker}\")\n",
        "    os.makedirs(model_folder, exist_ok=True)\n",
        "\n",
        "    # Hyperparameter Tuning\n",
        "    best_hps_file = os.path.join(model_folder, \"best_hyperparameters.json\")\n",
        "    tuning_flag_file = os.path.join(model_folder, \"hp_tuning_complete.flag\")\n",
        "\n",
        "    def model_builder(hp):\n",
        "        return build_ensemble_model(hp, input_shape=input_shape)\n",
        "\n",
        "    if not os.path.exists(tuning_flag_file):\n",
        "        # If not tuned yet, run tuner\n",
        "        if os.path.exists(best_hps_file):\n",
        "            os.remove(best_hps_file)\n",
        "        print(f\"ğŸ” Hyperparameter tuning for {ticker}...\")\n",
        "        tuner = kt.BayesianOptimization(\n",
        "            model_builder,\n",
        "            objective=\"val_loss\",\n",
        "            max_trials=15,\n",
        "            executions_per_trial=2,\n",
        "            directory=os.path.join(model_folder, \"tuning\"),\n",
        "            project_name=f\"ensemble_{ticker}\"\n",
        "        )\n",
        "        tuner.search(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=1)\n",
        "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "        best_hps_dict = {param: best_hps.get(param) for param in best_hps.values.keys()}\n",
        "        with open(best_hps_file, \"w\") as f:\n",
        "            json.dump(best_hps_dict, f)\n",
        "        with open(tuning_flag_file, \"w\") as f:\n",
        "            f.write(\"tuning complete\")\n",
        "        model = tuner.hypermodel.build(best_hps)\n",
        "    else:\n",
        "        # Already tuned; load best hyperparameters\n",
        "        print(f\"âœ… Loading best hyperparameters for {ticker} from file...\")\n",
        "        with open(best_hps_file, \"r\") as f:\n",
        "            best_hps_dict = json.load(f)\n",
        "        best_hps = kt.HyperParameters()\n",
        "        for key, value in best_hps_dict.items():\n",
        "            best_hps.Fixed(key, value)\n",
        "        model = build_ensemble_model(best_hps, input_shape)\n",
        "\n",
        "    print(\"Best hyperparameters:\", best_hps_dict)\n",
        "\n",
        "    # Train the Model\n",
        "    BATCH_SIZE = 32\n",
        "    early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=500,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[early_stop],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    best_model_path = os.path.join(model_folder, \"best_ensemble_model.keras\")\n",
        "    model.save(best_model_path)\n",
        "    print(f\"âœ… Best Ensemble Model for {ticker} saved to {best_model_path}\")\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"{ticker} - Training & Validation Loss\")\n",
        "    plt.legend()\n",
        "    history_plot_path = os.path.join(model_folder, \"training_history.png\")\n",
        "    plt.savefig(history_plot_path)\n",
        "    plt.close()\n",
        "    print(f\"âœ… Training history graph saved to {history_plot_path}\")\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f\"Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    if scaler_y is not None:\n",
        "        predictions_rescaled = inverse_transform_single_feature(scaler_y, predictions)\n",
        "        y_test_rescaled = inverse_transform_single_feature(scaler_y, y_test)\n",
        "    else:\n",
        "        predictions_rescaled = predictions.flatten()\n",
        "        y_test_rescaled = y_test.flatten()\n",
        "\n",
        "    # Calculate Directional Accuracy\n",
        "    correct_direction = 0\n",
        "    for i in range(len(y_test_rescaled)-1):\n",
        "        if (y_test_rescaled[i+1]-y_test_rescaled[i])*(predictions_rescaled[i+1]-predictions_rescaled[i]) >= 0:\n",
        "            correct_direction += 1\n",
        "    directional_accuracy = (correct_direction / (len(y_test_rescaled)-1)) * 100 if len(y_test_rescaled) > 1 else 0\n",
        "    print(f\"Directional Accuracy: {directional_accuracy:.2f}%\")\n",
        "\n",
        "    # Plot predicted vs actual with directional coloring\n",
        "    x_vals = np.arange(len(y_test_rescaled))\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(x_vals, y_test_rescaled, label=\"Actual Price\", color=\"blue\")\n",
        "    for i in range(len(x_vals)-1):\n",
        "        color = 'green' if (y_test_rescaled[i+1]-y_test_rescaled[i])*(predictions_rescaled[i+1]-predictions_rescaled[i]) >= 0 else 'red'\n",
        "        plt.plot(x_vals[i:i+2], predictions_rescaled[i:i+2], color=color)\n",
        "    blue_line = Line2D([0], [0], color='blue', label='Actual Price')\n",
        "    green_line = Line2D([0], [0], color='green', label='Predicted (Correct Dir)')\n",
        "    red_line = Line2D([0], [0], color='red', label='Predicted (Wrong Dir)')\n",
        "    plt.legend(handles=[blue_line, green_line, red_line])\n",
        "    plt.xlabel(\"Time\")\n",
        "    plt.ylabel(\"Price\")\n",
        "    plt.title(f\"{ticker} - Predicted vs Actual\")\n",
        "    pred_plot_path = os.path.join(model_folder, \"pred_vs_actual.png\")\n",
        "    plt.savefig(pred_plot_path)\n",
        "    plt.close()\n",
        "    print(f\"âœ… Prediction vs Actual plot saved to {pred_plot_path}\")\n",
        "\n",
        "    # Save performance stats\n",
        "    stats_filepath = os.path.join(model_folder, \"model_performance.txt\")\n",
        "    with open(stats_filepath, \"w\") as f:\n",
        "        f.write(f\"Test Loss: {loss:.4f}\\n\")\n",
        "        f.write(f\"Test MAE: {mae:.4f}\\n\")\n",
        "        f.write(f\"Directional Accuracy: {directional_accuracy:.2f}%\\n\")\n",
        "    print(f\"âœ… Model performance stats saved to {stats_filepath}\")\n",
        "\n",
        "    print(f\"ğŸ‰ Finished training for {ticker}!\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tCgbPkOwDsO",
        "outputId": "0ff9deca-7b97-408c-bdd8-b1d12c85fe22"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 15 Complete [00h 03m 31s]\n",
            "val_loss: 0.015526477713137865\n",
            "\n",
            "Best val_loss So Far: 0.00865392992272973\n",
            "Total elapsed time: 00h 54m 18s\n",
            "Best hyperparameters: {'cnn_filters': 128, 'cnn_kernel_size': 7, 'rnn_units': 75, 'dropout_rate': 0.1, 'rnn_units_2': 100, 'dropout_rate_2': 0.05, 'lstm_units': 100, 'lstm_units_2': 50, 'dropout_rate_lstm': 0.2, 'dense_units': 100, 'dropout_rate_dense': 0.1, 'learning_rate': 0.001}\n",
            "Epoch 1/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 270ms/step - loss: 0.2094 - mae: 0.2479 - val_loss: 0.1195 - val_mae: 0.1025\n",
            "Epoch 2/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step - loss: 0.1146 - mae: 0.0834 - val_loss: 0.1012 - val_mae: 0.0639\n",
            "Epoch 3/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.1013 - mae: 0.0731 - val_loss: 0.0966 - val_mae: 0.0986\n",
            "Epoch 4/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - loss: 0.0897 - mae: 0.0613 - val_loss: 0.0801 - val_mae: 0.0536\n",
            "Epoch 5/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - loss: 0.0781 - mae: 0.0516 - val_loss: 0.0720 - val_mae: 0.0535\n",
            "Epoch 6/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - loss: 0.0705 - mae: 0.0515 - val_loss: 0.0718 - val_mae: 0.0983\n",
            "Epoch 7/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 0.0642 - mae: 0.0547 - val_loss: 0.0618 - val_mae: 0.0755\n",
            "Epoch 8/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 149ms/step - loss: 0.0579 - mae: 0.0509 - val_loss: 0.0600 - val_mae: 0.0946\n",
            "Epoch 9/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 250ms/step - loss: 0.0526 - mae: 0.0489 - val_loss: 0.0556 - val_mae: 0.0953\n",
            "Epoch 10/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 0.0481 - mae: 0.0504 - val_loss: 0.0498 - val_mae: 0.0853\n",
            "Epoch 11/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0435 - mae: 0.0454 - val_loss: 0.0427 - val_mae: 0.0604\n",
            "Epoch 12/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0402 - mae: 0.0441 - val_loss: 0.0417 - val_mae: 0.0737\n",
            "Epoch 13/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 0.0373 - mae: 0.0453 - val_loss: 0.0442 - val_mae: 0.1024\n",
            "Epoch 14/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0352 - mae: 0.0468 - val_loss: 0.0431 - val_mae: 0.1085\n",
            "Epoch 15/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.0325 - mae: 0.0430 - val_loss: 0.0312 - val_mae: 0.0462\n",
            "Epoch 16/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.0299 - mae: 0.0399 - val_loss: 0.0317 - val_mae: 0.0639\n",
            "Epoch 17/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 0.0280 - mae: 0.0378 - val_loss: 0.0264 - val_mae: 0.0378\n",
            "Epoch 18/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 233ms/step - loss: 0.0265 - mae: 0.0389 - val_loss: 0.0257 - val_mae: 0.0433\n",
            "Epoch 19/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 174ms/step - loss: 0.0249 - mae: 0.0378 - val_loss: 0.0340 - val_mae: 0.1029\n",
            "Epoch 20/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 0.0245 - mae: 0.0425 - val_loss: 0.0290 - val_mae: 0.0832\n",
            "Epoch 21/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - loss: 0.0223 - mae: 0.0353 - val_loss: 0.0306 - val_mae: 0.0974\n",
            "Epoch 22/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - loss: 0.0212 - mae: 0.0332 - val_loss: 0.0237 - val_mae: 0.0596\n",
            "Epoch 23/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - loss: 0.0197 - mae: 0.0308 - val_loss: 0.0199 - val_mae: 0.0430\n",
            "Epoch 24/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0190 - mae: 0.0328 - val_loss: 0.0187 - val_mae: 0.0394\n",
            "Epoch 25/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - loss: 0.0183 - mae: 0.0353 - val_loss: 0.0182 - val_mae: 0.0431\n",
            "Epoch 26/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.0175 - mae: 0.0341 - val_loss: 0.0189 - val_mae: 0.0523\n",
            "Epoch 27/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 198ms/step - loss: 0.0181 - mae: 0.0444 - val_loss: 0.0168 - val_mae: 0.0427\n",
            "Epoch 28/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 208ms/step - loss: 0.0166 - mae: 0.0385 - val_loss: 0.0166 - val_mae: 0.0462\n",
            "Epoch 29/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - loss: 0.0151 - mae: 0.0311 - val_loss: 0.0165 - val_mae: 0.0472\n",
            "Epoch 30/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0147 - mae: 0.0321 - val_loss: 0.0160 - val_mae: 0.0469\n",
            "Epoch 31/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 0.0139 - mae: 0.0285 - val_loss: 0.0175 - val_mae: 0.0579\n",
            "Epoch 32/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.0134 - mae: 0.0308 - val_loss: 0.0142 - val_mae: 0.0444\n",
            "Epoch 33/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0127 - mae: 0.0287 - val_loss: 0.0145 - val_mae: 0.0481\n",
            "Epoch 34/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - loss: 0.0124 - mae: 0.0309 - val_loss: 0.0150 - val_mae: 0.0526\n",
            "Epoch 35/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0121 - mae: 0.0312 - val_loss: 0.0131 - val_mae: 0.0467\n",
            "Epoch 36/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 197ms/step - loss: 0.0119 - mae: 0.0337 - val_loss: 0.0141 - val_mae: 0.0524\n",
            "Epoch 37/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 205ms/step - loss: 0.0110 - mae: 0.0295 - val_loss: 0.0154 - val_mae: 0.0625\n",
            "Epoch 38/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 0.0105 - mae: 0.0272 - val_loss: 0.0117 - val_mae: 0.0451\n",
            "Epoch 39/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - loss: 0.0112 - mae: 0.0366 - val_loss: 0.0131 - val_mae: 0.0519\n",
            "Epoch 40/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - loss: 0.0099 - mae: 0.0283 - val_loss: 0.0170 - val_mae: 0.0784\n",
            "Epoch 41/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 0.0098 - mae: 0.0306 - val_loss: 0.0150 - val_mae: 0.0676\n",
            "Epoch 42/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - loss: 0.0101 - mae: 0.0341 - val_loss: 0.0149 - val_mae: 0.0688\n",
            "Epoch 43/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 0.0090 - mae: 0.0292 - val_loss: 0.0132 - val_mae: 0.0603\n",
            "Epoch 44/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - loss: 0.0090 - mae: 0.0303 - val_loss: 0.0117 - val_mae: 0.0539\n",
            "Epoch 45/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 170ms/step - loss: 0.0081 - mae: 0.0252 - val_loss: 0.0109 - val_mae: 0.0529\n",
            "Epoch 46/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 151ms/step - loss: 0.0081 - mae: 0.0277 - val_loss: 0.0120 - val_mae: 0.0585\n",
            "Epoch 47/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - loss: 0.0080 - mae: 0.0287 - val_loss: 0.0125 - val_mae: 0.0613\n",
            "Epoch 48/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - loss: 0.0075 - mae: 0.0286 - val_loss: 0.0114 - val_mae: 0.0568\n",
            "Epoch 49/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - loss: 0.0072 - mae: 0.0269 - val_loss: 0.0103 - val_mae: 0.0541\n",
            "Epoch 50/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 0.0067 - mae: 0.0225 - val_loss: 0.0117 - val_mae: 0.0609\n",
            "Epoch 51/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0068 - mae: 0.0265 - val_loss: 0.0097 - val_mae: 0.0522\n",
            "Epoch 52/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0065 - mae: 0.0250 - val_loss: 0.0094 - val_mae: 0.0505\n",
            "Epoch 53/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 0.0065 - mae: 0.0264 - val_loss: 0.0086 - val_mae: 0.0501\n",
            "Epoch 54/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 205ms/step - loss: 0.0066 - mae: 0.0295 - val_loss: 0.0105 - val_mae: 0.0584\n",
            "Epoch 55/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 201ms/step - loss: 0.0061 - mae: 0.0256 - val_loss: 0.0131 - val_mae: 0.0748\n",
            "Epoch 56/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 127ms/step - loss: 0.0064 - mae: 0.0317 - val_loss: 0.0106 - val_mae: 0.0615\n",
            "Epoch 57/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 0.0057 - mae: 0.0248 - val_loss: 0.0089 - val_mae: 0.0557\n",
            "Epoch 58/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - loss: 0.0057 - mae: 0.0273 - val_loss: 0.0090 - val_mae: 0.0562\n",
            "Epoch 59/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.0055 - mae: 0.0261 - val_loss: 0.0094 - val_mae: 0.0605\n",
            "Epoch 60/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.0052 - mae: 0.0246 - val_loss: 0.0089 - val_mae: 0.0562\n",
            "Epoch 61/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 0.0051 - mae: 0.0267 - val_loss: 0.0078 - val_mae: 0.0535\n",
            "Epoch 62/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0051 - mae: 0.0269 - val_loss: 0.0098 - val_mae: 0.0619\n",
            "Epoch 63/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 214ms/step - loss: 0.0052 - mae: 0.0288 - val_loss: 0.0128 - val_mae: 0.0795\n",
            "Epoch 64/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 145ms/step - loss: 0.0049 - mae: 0.0287 - val_loss: 0.0163 - val_mae: 0.0991\n",
            "Epoch 65/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - loss: 0.0050 - mae: 0.0296 - val_loss: 0.0076 - val_mae: 0.0537\n",
            "Epoch 66/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.0051 - mae: 0.0305 - val_loss: 0.0080 - val_mae: 0.0592\n",
            "Epoch 67/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - loss: 0.0051 - mae: 0.0329 - val_loss: 0.0078 - val_mae: 0.0565\n",
            "Epoch 68/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - loss: 0.0050 - mae: 0.0315 - val_loss: 0.0088 - val_mae: 0.0612\n",
            "Epoch 69/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 0.0042 - mae: 0.0258 - val_loss: 0.0104 - val_mae: 0.0695\n",
            "Epoch 70/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 0.0040 - mae: 0.0243 - val_loss: 0.0114 - val_mae: 0.0740\n",
            "Epoch 71/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - loss: 0.0038 - mae: 0.0243 - val_loss: 0.0108 - val_mae: 0.0724\n",
            "Epoch 72/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 211ms/step - loss: 0.0038 - mae: 0.0241 - val_loss: 0.0096 - val_mae: 0.0676\n",
            "Epoch 73/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0036 - mae: 0.0237 - val_loss: 0.0139 - val_mae: 0.0881\n",
            "Epoch 74/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - loss: 0.0040 - mae: 0.0284 - val_loss: 0.0098 - val_mae: 0.0695\n",
            "Epoch 75/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - loss: 0.0037 - mae: 0.0266 - val_loss: 0.0081 - val_mae: 0.0614\n",
            "Epoch 76/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0036 - mae: 0.0268 - val_loss: 0.0074 - val_mae: 0.0592\n",
            "Epoch 77/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 0.0036 - mae: 0.0266 - val_loss: 0.0084 - val_mae: 0.0637\n",
            "Epoch 78/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.0031 - mae: 0.0208 - val_loss: 0.0085 - val_mae: 0.0644\n",
            "Epoch 79/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step - loss: 0.0033 - mae: 0.0244 - val_loss: 0.0121 - val_mae: 0.0824\n",
            "Epoch 80/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 143ms/step - loss: 0.0035 - mae: 0.0273 - val_loss: 0.0079 - val_mae: 0.0625\n",
            "Epoch 81/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 231ms/step - loss: 0.0032 - mae: 0.0259 - val_loss: 0.0073 - val_mae: 0.0620\n",
            "Epoch 82/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 129ms/step - loss: 0.0034 - mae: 0.0290 - val_loss: 0.0067 - val_mae: 0.0579\n",
            "Epoch 83/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - loss: 0.0033 - mae: 0.0289 - val_loss: 0.0074 - val_mae: 0.0597\n",
            "Epoch 84/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - loss: 0.0027 - mae: 0.0212 - val_loss: 0.0075 - val_mae: 0.0611\n",
            "Epoch 85/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.0029 - mae: 0.0240 - val_loss: 0.0068 - val_mae: 0.0576\n",
            "Epoch 86/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 0.0033 - mae: 0.0297 - val_loss: 0.0062 - val_mae: 0.0550\n",
            "Epoch 87/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - loss: 0.0030 - mae: 0.0280 - val_loss: 0.0086 - val_mae: 0.0676\n",
            "Epoch 88/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.0024 - mae: 0.0215 - val_loss: 0.0101 - val_mae: 0.0755\n",
            "Epoch 89/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - loss: 0.0028 - mae: 0.0272 - val_loss: 0.0093 - val_mae: 0.0707\n",
            "Epoch 90/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 229ms/step - loss: 0.0027 - mae: 0.0255 - val_loss: 0.0121 - val_mae: 0.0847\n",
            "Epoch 91/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 159ms/step - loss: 0.0027 - mae: 0.0260 - val_loss: 0.0114 - val_mae: 0.0820\n",
            "Epoch 92/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - loss: 0.0029 - mae: 0.0287 - val_loss: 0.0136 - val_mae: 0.0909\n",
            "Epoch 93/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 0.0028 - mae: 0.0277 - val_loss: 0.0086 - val_mae: 0.0676\n",
            "Epoch 94/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 0.0024 - mae: 0.0241 - val_loss: 0.0073 - val_mae: 0.0626\n",
            "Epoch 95/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - loss: 0.0024 - mae: 0.0238 - val_loss: 0.0090 - val_mae: 0.0714\n",
            "Epoch 96/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 0.0022 - mae: 0.0222 - val_loss: 0.0135 - val_mae: 0.0891\n",
            "Epoch 97/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 0.0024 - mae: 0.0260 - val_loss: 0.0121 - val_mae: 0.0834\n",
            "Epoch 98/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 192ms/step - loss: 0.0023 - mae: 0.0242 - val_loss: 0.0068 - val_mae: 0.0611\n",
            "Epoch 99/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 225ms/step - loss: 0.0020 - mae: 0.0216 - val_loss: 0.0063 - val_mae: 0.0606\n",
            "Epoch 100/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 124ms/step - loss: 0.0020 - mae: 0.0225 - val_loss: 0.0070 - val_mae: 0.0653\n",
            "Epoch 101/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - loss: 0.0019 - mae: 0.0211 - val_loss: 0.0075 - val_mae: 0.0664\n",
            "Epoch 102/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 117ms/step - loss: 0.0020 - mae: 0.0217 - val_loss: 0.0071 - val_mae: 0.0662\n",
            "Epoch 103/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - loss: 0.0018 - mae: 0.0212 - val_loss: 0.0093 - val_mae: 0.0741\n",
            "Epoch 104/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 124ms/step - loss: 0.0020 - mae: 0.0235 - val_loss: 0.0165 - val_mae: 0.1044\n",
            "Epoch 105/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step - loss: 0.0025 - mae: 0.0283 - val_loss: 0.0122 - val_mae: 0.0851\n",
            "Epoch 106/500\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - loss: 0.0020 - mae: 0.0228 - val_loss: 0.0087 - val_mae: 0.0716\n",
            "âœ… Best Ensemble Model for QQQ saved to /content/drive/MyDrive/StockData/BestEnsembleModel_QQQ/best_ensemble_model.keras\n",
            "âœ… Training history graph saved to /content/drive/MyDrive/StockData/BestEnsembleModel_QQQ/training_history.png\n",
            "Test Loss: 0.0151, Test MAE: 0.1053\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step\n",
            "Directional Accuracy: 44.44%\n",
            "âœ… Prediction vs Actual plot saved to /content/drive/MyDrive/StockData/BestEnsembleModel_QQQ/pred_vs_actual.png\n",
            "âœ… Model performance stats saved to /content/drive/MyDrive/StockData/BestEnsembleModel_QQQ/model_performance.txt\n",
            "ğŸ‰ Finished training for QQQ!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def daily_data_pipeline(ticker, date, model, scaler, sequence_length=60):\n",
        "    \"\"\"\n",
        "    Pipeline to:\n",
        "      1. Fetch the latest daily stock data (with sentiment and technical indicators).\n",
        "      2. Preprocess and calculate technical indicators.\n",
        "      3. Scale using the saved scaler.\n",
        "      4. Build a rolling window sequence and predict.\n",
        "    \"\"\"\n",
        "    # Set your Polygon API key from environment variable\n",
        "    api_key = os.environ.get(\"POLYGON_API_KEY\", \"YOUR_POLYGON_API_KEY\")\n",
        "    if api_key == \"YOUR_POLYGON_API_KEY\":\n",
        "        print(\"Please set your Polygon API key in the environment variable POLYGON_API_KEY\")\n",
        "        return None\n",
        "\n",
        "    # Fetch stock data for the day (using the raw data function)\n",
        "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{date}/{date}?apiKey={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error fetching data for {ticker} on {date}: {response.text}\")\n",
        "        return None\n",
        "    data = response.json()\n",
        "    if \"results\" not in data:\n",
        "        print(f\"No stock data available for {ticker} on {date}.\")\n",
        "        return None\n",
        "    df_stock = pd.DataFrame(data[\"results\"])\n",
        "    df_stock[\"Date\"] = pd.to_datetime(df_stock[\"t\"], unit=\"ms\").dt.date\n",
        "    df_stock.rename(columns={\"o\": \"Open\", \"h\": \"High\", \"l\": \"Low\", \"c\": \"Close\", \"v\": \"Volume\"}, inplace=True)\n",
        "    df_stock = df_stock[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "\n",
        "    # (Optionally) Fetch and process sentiment data for the day...\n",
        "    # For simplicity, assume neutral sentiment if not available.\n",
        "    df_stock[\"sentiment_polarity\"] = 0\n",
        "    df_stock[\"sentiment_subjectivity\"] = 0\n",
        "\n",
        "    # Calculate technical indicators\n",
        "    df_stock = calculate_technical_indicators(df_stock)\n",
        "\n",
        "    # In a production setting, you would build your sequence using the previous (historical) data.\n",
        "    # Here we simulate a rolling window by reading a stored scaled CSV file and appending the new day.\n",
        "    historical_filepath = f\"/content/drive/MyDrive/StockData/{ticker}_processed.csv\"\n",
        "    if not os.path.exists(historical_filepath):\n",
        "        print(\"Historical data file not found for\", ticker)\n",
        "        return None\n",
        "    df_hist = pd.read_csv(historical_filepath, parse_dates=[\"Date\"])\n",
        "    df_hist = calculate_technical_indicators(df_hist)\n",
        "\n",
        "    # Append today's data and then scale using the saved scaler\n",
        "    df_combined = pd.concat([df_hist, df_stock], ignore_index=True)\n",
        "    df_scaled = df_combined.copy()\n",
        "    # Identify columns to scale (same as in scaling function)\n",
        "    numeric_cols = [col for col in df_scaled.select_dtypes(include=['float64','int64']).columns\n",
        "                    if col not in ['sentiment_polarity', 'sentiment_subjectivity']]\n",
        "    df_scaled[numeric_cols] = scaler.transform(df_scaled[numeric_cols])\n",
        "\n",
        "    # Build sequence: take the last 'sequence_length' rows for features.\n",
        "    try:\n",
        "        # Define the feature set for prediction (adjust column names as needed)\n",
        "        feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume',\n",
        "                        'SMA_10', 'SMA_20', 'EMA_10', 'EMA_20', 'RSI']\n",
        "        seq = df_scaled[feature_cols].tail(sequence_length).values\n",
        "    except Exception as e:\n",
        "        print(\"Error building feature sequence:\", e)\n",
        "        return None\n",
        "\n",
        "    if seq.shape[0] < sequence_length:\n",
        "        pad = np.tile(seq[0], (sequence_length - seq.shape[0], 1))\n",
        "        seq = np.vstack([pad, seq])\n",
        "    seq = seq.reshape(1, sequence_length, len(feature_cols))\n",
        "\n",
        "    prediction = model.predict(seq)\n",
        "    print(f\"Predicted price for {ticker} on {date}: {prediction[0][0]}\")\n",
        "    return prediction[0][0]\n",
        "\n",
        "# Example usage for daily deployment:\n",
        "# Load your trained model and the saved scaler (for the target features, if different, adjust accordingly)\n",
        "# Assume best_model has been loaded and scaler was saved in \"/content/drive/MyDrive/StockScalers/{ticker}_scaler.pkl\"\n",
        "ticker = \"AAPL\"\n",
        "today_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "scaler_path = f\"/content/drive/MyDrive/StockScalers/{ticker}_scaler.pkl\"\n",
        "if os.path.exists(scaler_path):\n",
        "    scaler = joblib.load(scaler_path)\n",
        "else:\n",
        "    print(\"Scaler file not found for\", ticker)\n",
        "    scaler = None\n",
        "\n",
        "# Load the model (or use the one from training)\n",
        "from tensorflow.keras.models import load_model\n",
        "model_path = f\"/content/drive/MyDrive/StockModels/Ensemble/best_ensemble_model.keras\"\n",
        "if os.path.exists(model_path):\n",
        "    deployed_model = load_model(model_path)\n",
        "else:\n",
        "    print(\"Model file not found at\", model_path)\n",
        "    deployed_model = None\n",
        "\n",
        "if scaler is not None and deployed_model is not None:\n",
        "    daily_prediction = daily_data_pipeline(ticker, today_date, deployed_model, scaler)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "sayDgErLwNo_",
        "outputId": "58bb0a8c-0a0a-4f65-882d-33d4bc6c0175"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The `{arg_name}` of this `Lambda` layer is a Python lambda. Deserializing it is unsafe. If you trust the source of the config artifact, you can override this error by passing `safe_mode=False` to `from_config()`, or calling `keras.config.enable_unsafe_deserialization().",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6f52c36cd3da>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/drive/MyDrive/StockModels/Ensemble/best_ensemble_model.keras\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mdeployed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model file not found at\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_keras_zip\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_keras_dir\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_hf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         return saving_lib.load_model(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    365\u001b[0m             )\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             return _load_model_from_fileobj(\n\u001b[0m\u001b[1;32m    368\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_load_model_from_fileobj\u001b[0;34m(fileobj, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0mconfig_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         model = _model_from_config(\n\u001b[0m\u001b[1;32m    445\u001b[0m             \u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py\u001b[0m in \u001b[0;36m_model_from_config\u001b[0;34m(config_json, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;31m# Construct the model from the configuration file in the archive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mObjectSharingScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         model = deserialize_keras_object(\n\u001b[0m\u001b[1;32m    434\u001b[0m             \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msafe_mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcustom_obj_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_from_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m             return functional_from_config(\n\u001b[0m\u001b[1;32m    583\u001b[0m                 \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunctional_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"layers\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[1;32m    522\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m             layer = serialization_lib.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    524\u001b[0m                 \u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcustom_obj_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m             \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/lambda_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects, safe_mode)\u001b[0m\n\u001b[1;32m    188\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mfn_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"class_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__lambda__\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         ):\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_for_lambda_deserialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"function\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minner_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             fn = python_utils.func_load(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/lambda_layer.py\u001b[0m in \u001b[0;36m_raise_for_lambda_deserialization\u001b[0;34m(arg_name, safe_mode)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_raise_for_lambda_deserialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msafe_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0;34m\"The `{arg_name}` of this `Lambda` layer is a Python lambda. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0;34m\"Deserializing it is unsafe. If you trust the source of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The `{arg_name}` of this `Lambda` layer is a Python lambda. Deserializing it is unsafe. If you trust the source of the config artifact, you can override this error by passing `safe_mode=False` to `from_config()`, or calling `keras.config.enable_unsafe_deserialization()."
          ]
        }
      ]
    }
  ]
}