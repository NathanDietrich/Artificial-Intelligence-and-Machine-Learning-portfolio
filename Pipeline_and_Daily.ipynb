{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNvwg/XICC3r0vzhIxcDYdS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NathanDietrich/Artificial-Intelligence-and-Machine-Learning-portfolio/blob/main/Pipeline_and_Daily.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yfinance textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjtM3wYeuP2B",
        "outputId": "0a503d90-3734-417e-c238-aa1892b2a958"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.54)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (1.26.4)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2025.1)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.12.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3pUTpBPuX94",
        "outputId": "fc8691ca-e542-4391-8294-a50ec14a9609"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.4.7-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Collecting kt-legacy (from keras-tuner)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n",
            "Downloading keras_tuner-1.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.1/129.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.4.7 kt-legacy-1.0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# ✅ Enable GPU & Force TensorFlow to Use It\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "        print(f\"✅ GPU detected: {gpu_devices[0].name} (Memory Growth Enabled)\")\n",
        "    except:\n",
        "        print(\"⚠️ GPU found, but could not enable memory growth.\")\n",
        "else:\n",
        "    print(\"❌ No GPU detected. Running on CPU.\")\n",
        "\n",
        "# ✅ Enable Mixed Precision for Faster Training (Uses float16 on GPU)\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"✅ Mixed Precision Enabled (float16) for Faster GPU Training\")\n",
        "\n",
        "# ✅ Check GPU Usage Before Training\n",
        "!nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n",
        "\n",
        "# ✅ Function to Monitor GPU Usage Live\n",
        "def monitor_gpu():\n",
        "    print(\"\\n🔍 Checking GPU Usage...\")\n",
        "    os.system(\"nvidia-smi --query-gpu=memory.used,memory.total --format=csv\")\n",
        "\n",
        "monitor_gpu()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snHPji8yueOp",
        "outputId": "cffa4767-3d66-457b-a251-9e11e2ee9f8e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GPU detected: /physical_device:GPU:0 (Memory Growth Enabled)\n",
            "✅ Mixed Precision Enabled (float16) for Faster GPU Training\n",
            "memory.used [MiB], memory.total [MiB]\n",
            "2 MiB, 15360 MiB\n",
            "\n",
            "🔍 Checking GPU Usage...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94JfqCFRuKKU",
        "outputId": "9f4c30cb-ec5e-4308-9384-ff3b154f3967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "================== Processing AAPL ==================\n",
            "📊 Fetching stock data for AAPL from 2021-01-01 to 2025-03-16...\n",
            "📰 Fetching sentiment data for AAPL from 2021-01-01 to 2025-03-16...\n",
            "📡 Fetching sentiment data for AAPL from 2021-01-01 to 2021-01-31...\n",
            "📡 Fetching sentiment data for AAPL from 2021-01-31 to 2021-03-02...\n",
            "📡 Fetching sentiment data for AAPL from 2021-03-02 to 2021-04-01...\n",
            "📡 Fetching sentiment data for AAPL from 2021-04-01 to 2021-05-01...\n",
            "📡 Fetching sentiment data for AAPL from 2021-05-01 to 2021-05-31...\n",
            "📡 Fetching sentiment data for AAPL from 2021-05-31 to 2021-06-30...\n",
            "📡 Fetching sentiment data for AAPL from 2021-06-30 to 2021-07-30...\n",
            "📡 Fetching sentiment data for AAPL from 2021-07-30 to 2021-08-29...\n",
            "📡 Fetching sentiment data for AAPL from 2021-08-29 to 2021-09-28...\n",
            "📡 Fetching sentiment data for AAPL from 2021-09-28 to 2021-10-28...\n",
            "📡 Fetching sentiment data for AAPL from 2021-10-28 to 2021-11-27...\n",
            "📡 Fetching sentiment data for AAPL from 2021-11-27 to 2021-12-27...\n",
            "📡 Fetching sentiment data for AAPL from 2021-12-27 to 2022-01-26...\n",
            "📡 Fetching sentiment data for AAPL from 2022-01-26 to 2022-02-25...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import time\n",
        "from textblob import TextBlob\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "userdata.get('Polygon_Key')\n",
        "\n",
        "# Mount Google Drive for saving raw data\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Provided functions for stock & sentiment data collection ---\n",
        "def fetch_stock_data_polygon(ticker, start_date, end_date, api_key):\n",
        "    \"\"\"\n",
        "    Fetches historical stock data from Polygon.io.\n",
        "    \"\"\"\n",
        "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{start_date}/{end_date}?apiKey={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error fetching stock data for {ticker}: {response.text}\")\n",
        "        return None\n",
        "    data = response.json()\n",
        "    if \"results\" not in data:\n",
        "        print(f\"No results found for {ticker}.\")\n",
        "        return None\n",
        "    df = pd.DataFrame(data[\"results\"])\n",
        "    df[\"Date\"] = pd.to_datetime(df[\"t\"], unit=\"ms\").dt.date\n",
        "    df.rename(columns={\"o\": \"Open\", \"h\": \"High\", \"l\": \"Low\", \"c\": \"Close\", \"v\": \"Volume\"}, inplace=True)\n",
        "    df = df[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "    return df\n",
        "\n",
        "def fetch_sentiment_data_polygon(ticker, start_date, end_date, api_key, limit=1000):\n",
        "    \"\"\"\n",
        "    Fetches sentiment data from Polygon.io in chunks and computes daily sentiment scores.\n",
        "    \"\"\"\n",
        "    url = \"https://api.polygon.io/v2/reference/news\"\n",
        "    all_results = []\n",
        "    current_start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    final_end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "    while current_start_date < final_end_date:\n",
        "        chunk_end_date = current_start_date + datetime.timedelta(days=30)\n",
        "        if chunk_end_date > final_end_date:\n",
        "            chunk_end_date = final_end_date\n",
        "        chunk_start_str = current_start_date.strftime(\"%Y-%m-%d\")\n",
        "        chunk_end_str = chunk_end_date.strftime(\"%Y-%m-%d\")\n",
        "        print(f\"📡 Fetching sentiment data for {ticker} from {chunk_start_str} to {chunk_end_str}...\")\n",
        "        params = {\n",
        "            \"ticker\": ticker,\n",
        "            \"published_utc.gte\": chunk_start_str,\n",
        "            \"published_utc.lte\": chunk_end_str,\n",
        "            \"apiKey\": api_key,\n",
        "            \"limit\": limit\n",
        "        }\n",
        "        while True:\n",
        "            response = requests.get(url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                results = data.get(\"results\", [])\n",
        "                all_results.extend(results)\n",
        "                next_cursor = data.get(\"next_cursor\")\n",
        "                if not next_cursor:\n",
        "                    break\n",
        "                params[\"cursor\"] = next_cursor\n",
        "            else:\n",
        "                print(f\"⚠️ Error fetching sentiment data for {ticker}: {response.status_code}, {response.text}\")\n",
        "                break\n",
        "        current_start_date = chunk_end_date\n",
        "        time.sleep(14)  # Avoid hitting API rate limits\n",
        "    return all_results\n",
        "\n",
        "def analyze_sentiment(news_data):\n",
        "    \"\"\"\n",
        "    Uses TextBlob to compute sentiment polarity and subjectivity for each news article.\n",
        "    \"\"\"\n",
        "    analyzed_data = []\n",
        "    for article in news_data:\n",
        "        title = article.get(\"title\", \"\")\n",
        "        description = article.get(\"description\", \"\")\n",
        "        full_text = f\"{title} {description}\"\n",
        "        sentiment = TextBlob(full_text).sentiment\n",
        "        analyzed_data.append({\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "            \"published_date\": article.get(\"published_utc\", \"\"),\n",
        "            \"sentiment_polarity\": sentiment.polarity,\n",
        "            \"sentiment_subjectivity\": sentiment.subjectivity\n",
        "        })\n",
        "    return analyzed_data\n",
        "\n",
        "def merge_stock_and_sentiment(stock_df, sentiment_data):\n",
        "    \"\"\"\n",
        "    Merges stock data with sentiment data by date.\n",
        "    \"\"\"\n",
        "    sentiment_df = pd.DataFrame(sentiment_data)\n",
        "    if sentiment_df.empty:\n",
        "        print(\"⚠️ No sentiment data found, proceeding without sentiment.\")\n",
        "        stock_df[\"sentiment_polarity\"] = 0  # Default neutral\n",
        "        stock_df[\"sentiment_subjectivity\"] = 0\n",
        "        return stock_df\n",
        "    sentiment_df['published_date'] = pd.to_datetime(sentiment_df['published_date'], errors='coerce')\n",
        "    sentiment_df['Date'] = sentiment_df['published_date'].dt.date\n",
        "    daily_sentiment = sentiment_df.groupby('Date').agg({\n",
        "        'sentiment_polarity': 'mean',\n",
        "        'sentiment_subjectivity': 'mean'\n",
        "    }).reset_index()\n",
        "    merged_df = pd.merge(stock_df, daily_sentiment, on=\"Date\", how=\"left\")\n",
        "    merged_df[['sentiment_polarity', 'sentiment_subjectivity']] = (\n",
        "        merged_df[['sentiment_polarity', 'sentiment_subjectivity']]\n",
        "        .replace(0, pd.NA)\n",
        "        .ffill()\n",
        "    )\n",
        "    merged_df.fillna(0, inplace=True)\n",
        "    return merged_df\n",
        "\n",
        "# --- Main raw data collection for selected tickers ---\n",
        "def collect_raw_data():\n",
        "    # Set fixed start/end dates (adjust as needed)\n",
        "    start_date = \"2021-01-01\"\n",
        "    end_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "    # List of tickers for stock + sentiment collection\n",
        "    tickers = [\"AAPL\", \"AMZN\", \"MSFT\", \"SPY\", \"QQQ\"]\n",
        "\n",
        "    # Load your Polygon API key (assume stored in Google Colab user data or environment)\n",
        "    # For example, you can store it in a file or use environment variables.\n",
        "    # Here, we simulate fetching it:\n",
        "    api_key = userdata.get(\"Polygon_Key\")\n",
        "    if api_key == \"YOUR_POLYGON_API_KEY\":\n",
        "        print(\"Please set your Polygon API key in the environment variable POLYGON_API_KEY\")\n",
        "        return\n",
        "\n",
        "    for ticker in tickers:\n",
        "        print(f\"\\n================== Processing {ticker} ==================\")\n",
        "        print(f\"📊 Fetching stock data for {ticker} from {start_date} to {end_date}...\")\n",
        "        stock_df = fetch_stock_data_polygon(ticker, start_date, end_date, api_key)\n",
        "        if stock_df is None:\n",
        "            print(f\"❌ No stock data found for {ticker}. Skipping.\")\n",
        "            continue\n",
        "        print(f\"📰 Fetching sentiment data for {ticker} from {start_date} to {end_date}...\")\n",
        "        news_data = fetch_sentiment_data_polygon(ticker, start_date, end_date, api_key, limit=1000)\n",
        "        if not news_data:\n",
        "            print(f\"⚠️ No news data found for {ticker}. Proceeding without sentiment data.\")\n",
        "        print(\"💡 Performing sentiment analysis...\")\n",
        "        sentiment_data = analyze_sentiment(news_data)\n",
        "        print(\"🔗 Merging stock and sentiment data...\")\n",
        "        merged_df = merge_stock_and_sentiment(stock_df, sentiment_data)\n",
        "        # Save raw merged data to Google Drive\n",
        "        save_dir = \"/content/drive/MyDrive/StockData\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        filename = os.path.join(save_dir, f\"{ticker}_{start_date}_to_{end_date}_raw.csv\")\n",
        "        merged_df.to_csv(filename, index=False)\n",
        "        print(f\"✅ Raw data for {ticker} saved to: {filename}\")\n",
        "\n",
        "# Run raw data collection\n",
        "collect_raw_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def calculate_technical_indicators(df):\n",
        "    \"\"\"\n",
        "    Adds common technical indicators to the dataframe:\n",
        "      - SMA (10 & 20 days)\n",
        "      - EMA (10 & 20 days)\n",
        "      - RSI (14-day)\n",
        "      - MACD and MACD Signal\n",
        "    Assumes df has a 'Close' column.\n",
        "    \"\"\"\n",
        "    # Simple Moving Averages\n",
        "    df['SMA_10'] = df['Close'].rolling(window=10).mean()\n",
        "    df['SMA_20'] = df['Close'].rolling(window=20).mean()\n",
        "\n",
        "    # Exponential Moving Averages\n",
        "    df['EMA_10'] = df['Close'].ewm(span=10, adjust=False).mean()\n",
        "    df['EMA_20'] = df['Close'].ewm(span=20, adjust=False).mean()\n",
        "\n",
        "    # Relative Strength Index (RSI)\n",
        "    delta = df['Close'].diff()\n",
        "    gain = delta.clip(lower=0)\n",
        "    loss = -delta.clip(upper=0)\n",
        "    avg_gain = gain.rolling(window=14, min_periods=14).mean()\n",
        "    avg_loss = loss.rolling(window=14, min_periods=14).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    df['RSI'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "    # MACD and MACD Signal\n",
        "    ema12 = df['Close'].ewm(span=12, adjust=False).mean()\n",
        "    ema26 = df['Close'].ewm(span=26, adjust=False).mean()\n",
        "    df['MACD'] = ema12 - ema26\n",
        "    df['MACD_Signal'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
        "\n",
        "    # Forward-fill any indicator missing values\n",
        "    df.fillna(method='ffill', inplace=True)\n",
        "    return df\n",
        "\n",
        "def preprocess_and_save(ticker, raw_filepath):\n",
        "    \"\"\"\n",
        "    Loads raw CSV data, calculates technical indicators, and saves the processed file.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(raw_filepath, parse_dates=[\"Date\"])\n",
        "    df = calculate_technical_indicators(df)\n",
        "\n",
        "    # Save processed data to a new file\n",
        "    processed_filepath = raw_filepath.replace(\"_raw.csv\", \"_processed.csv\")\n",
        "    df.to_csv(processed_filepath, index=False)\n",
        "    print(f\"✅ Processed data with indicators for {ticker} saved to: {processed_filepath}\")\n",
        "    return processed_filepath\n",
        "\n",
        "# Example: Process each ticker's raw data file in the Google Drive folder\n",
        "import glob, os\n",
        "raw_files = glob.glob(\"/content/drive/MyDrive/StockData/*_raw.csv\")\n",
        "for file in raw_files:\n",
        "    ticker = os.path.basename(file).split(\"_\")[0]\n",
        "    preprocess_and_save(ticker, file)\n"
      ],
      "metadata": {
        "id": "r4lgzdgaujo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "def scale_data(df, exclude_cols=['sentiment_polarity', 'sentiment_subjectivity']):\n",
        "    \"\"\"\n",
        "    Scales numeric columns (except those in exclude_cols) using MinMaxScaler.\n",
        "    Returns the scaled dataframe and the scaler object.\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    # Identify numeric columns to scale (exclude date and specified columns)\n",
        "    cols_to_scale = [col for col in df.select_dtypes(include=['float64','int64']).columns\n",
        "                     if col not in exclude_cols]\n",
        "\n",
        "    df_scaled = df.copy()\n",
        "    df_scaled[cols_to_scale] = scaler.fit_transform(df_scaled[cols_to_scale])\n",
        "    return df_scaled, scaler\n",
        "\n",
        "def scale_and_save(ticker, processed_filepath):\n",
        "    \"\"\"\n",
        "    Loads processed data, scales it (saving scaler), and writes the scaled data.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(processed_filepath, parse_dates=[\"Date\"])\n",
        "    df_scaled, scaler = scale_data(df)\n",
        "\n",
        "    # Save the scaler to Google Drive for later use\n",
        "    scaler_dir = \"/content/drive/MyDrive/StockScalers\"\n",
        "    os.makedirs(scaler_dir, exist_ok=True)\n",
        "    scaler_filepath = os.path.join(scaler_dir, f\"{ticker}_scaler.pkl\")\n",
        "    joblib.dump(scaler, scaler_filepath)\n",
        "    print(f\"✅ Scaler for {ticker} saved to: {scaler_filepath}\")\n",
        "\n",
        "    # Save the scaled dataframe\n",
        "    scaled_filepath = processed_filepath.replace(\"_processed.csv\", \"_scaled.csv\")\n",
        "    df_scaled.to_csv(scaled_filepath, index=False)\n",
        "    print(f\"✅ Scaled data for {ticker} saved to: {scaled_filepath}\")\n",
        "    return scaled_filepath\n",
        "\n",
        "# Process scaling for each processed file\n",
        "processed_files = glob.glob(\"/content/drive/MyDrive/StockData/*_processed.csv\")\n",
        "for file in processed_files:\n",
        "    ticker = os.path.basename(file).split(\"_\")[0]\n",
        "    scale_and_save(ticker, file)\n"
      ],
      "metadata": {
        "id": "yAWTGjXcur5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import joblib\n",
        "import keras_tuner as kt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout,\n",
        "                                     SimpleRNN, LSTM, Concatenate, Multiply, Activation, Lambda)\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "# === Helper: Inverse scaling for a single feature ===\n",
        "def inverse_transform_single_feature(scaler, data):\n",
        "    data = np.array(data).reshape(-1, 1)\n",
        "    return scaler.inverse_transform(data)\n",
        "\n",
        "# === Build Ensemble Model Function ===\n",
        "def build_ensemble_model(hp, input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # --- CNN Branch ---\n",
        "    cnn = Conv1D(filters=hp.Choice('cnn_filters', [64, 128, 256]),\n",
        "                 kernel_size=hp.Choice('cnn_kernel_size', [3, 5, 7]),\n",
        "                 activation='relu',\n",
        "                 padding='same')(inputs)\n",
        "    if input_shape[0] > 1:\n",
        "        cnn = MaxPooling1D(pool_size=2)(cnn)\n",
        "    cnn = Flatten()(cnn)\n",
        "    cnn = Dense(50, activation='relu')(cnn)\n",
        "    # --- RNN Branch ---\n",
        "    rnn = SimpleRNN(units=hp.Choice('rnn_units', [75, 100, 125, 150]), return_sequences=True)(inputs)\n",
        "    rnn = Dropout(hp.Choice('dropout_rate', [0.05, 0.1, 0.2]))(rnn)\n",
        "    rnn = SimpleRNN(units=hp.Choice('rnn_units_2', [75, 100, 125, 150]))(rnn)\n",
        "    rnn = Dropout(hp.Choice('dropout_rate_2', [0.05, 0.1, 0.2]))(rnn)\n",
        "    rnn = Dense(50, activation='relu')(rnn)\n",
        "    # --- LSTM Branch ---\n",
        "    lstm = LSTM(units=hp.Choice('lstm_units', [50, 75, 100]), return_sequences=True)(inputs)\n",
        "    lstm = LSTM(units=hp.Choice('lstm_units_2', [50, 75, 100]))(lstm)\n",
        "    lstm = Dense(50, activation='relu')(lstm)\n",
        "    lstm = Dropout(hp.Choice('dropout_rate_lstm', [0.1, 0.2, 0.3]))(lstm)\n",
        "    # --- Adaptive Fusion ---\n",
        "    combined = Concatenate()([cnn, rnn, lstm])\n",
        "    weight_logits = Dense(3)(combined)\n",
        "    branch_weights = Activation('softmax')(weight_logits)\n",
        "    cnn_weight  = Lambda(lambda x: tf.reshape(x[:, 0], (-1, 1)))(branch_weights)\n",
        "    rnn_weight  = Lambda(lambda x: tf.reshape(x[:, 1], (-1, 1)))(branch_weights)\n",
        "    lstm_weight = Lambda(lambda x: tf.reshape(x[:, 2], (-1, 1)))(branch_weights)\n",
        "    cnn_scaled  = Multiply()([cnn, cnn_weight])\n",
        "    rnn_scaled  = Multiply()([rnn, rnn_weight])\n",
        "    lstm_scaled = Multiply()([lstm, lstm_weight])\n",
        "    merged = Concatenate()([cnn_scaled, rnn_scaled, lstm_scaled])\n",
        "    merged = Dense(units=hp.Choice('dense_units', [50, 100, 150]),\n",
        "                   activation=\"relu\",\n",
        "                   kernel_regularizer=l2(0.001))(merged)\n",
        "    merged = Dropout(hp.Choice('dropout_rate_dense', [0.1, 0.2, 0.3]))(merged)\n",
        "    output = Dense(1)(merged)\n",
        "    model = Model(inputs, output)\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=hp.Choice('learning_rate', [0.001, 0.0005, 0.0001])),\n",
        "        loss=\"mse\",\n",
        "        metrics=[\"mae\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# === Assume you have already generated training sequences (X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "# For demonstration, we create dummy data:\n",
        "sequence_length = 60  # e.g., using past 60 days\n",
        "num_features = 10     # Adjust to match your feature set (price, volume, indicators, etc.)\n",
        "X_train = np.random.rand(200, sequence_length, num_features)\n",
        "y_train = np.random.rand(200)\n",
        "X_val = np.random.rand(50, sequence_length, num_features)\n",
        "y_val = np.random.rand(50)\n",
        "X_test = np.random.rand(50, sequence_length, num_features)\n",
        "y_test = np.random.rand(50)\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model_folder = \"/content/drive/MyDrive/StockModels/Ensemble\"\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "best_hps_file = os.path.join(model_folder, \"best_hyperparameters.json\")\n",
        "tuning_flag_file = os.path.join(model_folder, \"hp_tuning_complete.flag\")\n",
        "\n",
        "# === Hyperparameter Tuning ===\n",
        "if not os.path.exists(tuning_flag_file):\n",
        "    if os.path.exists(best_hps_file):\n",
        "        os.remove(best_hps_file)\n",
        "    print(\"🔍 Hyperparameter tuning...\")\n",
        "    tuner = kt.BayesianOptimization(\n",
        "        lambda hp: build_ensemble_model(hp, input_shape),\n",
        "        objective=\"val_loss\",\n",
        "        max_trials=15,\n",
        "        executions_per_trial=2,\n",
        "        directory=os.path.join(model_folder, \"tuning\"),\n",
        "        project_name=\"ensemble_stock_prediction\"\n",
        "    )\n",
        "    tuner.search(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=1)\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    best_hps_dict = {param: best_hps.get(param) for param in best_hps.values.keys()}\n",
        "    with open(best_hps_file, \"w\") as f:\n",
        "        json.dump(best_hps_dict, f)\n",
        "    with open(tuning_flag_file, \"w\") as f:\n",
        "        f.write(\"tuning complete\")\n",
        "    model = tuner.hypermodel.build(best_hps)\n",
        "else:\n",
        "    print(\"✅ Loading best hyperparameters from file...\")\n",
        "    with open(best_hps_file, \"r\") as f:\n",
        "        best_hps_dict = json.load(f)\n",
        "    best_hps = kt.HyperParameters()\n",
        "    for key, value in best_hps_dict.items():\n",
        "        best_hps.Fixed(key, value)\n",
        "    model = build_ensemble_model(best_hps, input_shape)\n",
        "print(\"✅ Best hyperparameters:\", best_hps_dict)\n",
        "\n",
        "# === Model Training ===\n",
        "BATCH_SIZE = 32\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=500,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[early_stop],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "best_model_path = os.path.join(model_folder, \"best_ensemble_model.keras\")\n",
        "model.save(best_model_path)\n",
        "print(\"✅ Best Ensemble Model saved to\", best_model_path)\n",
        "\n",
        "# Plot training history\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.legend()\n",
        "history_plot_path = os.path.join(model_folder, \"training_history.png\")\n",
        "plt.savefig(history_plot_path)\n",
        "plt.close()\n",
        "print(\"✅ Training history graph saved to\", history_plot_path)\n",
        "\n",
        "# === Evaluate Model & Directional Accuracy ===\n",
        "loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"✅ Test Loss: {loss:.4f}, Test MAE: {mae:.4f}\")\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "# For inverse scaling, load scaler for target variable (assume saved separately)\n",
        "# For demonstration, we simulate a scaler (in practice, use joblib.load(scaler_y_path))\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler_y = MinMaxScaler()\n",
        "scaler_y.fit(y_test.reshape(-1, 1))  # dummy fit\n",
        "pred_rescaled = inverse_transform_single_feature(scaler_y, predictions)\n",
        "y_test_rescaled = inverse_transform_single_feature(scaler_y, y_test)\n",
        "\n",
        "# Calculate directional accuracy\n",
        "correct_direction = 0\n",
        "for i in range(len(y_test_rescaled)-1):\n",
        "    actual_diff = y_test_rescaled[i+1] - y_test_rescaled[i]\n",
        "    pred_diff = pred_rescaled[i+1] - pred_rescaled[i]\n",
        "    if (actual_diff * pred_diff) >= 0:\n",
        "        correct_direction += 1\n",
        "directional_accuracy = (correct_direction / (len(y_test_rescaled) - 1)) * 100\n",
        "print(f\"✅ Directional Accuracy: {directional_accuracy:.2f}%\")\n",
        "\n",
        "# Plot Actual vs Predicted with directional coloring\n",
        "x_vals = np.arange(len(y_test_rescaled))\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(x_vals, y_test_rescaled, label=\"Actual Price\", color=\"blue\")\n",
        "for i in range(len(x_vals)-1):\n",
        "    actual_diff = y_test_rescaled[i+1] - y_test_rescaled[i]\n",
        "    pred_diff = pred_rescaled[i+1] - pred_rescaled[i]\n",
        "    color = 'green' if (actual_diff * pred_diff) >= 0 else 'red'\n",
        "    plt.plot(x_vals[i:i+2], pred_rescaled[i:i+2], color=color)\n",
        "blue_line = Line2D([0], [0], color='blue', label='Actual Price')\n",
        "green_line = Line2D([0], [0], color='green', label='Predicted (Correct Dir)')\n",
        "red_line = Line2D([0], [0], color='red', label='Predicted (Wrong Dir)')\n",
        "plt.legend(handles=[blue_line, green_line, red_line])\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Stock Price\")\n",
        "plt.title(\"Predicted vs Actual Prices\")\n",
        "pred_plot_path = os.path.join(model_folder, \"pred_vs_actual.png\")\n",
        "plt.savefig(pred_plot_path)\n",
        "plt.close()\n",
        "print(\"✅ Prediction vs Actual plot saved to\", pred_plot_path)\n"
      ],
      "metadata": {
        "id": "-tCgbPkOwDsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import datetime\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import requests\n",
        "import time\n",
        "\n",
        "def daily_data_pipeline(ticker, date, model, scaler, sequence_length=60):\n",
        "    \"\"\"\n",
        "    Pipeline to:\n",
        "      1. Fetch the latest daily stock data (with sentiment and technical indicators).\n",
        "      2. Preprocess and calculate technical indicators.\n",
        "      3. Scale using the saved scaler.\n",
        "      4. Build a rolling window sequence and predict.\n",
        "    \"\"\"\n",
        "    # Set your Polygon API key from environment variable\n",
        "    api_key = os.environ.get(\"POLYGON_API_KEY\", \"YOUR_POLYGON_API_KEY\")\n",
        "    if api_key == \"YOUR_POLYGON_API_KEY\":\n",
        "        print(\"Please set your Polygon API key in the environment variable POLYGON_API_KEY\")\n",
        "        return None\n",
        "\n",
        "    # Fetch stock data for the day (using the raw data function)\n",
        "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/1/day/{date}/{date}?apiKey={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error fetching data for {ticker} on {date}: {response.text}\")\n",
        "        return None\n",
        "    data = response.json()\n",
        "    if \"results\" not in data:\n",
        "        print(f\"No stock data available for {ticker} on {date}.\")\n",
        "        return None\n",
        "    df_stock = pd.DataFrame(data[\"results\"])\n",
        "    df_stock[\"Date\"] = pd.to_datetime(df_stock[\"t\"], unit=\"ms\").dt.date\n",
        "    df_stock.rename(columns={\"o\": \"Open\", \"h\": \"High\", \"l\": \"Low\", \"c\": \"Close\", \"v\": \"Volume\"}, inplace=True)\n",
        "    df_stock = df_stock[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
        "\n",
        "    # (Optionally) Fetch and process sentiment data for the day...\n",
        "    # For simplicity, assume neutral sentiment if not available.\n",
        "    df_stock[\"sentiment_polarity\"] = 0\n",
        "    df_stock[\"sentiment_subjectivity\"] = 0\n",
        "\n",
        "    # Calculate technical indicators\n",
        "    df_stock = calculate_technical_indicators(df_stock)\n",
        "\n",
        "    # In a production setting, you would build your sequence using the previous (historical) data.\n",
        "    # Here we simulate a rolling window by reading a stored scaled CSV file and appending the new day.\n",
        "    historical_filepath = f\"/content/drive/MyDrive/StockData/{ticker}_processed.csv\"\n",
        "    if not os.path.exists(historical_filepath):\n",
        "        print(\"Historical data file not found for\", ticker)\n",
        "        return None\n",
        "    df_hist = pd.read_csv(historical_filepath, parse_dates=[\"Date\"])\n",
        "    df_hist = calculate_technical_indicators(df_hist)\n",
        "\n",
        "    # Append today's data and then scale using the saved scaler\n",
        "    df_combined = pd.concat([df_hist, df_stock], ignore_index=True)\n",
        "    df_scaled = df_combined.copy()\n",
        "    # Identify columns to scale (same as in scaling function)\n",
        "    numeric_cols = [col for col in df_scaled.select_dtypes(include=['float64','int64']).columns\n",
        "                    if col not in ['sentiment_polarity', 'sentiment_subjectivity']]\n",
        "    df_scaled[numeric_cols] = scaler.transform(df_scaled[numeric_cols])\n",
        "\n",
        "    # Build sequence: take the last 'sequence_length' rows for features.\n",
        "    try:\n",
        "        # Define the feature set for prediction (adjust column names as needed)\n",
        "        feature_cols = ['Open', 'High', 'Low', 'Close', 'Volume',\n",
        "                        'SMA_10', 'SMA_20', 'EMA_10', 'EMA_20', 'RSI']\n",
        "        seq = df_scaled[feature_cols].tail(sequence_length).values\n",
        "    except Exception as e:\n",
        "        print(\"Error building feature sequence:\", e)\n",
        "        return None\n",
        "\n",
        "    if seq.shape[0] < sequence_length:\n",
        "        pad = np.tile(seq[0], (sequence_length - seq.shape[0], 1))\n",
        "        seq = np.vstack([pad, seq])\n",
        "    seq = seq.reshape(1, sequence_length, len(feature_cols))\n",
        "\n",
        "    prediction = model.predict(seq)\n",
        "    print(f\"Predicted price for {ticker} on {date}: {prediction[0][0]}\")\n",
        "    return prediction[0][0]\n",
        "\n",
        "# Example usage for daily deployment:\n",
        "# Load your trained model and the saved scaler (for the target features, if different, adjust accordingly)\n",
        "# Assume best_model has been loaded and scaler was saved in \"/content/drive/MyDrive/StockScalers/{ticker}_scaler.pkl\"\n",
        "ticker = \"AAPL\"\n",
        "today_date = datetime.date.today().strftime(\"%Y-%m-%d\")\n",
        "scaler_path = f\"/content/drive/MyDrive/StockScalers/{ticker}_scaler.pkl\"\n",
        "if os.path.exists(scaler_path):\n",
        "    scaler = joblib.load(scaler_path)\n",
        "else:\n",
        "    print(\"Scaler file not found for\", ticker)\n",
        "    scaler = None\n",
        "\n",
        "# Load the model (or use the one from training)\n",
        "from tensorflow.keras.models import load_model\n",
        "model_path = f\"/content/drive/MyDrive/StockModels/Ensemble/best_ensemble_model.keras\"\n",
        "if os.path.exists(model_path):\n",
        "    deployed_model = load_model(model_path)\n",
        "else:\n",
        "    print(\"Model file not found at\", model_path)\n",
        "    deployed_model = None\n",
        "\n",
        "if scaler is not None and deployed_model is not None:\n",
        "    daily_prediction = daily_data_pipeline(ticker, today_date, deployed_model, scaler)\n"
      ],
      "metadata": {
        "id": "sayDgErLwNo_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}