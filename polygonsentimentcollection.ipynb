{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPlITcU+cN4qqeXMmLm3FBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NathanDietrich/Artificial-Intelligence-and-Machine-Learning-portfolio/blob/main/polygonsentimentcollection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "8dTBgspeymu_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8ZaO9OfvdyU",
        "outputId": "bc0d575e-6ae5-469c-c026-70d3aac2b33d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment analysis completed. Results saved to TSLA_news_sentiment_2021-02-02_to_2025-01-15.csv\n"
          ]
        }
      ],
      "source": [
        "!pip install ta\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ta.volatility import AverageTrueRange, BollingerBands, DonchianChannel, KeltnerChannel\n",
        "from ta.momentum import WilliamsRIndicator\n",
        "#define start and end dates to line up with sentiment collection\n",
        "start_date = \"2022-09-29\"\n",
        "end_date = \"2025-01-08\"\n",
        "# Fetch the historical data\n",
        "tsla = yf.Ticker(\"TSLA\")\n",
        "data = tsla.history(start = start_date, end = end_date)\n",
        "data = data[['Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "# Calculate Technical Indicators\n",
        "# Average True Range (ATR)\n",
        "data['ATR'] = AverageTrueRange(high=data['High'], low=data['Low'], close=data['Close']).average_true_range()\n",
        "\n",
        "#bollinger Bands\n",
        "bb = BollingerBands(close=data['Close'])\n",
        "data['BB_High'] = bb.bollinger_hband()\n",
        "data['BB_low'] = bb.bollinger_lband()\n",
        "\n",
        "# Donichian Channel\n",
        "dc = DonchianChannel(high=data['High'], low=data['Low'], close=data['Close'])\n",
        "data['DC_High'] = dc.donchian_channel_hband()\n",
        "data['DC_low'] = dc.donchian_channel_lband()\n",
        "\n",
        "# Keltner Channel\n",
        "kc = KeltnerChannel(high=data['High'], low=data['Low'], close=data['Close'])\n",
        "data['KC_High'] = kc.keltner_channel_hband()\n",
        "data['KC_Low'] = kc.keltner_channel_lband()\n",
        "\n",
        "#Chaikin Volatility\n",
        "high_low_range = data['High'] - data['Low']\n",
        "data['Chaikin_volatility'] = high_low_range.rolling(window=10).mean() / high_low_range.rolling(window=10).std()\n",
        "\n",
        "#Historical Volatility\n",
        "log_returns = np.log(data['Close'] / data['Close'].shift(1))\n",
        "data['Historical_volatility'] = log_returns.rolling(window=30).std() * np.sqrt(252)\n",
        "\n",
        "# Standard Deviation\n",
        "data['Standard_Deviation'] = data['Close']. rolling(window=14).std()\n",
        "\n",
        "# Williams %R\n",
        "wr = WilliamsRIndicator(high=data['High'], low=data['Low'], close=data['Close'])\n",
        "data['Williams_%R'] = wr.williams_r()\n",
        "\n",
        "# Commodity Channel Index(CCI)\n",
        "data['CCI'] = (data['Close'] - data['Close'].rolling(20).mean()) / (0.015 * data['Close'].rolling(20).std())\n",
        "\n",
        "# RSI-Based Volatility\n",
        "rsi_diff = data['Close'].rolling(window=14).apply(lambda x: max(x) - min(x))\n",
        "data['RSI_Based_Volatility'] = rsi_diff / data['Close']\n",
        "\n",
        "#Ulcer Index\n",
        "data['Ulcer_Index'] = ((data['Close'] - data['Close'].rolling(window=14).max()) ** 2).rolling(window=14).mean()\n",
        "\n",
        "# True Strength Index (TSI)\n",
        "data['TSI'] = (log_returns.ewm(span=25).mean() / log_returns.abs().ewm(span=13).mean()) * 100\n",
        "\n",
        "# Fractal Chaos Oscillator\n",
        "data['Fractal_chaos_Oscillator'] = data['Close'].rolling(window=14).apply(lambda x: np.ptp(x))\n",
        "\n",
        "# Drop NAN values\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "#save data to a csv\n",
        "output_path = f\"tsla_with_technical_indicators_{start_date}_to_{end_date}.csv\"\n",
        "data.to_csv(output_path)\n",
        "\n",
        "print(f\"Data with technical indicators saved to {output_path}\")\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print"
      ],
      "metadata": {
        "id": "TvQ4u8Uee8yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "this program collects news data from polygon api and collects 1000 articles in chunks of 1 month but can only have 5 api requests per minute"
      ],
      "metadata": {
        "id": "BGqwU0rz038W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_historical_news_chunked(ticker, start_date, end_date, api_key, limit=1000):\n",
        "    url = f\"https://api.polygon.io/v2/reference/news\"\n",
        "    all_results = []\n",
        "\n",
        "    current_start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
        "    final_end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
        "\n",
        "    while current_start_date < final_end_date:\n",
        "        # Calculate the chunk end date (1 months from the start date)\n",
        "        chunk_end_date = current_start_date + datetime.timedelta(days=1 * 30)\n",
        "        if chunk_end_date > final_end_date:\n",
        "            chunk_end_date = final_end_date\n",
        "\n",
        "        chunk_start_str = current_start_date.strftime(\"%Y-%m-%d\")\n",
        "        chunk_end_str = chunk_end_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        print(f\"Fetching news from {chunk_start_str} to {chunk_end_str}...\")\n",
        "\n",
        "        params = {\n",
        "            \"ticker\": ticker,\n",
        "            \"published_utc.gte\": chunk_start_str,\n",
        "            \"published_utc.lte\": chunk_end_str,\n",
        "            \"apiKey\": api_key,\n",
        "            \"limit\": limit\n",
        "        }\n",
        "\n",
        "        while True:\n",
        "            response = requests.get(url, params=params)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                results = data.get(\"results\", [])\n",
        "                all_results.extend(results)\n",
        "\n",
        "                # Check for pagination\n",
        "                next_cursor = data.get(\"next_cursor\")\n",
        "                if not next_cursor:\n",
        "                    break\n",
        "\n",
        "                # Update the cursor for the next request\n",
        "                params[\"cursor\"] = next_cursor\n",
        "            else:\n",
        "                print(f\"Error: {response.status_code}, {response.text}\")\n",
        "                break\n",
        "\n",
        "        # Move to the next chunk\n",
        "        current_start_date = chunk_end_date\n",
        "\n",
        "        # Pause once after processing the entire chunk\n",
        "        print(\"Waiting for 1 minute to respect API rate limits...\")\n",
        "        time.sleep(14)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def analyze_sentiment(news_data):\n",
        "    analyzed_data = []\n",
        "\n",
        "    for article in news_data:\n",
        "        title = article.get(\"title\", \"\")\n",
        "        description = article.get(\"description\", \"\")\n",
        "\n",
        "        # Combine title and description\n",
        "        full_text = f\"{title} {description}\"\n",
        "        sentiment = TextBlob(full_text).sentiment\n",
        "\n",
        "        analyzed_data.append({\n",
        "            \"title\": title,\n",
        "            \"description\": description,\n",
        "            \"published_date\": article.get(\"published_utc\", \"\"),\n",
        "            \"sentiment_polarity\": sentiment.polarity,\n",
        "            \"sentiment_subjectivity\": sentiment.subjectivity\n",
        "        })\n",
        "\n",
        "    return analyzed_data\n",
        "\n",
        "def main():\n",
        "    # API config\n",
        "    api_key = userdata.get('Polygon_Key')\n",
        "    ticker = \"TSLA\"\n",
        "    start_date = (datetime.datetime.now() - datetime.timedelta(days=2.5 * 365)).strftime(\"%Y-%m-%d\")\n",
        "    end_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")  # 2.5 years backward from today\n",
        "\n",
        "    # Fetch news in chunks\n",
        "    news_data = get_historical_news_chunked(ticker, start_date, end_date, api_key, limit=1000)\n",
        "\n",
        "    if not news_data:\n",
        "        print(\"No news data found.\")\n",
        "        return\n",
        "\n",
        "    # Perform sentiment analysis\n",
        "    analyzed_news = analyze_sentiment(news_data)\n",
        "\n",
        "    # Convert to DataFrame for better visualization\n",
        "    df = pd.DataFrame(analyzed_news)\n",
        "\n",
        "    # Sort the data by published_date\n",
        "    df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
        "    df = df.sort_values(by='published_date')\n",
        "\n",
        "    # Save to CSV file\n",
        "    csv_file = f\"{ticker}_news_sentiment_{start_date}_to_{end_date}.csv\"\n",
        "    df.to_csv(csv_file, index=False)\n",
        "\n",
        "    print(f\"Sentiment analysis completed. Results saved to {csv_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlDmMRbOzThD",
        "outputId": "851a219e-748b-4801-f0f0-a5f46f1103f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching news from 2022-07-26 to 2022-08-25...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2022-08-25 to 2022-09-24...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2022-09-24 to 2022-10-24...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2022-10-24 to 2022-11-23...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2022-11-23 to 2022-12-23...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2022-12-23 to 2023-01-22...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2023-01-22 to 2023-02-21...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2023-02-21 to 2023-03-23...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2023-03-23 to 2023-04-22...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2023-04-22 to 2023-05-22...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2023-05-22 to 2023-06-21...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2023-06-21 to 2023-07-21...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2023-07-21 to 2023-08-20...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2023-08-20 to 2023-09-19...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2023-09-19 to 2023-10-19...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2023-10-19 to 2023-11-18...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2023-11-18 to 2023-12-18...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2023-12-18 to 2024-01-17...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-01-17 to 2024-02-16...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-02-16 to 2024-03-17...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-03-17 to 2024-04-16...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-04-16 to 2024-05-16...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-05-16 to 2024-06-15...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-06-15 to 2024-07-15...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-07-15 to 2024-08-14...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-08-14 to 2024-09-13...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-09-13 to 2024-10-13...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-10-13 to 2024-11-12...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-11-12 to 2024-12-12...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2024-12-12 to 2025-01-11...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Fetching news from 2025-01-11 to 2025-01-23...\n",
            "Waiting for 1 minute to respect API rate limits...\n",
            "Sentiment analysis completed. Results saved to TSLA_news_sentiment_2022-07-26_to_2025-01-23.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_news_data(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Orders the dataset by date, removes duplicates, and saves it to a specified file.\n",
        "\n",
        "    Parameters:\n",
        "        input_file (str): Path to the input CSV file.\n",
        "        output_file (str): Path to save the processed CSV file.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Load the dataset\n",
        "    data = pd.read_csv(input_file)\n",
        "\n",
        "    # Convert 'published_date' to datetime for sorting\n",
        "    if 'published_date' in data.columns:\n",
        "        data['published_date'] = pd.to_datetime(data['published_date'])\n",
        "    else:\n",
        "        raise ValueError(\"The dataset must contain a 'published_date' column.\")\n",
        "\n",
        "    # Sort the dataset by 'published_date'\n",
        "    data = data.sort_values('published_date')\n",
        "\n",
        "    # Remove duplicate rows\n",
        "    data = data.drop_duplicates()\n",
        "\n",
        "    # Save the cleaned and sorted dataset to a new CSV file\n",
        "    data.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"Processed dataset saved as: {output_file}\")\n",
        "\n",
        "\n",
        "process_news_data('TSLA_news_sentiment_2022-07-26_to_2025-01-23.csv', 'TSLA_news_sentiment_sorted.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z6X8p264EZY",
        "outputId": "1b49e5f6-47a7-4a4a-b7c1-29665e93fbe2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed dataset saved as: TSLA_news_sentiment_sorted.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def preprocess_for_lstm(input_file, output_file):\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Ensure the published_date is in datetime format\n",
        "    df['published_date'] = pd.to_datetime(df['published_date'], errors='coerce')\n",
        "\n",
        "    # Drop rows with invalid or missing dates\n",
        "    df = df.dropna(subset=['published_date'])\n",
        "\n",
        "    # Sort the data by published_date\n",
        "    df = df.sort_values(by='published_date')\n",
        "\n",
        "    # Keep only the columns needed for LSTM: published_date, sentiment_polarity, sentiment_subjectivity\n",
        "    lstm_data = df[['published_date', 'sentiment_polarity', 'sentiment_subjectivity']]\n",
        "\n",
        "    # Save the preprocessed data to a new CSV file\n",
        "    lstm_data.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"Preprocessed data saved to {output_file}\")\n",
        "\n",
        "# Example usage\n",
        "input_file = 'TSLA_news_sentiment_sorted.csv'  # Replace with your file path\n",
        "output_file = 'TSLA_news_preprocessed_for_lstm.csv'  # Replace with your desired output file path\n",
        "preprocess_for_lstm(input_file, output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BhOuyfX2MgQ",
        "outputId": "69c58809-0262-49bd-b5a9-1431a4a38dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed data saved to TSLA_news_preprocessed_for_lstm.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_sentiment_and_stock(sentiment_file, stock_file, output_file):\n",
        "    # Load the sentiment data\n",
        "    sentiment_df = pd.read_csv(sentiment_file)\n",
        "    sentiment_df['published_date'] = pd.to_datetime(sentiment_df['published_date'], errors='coerce')\n",
        "\n",
        "    # Drop rows with invalid or missing dates\n",
        "    sentiment_df = sentiment_df.dropna(subset=['published_date'])\n",
        "\n",
        "    # Group sentiment data by day and calculate daily averages and article counts\n",
        "    daily_sentiment = sentiment_df.groupby(sentiment_df['published_date'].dt.date).agg({\n",
        "        'sentiment_polarity': 'mean',\n",
        "        'sentiment_subjectivity': 'mean',\n",
        "        'published_date': 'count'  # Count the number of articles per day\n",
        "    }).rename(columns={'published_date': 'article_count'}).reset_index()\n",
        "    daily_sentiment.rename(columns={'published_date': 'date'}, inplace=True)\n",
        "\n",
        "    # Load the stock price data\n",
        "    stock_df = pd.read_csv(stock_file)\n",
        "    stock_df['date'] = pd.to_datetime(stock_df['date'], errors='coerce', utc=True).dt.date\n",
        "\n",
        "    # Drop rows with invalid or missing dates\n",
        "    stock_df = stock_df.dropna(subset=['date'])\n",
        "\n",
        "    # Merge stock data with sentiment data on the 'date' column\n",
        "    combined_df = pd.merge(stock_df, daily_sentiment, on='date', how='left')\n",
        "\n",
        "    # Handle missing sentiment data\n",
        "    combined_df['sentiment_polarity'].fillna(0, inplace=True)  # Fill missing polarity with 0\n",
        "    combined_df['sentiment_subjectivity'].fillna(0, inplace=True)  # Fill missing subjectivity with 0\n",
        "    combined_df['article_count'].fillna(0, inplace=True)  # Fill missing article counts with 0\n",
        "\n",
        "    # Forward-fill stock prices for non-trading days (if required)\n",
        "    combined_df.sort_values(by='date', inplace=True)\n",
        "    combined_df.ffill(inplace=True)\n",
        "\n",
        "    # Dynamically normalize all numerical features, including article_count\n",
        "    numeric_features = combined_df.select_dtypes(include=[np.number]).columns\n",
        "    combined_df[numeric_features] = (combined_df[numeric_features] - combined_df[numeric_features].mean()) / combined_df[numeric_features].std()\n",
        "\n",
        "    # Save the preprocessed data to a new CSV file\n",
        "    combined_df.to_csv(output_file, index=False)\n",
        "    print(f\"Preprocessed data saved to {output_file}\")\n",
        "\n",
        "\n",
        "\n",
        "# Example usage\n",
        "sentiment_file = 'TSLA_news_preprocessed_for_lstm_input.csv'  # Replace with your sentiment data file\n",
        "stock_file = 'tsla_with_technical_indicators2022-09-29_to_2025-01-08.csv'  # Replace with your stock price data file\n",
        "output_file = 'TSLA_combined_preprocessed_fixed1.csv'  # Output file path\n",
        "preprocess_sentiment_and_stock(sentiment_file, stock_file, output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1AMz3avsbVG",
        "outputId": "a7a762c6-11a6-4234-d1dc-d7474b3b055b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed data saved to TSLA_combined_preprocessed_fixed1.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-bed34d5a9c90>:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  combined_df['sentiment_polarity'].fillna(0, inplace=True)  # Fill missing polarity with 0\n",
            "<ipython-input-2-bed34d5a9c90>:32: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  combined_df['sentiment_subjectivity'].fillna(0, inplace=True)  # Fill missing subjectivity with 0\n",
            "<ipython-input-2-bed34d5a9c90>:33: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  combined_df['article_count'].fillna(0, inplace=True)  # Fill missing article counts with 0\n"
          ]
        }
      ]
    }
  ]
}