{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPipTI0Lj3ugjVpm11849GO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NathanDietrich/Artificial-Intelligence-and-Machine-Learning-portfolio/blob/main/SigmoidEnsemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFNUBMzLHw8s",
        "outputId": "5434598b-9293-48ac-b439-513c379d3e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.11/dist-packages (1.4.7)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (2.32.3)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.11/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras->keras-tuner) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner) (2025.1.31)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras->keras-tuner) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras->keras-tuner) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras->keras-tuner) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "# ‚úÖ Enable GPU & Force TensorFlow to Use It\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "if gpu_devices:\n",
        "    try:\n",
        "        tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n",
        "        print(f\"‚úÖ GPU detected: {gpu_devices[0].name} (Memory Growth Enabled)\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è GPU found, but could not enable memory growth.\")\n",
        "else:\n",
        "    print(\"‚ùå No GPU detected. Running on CPU.\")\n",
        "\n",
        "# ‚úÖ Enable Mixed Precision for Faster Training (Uses float16 on GPU)\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"‚úÖ Mixed Precision Enabled (float16) for Faster GPU Training\")\n",
        "\n",
        "# ‚úÖ Check GPU Usage Before Training\n",
        "!nvidia-smi --query-gpu=memory.used,memory.total --format=csv\n",
        "\n",
        "# ‚úÖ Function to Monitor GPU Usage Live\n",
        "def monitor_gpu():\n",
        "    print(\"\\nüîç Checking GPU Usage...\")\n",
        "    os.system(\"nvidia-smi --query-gpu=memory.used,memory.total --format=csv\")\n",
        "\n",
        "monitor_gpu()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKMcRVLwHy7U",
        "outputId": "caaf23f8-0f81-422d-da65-6c2bd5dda113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ GPU detected: /physical_device:GPU:0 (Memory Growth Enabled)\n",
            "‚úÖ Mixed Precision Enabled (float16) for Faster GPU Training\n",
            "memory.used [MiB], memory.total [MiB]\n",
            "2 MiB, 15360 MiB\n",
            "\n",
            "üîç Checking GPU Usage...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vG9JWlCA0Zx7",
        "outputId": "d4ee86f2-9fea-471f-ec0d-593b730b562a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import json\n",
        "import keras_tuner as kt\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, SimpleRNN, LSTM, Dense, Dropout, Concatenate, Multiply, Attention\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# === Select Stock Folder ===\n",
        "base_stocks_folder = '/content/drive/MyDrive/stocks'\n",
        "stock_folders = [f for f in os.listdir(base_stocks_folder) if os.path.isdir(os.path.join(base_stocks_folder, f))]\n",
        "if not stock_folders:\n",
        "    print(\"No stock folders found in\", base_stocks_folder)\n",
        "    raise SystemExit\n",
        "\n",
        "print(\"Available stock tickers:\")\n",
        "for i, stock in enumerate(stock_folders):\n",
        "    print(f\"{i + 1}. {stock}\")\n",
        "\n",
        "choice = int(input(\"Enter the number of the stock ticker to use for ensemble modeling: \")) - 1\n",
        "selected_ticker = stock_folders[choice]\n",
        "stock_path = os.path.join(base_stocks_folder, selected_ticker)\n",
        "print(f\"Selected ticker: {selected_ticker}\")\n",
        "\n",
        "# === Define Ensemble Save Folder (inside stock folder) with Versioning ===\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "ensemble_folder = os.path.join(stock_path, f\"EnsembleModel_{timestamp}\")\n",
        "os.makedirs(ensemble_folder, exist_ok=True)\n",
        "\n",
        "# === Load Preprocessed Data from Stock Folder ===\n",
        "X_train = np.load(os.path.join(stock_path, \"X_train.npy\"))\n",
        "y_train = np.load(os.path.join(stock_path, \"y_train.npy\"))\n",
        "X_val   = np.load(os.path.join(stock_path, \"X_val.npy\"))\n",
        "y_val   = np.load(os.path.join(stock_path, \"y_val.npy\"))\n",
        "X_test  = np.load(os.path.join(stock_path, \"X_test.npy\"))\n",
        "y_test  = np.load(os.path.join(stock_path, \"y_test.npy\"))\n",
        "\n",
        "# Load scaler for target variable (for inverse scaling later)\n",
        "scaler_y = joblib.load(os.path.join(stock_path, \"scaler_y.pkl\"))\n",
        "\n",
        "print(f\"‚úÖ Data Loaded: X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"‚úÖ X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "print(f\"‚úÖ X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "# === Reshape Data if Needed ===\n",
        "# If X data is 2D (samples, features), reshape to (samples, timesteps, features)\n",
        "if X_train.ndim == 2:\n",
        "    X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "    X_val   = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
        "    X_test  = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "print(f\"‚úÖ Input shape for model: {input_shape}\")\n",
        "\n",
        "# === Hyperparameter Tuning / Load Best Hyperparameters ===\n",
        "best_hps_path = os.path.join(ensemble_folder, \"best_hyperparameters.json\")\n",
        "if os.path.exists(best_hps_path):\n",
        "    print(\"‚úÖ Loading best hyperparameters from file.\")\n",
        "    with open(best_hps_path, \"r\") as f:\n",
        "        best_hps_dict = json.load(f)\n",
        "    best_hps = kt.HyperParameters()\n",
        "    for key, value in best_hps_dict.items():\n",
        "        best_hps.Fixed(key, value)\n",
        "else:\n",
        "    print(\"üîç No best hyperparameters found. Running hyperparameter tuning...\")\n",
        "\n",
        "    def build_ensemble_model(hp):\n",
        "        inputs = Input(shape=input_shape)\n",
        "\n",
        "        # Determine kernel size options based on timesteps\n",
        "        if input_shape[0] == 1:\n",
        "            kernel_size_options = [1]\n",
        "            apply_pooling = False\n",
        "        else:\n",
        "            kernel_size_options = [3, 5, 7]\n",
        "            apply_pooling = True\n",
        "\n",
        "        # === CNN Branch ===\n",
        "        cnn = Conv1D(filters=hp.Choice('cnn_filters', [64, 128, 256]),\n",
        "                     kernel_size=hp.Choice('cnn_kernel_size', kernel_size_options),\n",
        "                     activation='relu')(inputs)\n",
        "        if apply_pooling:\n",
        "            cnn = MaxPooling1D(pool_size=2)(cnn)\n",
        "        cnn = Flatten()(cnn)\n",
        "        # Sigmoid for a single weight\n",
        "        cnn_weight = Dense(1, activation='sigmoid')(cnn)\n",
        "\n",
        "        # === RNN Branch ===\n",
        "        rnn = SimpleRNN(units=hp.Choice('rnn_units', [75, 100, 125]), return_sequences=True)(inputs)\n",
        "        rnn = SimpleRNN(units=hp.Choice('rnn_units_2', [75, 100, 125]), return_sequences=True)(rnn)\n",
        "        rnn = Attention()([rnn, rnn])\n",
        "        rnn = Flatten()(rnn)\n",
        "        # Sigmoid for a single weight\n",
        "        rnn_weight = Dense(1, activation='sigmoid')(rnn)\n",
        "\n",
        "        # === LSTM Branch ===\n",
        "        lstm = LSTM(units=hp.Choice('lstm_units', [50, 75, 100]), return_sequences=True)(inputs)\n",
        "        lstm = LSTM(units=hp.Choice('lstm_units_2', [50, 75, 100]), return_sequences=True)(lstm)\n",
        "        lstm = Attention()([lstm, lstm])\n",
        "        lstm = Flatten()(lstm)\n",
        "        # Sigmoid for a single weight\n",
        "        lstm_weight = Dense(1, activation='sigmoid')(lstm)\n",
        "\n",
        "        # === Adaptive Weighted Fusion ===\n",
        "        cnn_scaled = Multiply()([cnn, cnn_weight])\n",
        "        rnn_scaled = Multiply()([rnn, rnn_weight])\n",
        "        lstm_scaled = Multiply()([lstm, lstm_weight])\n",
        "\n",
        "        merged = Concatenate()([cnn_scaled, rnn_scaled, lstm_scaled])\n",
        "        merged = Dense(units=hp.Choice('dense_units', [50, 100, 150]), activation=\"relu\")(merged)\n",
        "        merged = Dropout(hp.Choice('dropout_rate', [0.1, 0.2, 0.3]))(merged)\n",
        "\n",
        "        output = Dense(1)(merged)\n",
        "\n",
        "        model = Model(inputs, output)\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=hp.Choice('learning_rate', [0.001, 0.0005, 0.0001])),\n",
        "            loss=\"mse\",\n",
        "            metrics=[\"mae\"]\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    tuner = kt.RandomSearch(\n",
        "        build_ensemble_model,\n",
        "        objective=\"val_loss\",\n",
        "        max_trials=15,\n",
        "        executions_per_trial=3,\n",
        "        # The tuner logs go under the versioned folder's \"tuning\" subfolder\n",
        "        directory=os.path.join(ensemble_folder, \"tuning\"),\n",
        "        project_name=\"stock_prediction_ensemble\"\n",
        "    )\n",
        "\n",
        "    tuner.search(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=1)\n",
        "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "    best_hps_dict = {param: best_hps.get(param) for param in best_hps.values.keys()}\n",
        "    with open(best_hps_path, \"w\") as f:\n",
        "        json.dump(best_hps_dict, f)\n",
        "    print(f\"‚úÖ Best hyperparameters saved to {best_hps_path}\")\n",
        "\n",
        "print(\"‚úÖ Best hyperparameters:\")\n",
        "print(best_hps_dict)\n",
        "\n",
        "# === Build and Train the Best Model ===\n",
        "BATCH_SIZE = 32\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5, verbose=1)\n",
        "\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "history = best_model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=500,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=[reduce_lr],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# === Save the Best Model and Training History ===\n",
        "best_model_path = os.path.join(ensemble_folder, \"best_ensemble_model.keras\")\n",
        "best_model.save(best_model_path)\n",
        "print(f\"‚úÖ Best Ensemble Model saved to {best_model_path}\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(history.history['loss'], label='Train Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.legend()\n",
        "history_plot_path = os.path.join(ensemble_folder, \"training_history.png\")\n",
        "plt.savefig(history_plot_path)\n",
        "plt.show()\n",
        "print(f\"‚úÖ Training history graph saved to {history_plot_path}\")\n",
        "\n",
        "# === Evaluate the Model ===\n",
        "loss, mae = best_model.evaluate(X_test, y_test)\n",
        "print(f\"‚úÖ Best Model Test Loss: {loss}\")\n",
        "print(f\"‚úÖ Best Model Test MAE: {mae}\")\n",
        "\n",
        "# === Make Predictions and Inverse Scale ===\n",
        "predictions = best_model.predict(X_test)\n",
        "def inverse_transform_single_feature(scaler, data):\n",
        "    data = np.array(data).reshape(-1, 1)\n",
        "    return scaler.inverse_transform(data)\n",
        "predictions_rescaled = inverse_transform_single_feature(scaler_y, predictions)\n",
        "y_test_rescaled = inverse_transform_single_feature(scaler_y, y_test)\n",
        "\n",
        "# === Plot Predicted vs Actual Prices ===\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(y_test_rescaled, label=\"Actual Price\", color=\"blue\")\n",
        "plt.plot(predictions_rescaled, label=\"Predicted Price\", color=\"red\", linestyle=\"dashed\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Stock Price\")\n",
        "plt.title(f\"{selected_ticker} - Predicted vs. Actual Stock Price\")\n",
        "plt.legend()\n",
        "pred_vs_actual_path = os.path.join(ensemble_folder, \"pred_vs_actual.png\")\n",
        "plt.savefig(pred_vs_actual_path)\n",
        "plt.show()\n",
        "print(f\"‚úÖ Prediction vs. Actual plot saved to {pred_vs_actual_path}\")\n",
        "\n",
        "print(f\"\\nüéØ Ensemble Model Training & Prediction Complete for {selected_ticker}! üöÄ\")\n"
      ],
      "metadata": {
        "id": "xNN37GgPaggy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78876b34-8aaa-442b-9282-56782fbe14c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5 Complete [00h 01m 25s]\n",
            "val_loss: 0.0042163765368362265\n",
            "\n",
            "Best val_loss So Far: 0.0013920645384738843\n",
            "Total elapsed time: 00h 06m 52s\n",
            "\n",
            "Search: Running Trial #6\n",
            "\n",
            "Value             |Best Value So Far |Hyperparameter\n",
            "128               |256               |cnn_filters\n",
            "1                 |1                 |cnn_kernel_size\n",
            "75                |100               |rnn_units\n",
            "75                |125               |rnn_units_2\n",
            "75                |50                |lstm_units\n",
            "100               |50                |lstm_units_2\n",
            "150               |150               |dense_units\n",
            "0.2               |0.1               |dropout_rate\n",
            "0.0005            |0.0001            |learning_rate\n",
            "\n",
            "Epoch 1/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 91ms/step - loss: 0.0870 - mae: 0.1972 - val_loss: 0.0173 - val_mae: 0.1007\n",
            "Epoch 2/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0175 - mae: 0.1052 - val_loss: 0.0071 - val_mae: 0.0634\n",
            "Epoch 3/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0101 - mae: 0.0755 - val_loss: 0.0135 - val_mae: 0.1073\n",
            "Epoch 4/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0084 - mae: 0.0686 - val_loss: 0.0029 - val_mae: 0.0425\n",
            "Epoch 5/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0069 - mae: 0.0626 - val_loss: 0.0027 - val_mae: 0.0421\n",
            "Epoch 6/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0044 - mae: 0.0511 - val_loss: 0.0037 - val_mae: 0.0517\n",
            "Epoch 7/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step - loss: 0.0051 - mae: 0.0566 - val_loss: 0.0016 - val_mae: 0.0316\n",
            "Epoch 8/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0046 - mae: 0.0525 - val_loss: 0.0027 - val_mae: 0.0429\n",
            "Epoch 9/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0043 - mae: 0.0517 - val_loss: 0.0031 - val_mae: 0.0431\n",
            "Epoch 10/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 0.0039 - mae: 0.0487 - val_loss: 0.0044 - val_mae: 0.0586\n",
            "Epoch 11/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0046 - mae: 0.0513 - val_loss: 0.0029 - val_mae: 0.0448\n",
            "Epoch 12/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0036 - mae: 0.0463 - val_loss: 0.0024 - val_mae: 0.0395\n",
            "Epoch 13/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0042 - mae: 0.0481 - val_loss: 0.0030 - val_mae: 0.0466\n",
            "Epoch 14/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0028 - mae: 0.0427 - val_loss: 0.0018 - val_mae: 0.0340\n",
            "Epoch 15/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0035 - mae: 0.0452 - val_loss: 0.0017 - val_mae: 0.0327\n",
            "Epoch 16/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.0037 - mae: 0.0472 - val_loss: 0.0016 - val_mae: 0.0312\n",
            "Epoch 17/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0046 - mae: 0.0505 - val_loss: 0.0033 - val_mae: 0.0492\n",
            "Epoch 18/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0033 - mae: 0.0445 - val_loss: 0.0033 - val_mae: 0.0492\n",
            "Epoch 19/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0034 - mae: 0.0441 - val_loss: 0.0028 - val_mae: 0.0443\n",
            "Epoch 20/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0032 - mae: 0.0452 - val_loss: 0.0020 - val_mae: 0.0358\n",
            "Epoch 21/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.0031 - mae: 0.0434 - val_loss: 0.0011 - val_mae: 0.0261\n",
            "Epoch 22/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - mae: 0.0388 - val_loss: 0.0014 - val_mae: 0.0290\n",
            "Epoch 23/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0025 - mae: 0.0394 - val_loss: 0.0012 - val_mae: 0.0262\n",
            "Epoch 24/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 0.0028 - mae: 0.0411 - val_loss: 0.0012 - val_mae: 0.0266\n",
            "Epoch 25/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0031 - mae: 0.0422 - val_loss: 0.0021 - val_mae: 0.0372\n",
            "Epoch 26/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0029 - mae: 0.0392 - val_loss: 0.0017 - val_mae: 0.0314\n",
            "Epoch 27/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0028 - mae: 0.0406 - val_loss: 0.0017 - val_mae: 0.0338\n",
            "Epoch 28/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0025 - mae: 0.0381 - val_loss: 0.0018 - val_mae: 0.0342\n",
            "Epoch 29/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.0022 - mae: 0.0361 - val_loss: 0.0010 - val_mae: 0.0244\n",
            "Epoch 30/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - mae: 0.0387 - val_loss: 0.0022 - val_mae: 0.0369\n",
            "Epoch 31/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0026 - mae: 0.0394 - val_loss: 0.0014 - val_mae: 0.0268\n",
            "Epoch 32/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0027 - mae: 0.0405 - val_loss: 0.0011 - val_mae: 0.0260\n",
            "Epoch 33/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0021 - mae: 0.0350 - val_loss: 0.0037 - val_mae: 0.0527\n",
            "Epoch 34/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0025 - mae: 0.0390 - val_loss: 0.0013 - val_mae: 0.0284\n",
            "Epoch 35/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - mae: 0.0366 - val_loss: 0.0018 - val_mae: 0.0348\n",
            "Epoch 36/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0024 - mae: 0.0365 - val_loss: 0.0016 - val_mae: 0.0299\n",
            "Epoch 37/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 0.0028 - mae: 0.0400 - val_loss: 0.0013 - val_mae: 0.0280\n",
            "Epoch 38/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0018 - mae: 0.0325 - val_loss: 0.0014 - val_mae: 0.0287\n",
            "Epoch 39/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - mae: 0.0348 - val_loss: 0.0020 - val_mae: 0.0355\n",
            "Epoch 40/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0026 - mae: 0.0386 - val_loss: 0.0020 - val_mae: 0.0347\n",
            "Epoch 41/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0022 - mae: 0.0355 - val_loss: 0.0028 - val_mae: 0.0428\n",
            "Epoch 42/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 0.0022 - mae: 0.0372 - val_loss: 0.0042 - val_mae: 0.0537\n",
            "Epoch 43/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0017 - mae: 0.0314 - val_loss: 0.0041 - val_mae: 0.0518\n",
            "Epoch 44/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0019 - mae: 0.0343 - val_loss: 0.0024 - val_mae: 0.0355\n",
            "Epoch 45/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0020 - mae: 0.0335 - val_loss: 0.0020 - val_mae: 0.0363\n",
            "Epoch 46/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0023 - mae: 0.0355 - val_loss: 0.0026 - val_mae: 0.0379\n",
            "Epoch 47/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.0022 - mae: 0.0366 - val_loss: 0.0029 - val_mae: 0.0437\n",
            "Epoch 48/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 0.0023 - mae: 0.0361 - val_loss: 0.0016 - val_mae: 0.0322\n",
            "Epoch 49/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0025 - mae: 0.0368 - val_loss: 0.0017 - val_mae: 0.0315\n",
            "Epoch 50/50\n",
            "\u001b[1m11/11\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0022 - mae: 0.0357 - val_loss: 0.0014 - val_mae: 0.0294\n",
            "Epoch 1/50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [code]\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "# Assuming you already have predictions_rescaled and y_test_rescaled from your previous cell\n",
        "# predictions_rescaled = ...\n",
        "# y_test_rescaled = ...\n",
        "\n",
        "# Calculate MAE and RMSE in the original stock price scale\n",
        "mae_original = mean_absolute_error(y_test_rescaled, predictions_rescaled)\n",
        "rmse_original = np.sqrt(mean_squared_error(y_test_rescaled, predictions_rescaled))\n",
        "\n",
        "print(\"=== Error Metrics in Original Scale ===\")\n",
        "print(f\"MAE:  {mae_original:.4f} (units of actual stock price)\")\n",
        "print(f\"RMSE: {rmse_original:.4f} (units of actual stock price)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4HppY8EHtJT",
        "outputId": "9ee2c54b-3124-4210-f861-00113665e3e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Error Metrics in Original Scale ===\n",
            "MAE:  2.4806 (units of actual stock price)\n",
            "RMSE: 3.0296 (units of actual stock price)\n"
          ]
        }
      ]
    }
  ]
}